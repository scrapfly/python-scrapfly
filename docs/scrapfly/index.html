<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>scrapfly API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>scrapfly</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">__version__ = &#39;0.8.21&#39;

from typing import Tuple
from .errors import ScrapflyError
from .errors import ScrapflyAspError
from .errors import ScrapflyProxyError
from .errors import ScrapflyScheduleError
from .errors import ScrapflyScrapeError
from .errors import ScrapflySessionError
from .errors import ScrapflyThrottleError
from .errors import ScrapflyWebhookError
from .errors import EncoderError
from .errors import ErrorFactory
from .errors import HttpError
from .errors import UpstreamHttpError
from .errors import UpstreamHttpClientError
from .errors import UpstreamHttpServerError
from .errors import ApiHttpClientError
from .errors import ApiHttpServerError
from .errors import ScreenshotAPIError
from .errors import ExtractionAPIError
from .api_response import ScrapeApiResponse, ScreenshotApiResponse, ExtractionApiResponse, ResponseBodyHandler
from .client import ScrapflyClient, ScraperAPI, MonitoringTargetPeriod, MonitoringAggregation
from .scrape_config import ScrapeConfig
from .screenshot_config import ScreenshotConfig
from .extraction_config import ExtractionConfig


__all__: Tuple[str, ...] = (
    &#39;ScrapflyError&#39;,
    &#39;ScrapflyAspError&#39;,
    &#39;ScrapflyProxyError&#39;,
    &#39;ScrapflyScheduleError&#39;,
    &#39;ScrapflyScrapeError&#39;,
    &#39;ScrapflySessionError&#39;,
    &#39;ScrapflyThrottleError&#39;,
    &#39;ScrapflyWebhookError&#39;,
    &#39;UpstreamHttpError&#39;,
    &#39;UpstreamHttpClientError&#39;,
    &#39;UpstreamHttpServerError&#39;,
    &#39;ApiHttpClientError&#39;,
    &#39;ApiHttpServerError&#39;,
    &#39;EncoderError&#39;,
    &#39;ScrapeApiResponse&#39;,
    &#39;ScreenshotApiResponse&#39;,
    &#39;ExtractionApiResponse&#39;,
    &#39;ErrorFactory&#39;,
    &#39;HttpError&#39;,
    &#39;ScrapflyClient&#39;,
    &#39;ResponseBodyHandler&#39;,
    &#39;ScrapeConfig&#39;,
    &#39;ScreenshotConfig&#39;,
    &#39;ExtractionConfig&#39;,
    &#39;ScreenshotAPIError&#39;,
    &#39;ExtractionAPIError&#39;,
    &#39;ScraperAPI&#39;,
    &#39;MonitoringTargetPeriod&#39;,
    &#39;MonitoringAggregation&#39;,
)</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="scrapfly.api_config" href="api_config.html">scrapfly.api_config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.api_response" href="api_response.html">scrapfly.api_response</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.client" href="client.html">scrapfly.client</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.errors" href="errors.html">scrapfly.errors</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.extraction_config" href="extraction_config.html">scrapfly.extraction_config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.frozen_dict" href="frozen_dict.html">scrapfly.frozen_dict</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.polyfill" href="polyfill/index.html">scrapfly.polyfill</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.reporter" href="reporter/index.html">scrapfly.reporter</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.scrape_config" href="scrape_config.html">scrapfly.scrape_config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.scrapy" href="scrapy/index.html">scrapfly.scrapy</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.screenshot_config" href="screenshot_config.html">scrapfly.screenshot_config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="scrapfly.webhook" href="webhook.html">scrapfly.webhook</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scrapfly.ApiHttpClientError"><code class="flex name class">
<span>class <span class="ident">ApiHttpClientError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiHttpClientError(HttpError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.ApiHttpServerError" href="errors.html#scrapfly.errors.ApiHttpServerError">ApiHttpServerError</a></li>
<li>scrapfly.errors.BadApiKeyError</li>
<li>scrapfly.errors.PaymentRequired</li>
<li>scrapfly.errors.TooManyRequest</li>
</ul>
</dd>
<dt id="scrapfly.ApiHttpServerError"><code class="flex name class">
<span>class <span class="ident">ApiHttpServerError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiHttpServerError(ApiHttpClientError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.ApiHttpClientError" href="errors.html#scrapfly.errors.ApiHttpClientError">ApiHttpClientError</a></li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.EncoderError"><code class="flex name class">
<span>class <span class="ident">EncoderError</span></span>
<span>(</span><span>content: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all exceptions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EncoderError(BaseException):

    def __init__(self, content:str):
        self.content = content
        super().__init__()

    def __str__(self) -&gt; str:
        return self.content

    def __repr__(self):
        return &#34;Invalid payload: %s&#34; % self.content</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ErrorFactory"><code class="flex name class">
<span>class <span class="ident">ErrorFactory</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ErrorFactory:
    RESOURCE_TO_ERROR = {
        ScrapflyError.RESOURCE_SCRAPE: ScrapflyScrapeError,
        ScrapflyError.RESOURCE_WEBHOOK: ScrapflyWebhookError,
        ScrapflyError.RESOURCE_PROXY: ScrapflyProxyError,
        ScrapflyError.RESOURCE_SCHEDULE: ScrapflyScheduleError,
        ScrapflyError.RESOURCE_ASP: ScrapflyAspError,
        ScrapflyError.RESOURCE_SESSION: ScrapflySessionError
    }

    # Notable http error has own class for more convenience
    # Only applicable for generic API error
    HTTP_STATUS_TO_ERROR = {
        401: BadApiKeyError,
        402: PaymentRequired,
        429: TooManyRequest
    }

    @staticmethod
    def _get_resource(code: str) -&gt; Optional[Tuple[str, str]]:

        if isinstance(code, str) and &#39;::&#39; in code:
            _, resource, _ = code.split(&#39;::&#39;)
            return resource

        return None

    @staticmethod
    def create(api_response: &#39;ScrapeApiResponse&#39;):
        is_retryable = False
        kind = ScrapflyError.KIND_HTTP_BAD_RESPONSE if api_response.success is False else ScrapflyError.KIND_SCRAPFLY_ERROR
        http_code = api_response.status_code
        retry_delay = 5
        retry_times = 3
        description = None
        error_url = &#39;https://scrapfly.io/docs/scrape-api/errors#api&#39;
        code = api_response.error[&#39;code&#39;]

        if code == &#39;ERR::SCRAPE::BAD_UPSTREAM_RESPONSE&#39;:
            http_code = api_response.scrape_result[&#39;status_code&#39;]

        if &#39;description&#39; in api_response.error:
            description = api_response.error[&#39;description&#39;]

        message = &#39;%s %s %s&#39; % (str(http_code), code, api_response.error[&#39;message&#39;])

        if &#39;doc_url&#39; in api_response.error:
            error_url = api_response.error[&#39;doc_url&#39;]

        if &#39;retryable&#39; in api_response.error:
            is_retryable = api_response.error[&#39;retryable&#39;]

        resource = ErrorFactory._get_resource(code=code)

        if is_retryable is True:
            if &#39;X-Retry&#39; in api_response.headers:
                retry_delay = int(api_response.headers[&#39;Retry-After&#39;])

        message = &#39;%s: %s&#39; % (message, description) if description else message

        if retry_delay is not None and is_retryable is True:
            message = &#39;%s. Retry delay : %s seconds&#39; % (message, str(retry_delay))

        args = {
            &#39;message&#39;: message,
            &#39;code&#39;: code,
            &#39;http_status_code&#39;: http_code,
            &#39;is_retryable&#39;: is_retryable,
            &#39;api_response&#39;: api_response,
            &#39;resource&#39;: resource,
            &#39;retry_delay&#39;: retry_delay,
            &#39;retry_times&#39;: retry_times,
            &#39;documentation_url&#39;: error_url,
            &#39;request&#39;: api_response.request,
            &#39;response&#39;: api_response.response
        }

        if kind == ScrapflyError.KIND_HTTP_BAD_RESPONSE:
            if http_code &gt;= 500:
                return ApiHttpServerError(**args)

            is_scraper_api_error = resource in ErrorFactory.RESOURCE_TO_ERROR

            if http_code in ErrorFactory.HTTP_STATUS_TO_ERROR and not is_scraper_api_error:
                return ErrorFactory.HTTP_STATUS_TO_ERROR[http_code](**args)

            if is_scraper_api_error:
                return ErrorFactory.RESOURCE_TO_ERROR[resource](**args)

            return ApiHttpClientError(**args)

        elif kind == ScrapflyError.KIND_SCRAPFLY_ERROR:
            if code == &#39;ERR::SCRAPE::BAD_UPSTREAM_RESPONSE&#39;:
                if http_code &gt;= 500:
                    return UpstreamHttpServerError(**args)

                if http_code &gt;= 400:
                    return UpstreamHttpClientError(**args)

            if resource in ErrorFactory.RESOURCE_TO_ERROR:
                return ErrorFactory.RESOURCE_TO_ERROR[resource](**args)

            return ScrapflyError(**args)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ErrorFactory.HTTP_STATUS_TO_ERROR"><code class="name">var <span class="ident">HTTP_STATUS_TO_ERROR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ErrorFactory.RESOURCE_TO_ERROR"><code class="name">var <span class="ident">RESOURCE_TO_ERROR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scrapfly.ErrorFactory.create"><code class="name flex">
<span>def <span class="ident">create</span></span>(<span>api_response: <a title="scrapfly.ScrapeApiResponse" href="#scrapfly.ScrapeApiResponse">ScrapeApiResponse</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create(api_response: &#39;ScrapeApiResponse&#39;):
    is_retryable = False
    kind = ScrapflyError.KIND_HTTP_BAD_RESPONSE if api_response.success is False else ScrapflyError.KIND_SCRAPFLY_ERROR
    http_code = api_response.status_code
    retry_delay = 5
    retry_times = 3
    description = None
    error_url = &#39;https://scrapfly.io/docs/scrape-api/errors#api&#39;
    code = api_response.error[&#39;code&#39;]

    if code == &#39;ERR::SCRAPE::BAD_UPSTREAM_RESPONSE&#39;:
        http_code = api_response.scrape_result[&#39;status_code&#39;]

    if &#39;description&#39; in api_response.error:
        description = api_response.error[&#39;description&#39;]

    message = &#39;%s %s %s&#39; % (str(http_code), code, api_response.error[&#39;message&#39;])

    if &#39;doc_url&#39; in api_response.error:
        error_url = api_response.error[&#39;doc_url&#39;]

    if &#39;retryable&#39; in api_response.error:
        is_retryable = api_response.error[&#39;retryable&#39;]

    resource = ErrorFactory._get_resource(code=code)

    if is_retryable is True:
        if &#39;X-Retry&#39; in api_response.headers:
            retry_delay = int(api_response.headers[&#39;Retry-After&#39;])

    message = &#39;%s: %s&#39; % (message, description) if description else message

    if retry_delay is not None and is_retryable is True:
        message = &#39;%s. Retry delay : %s seconds&#39; % (message, str(retry_delay))

    args = {
        &#39;message&#39;: message,
        &#39;code&#39;: code,
        &#39;http_status_code&#39;: http_code,
        &#39;is_retryable&#39;: is_retryable,
        &#39;api_response&#39;: api_response,
        &#39;resource&#39;: resource,
        &#39;retry_delay&#39;: retry_delay,
        &#39;retry_times&#39;: retry_times,
        &#39;documentation_url&#39;: error_url,
        &#39;request&#39;: api_response.request,
        &#39;response&#39;: api_response.response
    }

    if kind == ScrapflyError.KIND_HTTP_BAD_RESPONSE:
        if http_code &gt;= 500:
            return ApiHttpServerError(**args)

        is_scraper_api_error = resource in ErrorFactory.RESOURCE_TO_ERROR

        if http_code in ErrorFactory.HTTP_STATUS_TO_ERROR and not is_scraper_api_error:
            return ErrorFactory.HTTP_STATUS_TO_ERROR[http_code](**args)

        if is_scraper_api_error:
            return ErrorFactory.RESOURCE_TO_ERROR[resource](**args)

        return ApiHttpClientError(**args)

    elif kind == ScrapflyError.KIND_SCRAPFLY_ERROR:
        if code == &#39;ERR::SCRAPE::BAD_UPSTREAM_RESPONSE&#39;:
            if http_code &gt;= 500:
                return UpstreamHttpServerError(**args)

            if http_code &gt;= 400:
                return UpstreamHttpClientError(**args)

        if resource in ErrorFactory.RESOURCE_TO_ERROR:
            return ErrorFactory.RESOURCE_TO_ERROR[resource](**args)

        return ScrapflyError(**args)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.ExtractionAPIError"><code class="flex name class">
<span>class <span class="ident">ExtractionAPIError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtractionAPIError(HttpError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ExtractionApiResponse"><code class="flex name class">
<span>class <span class="ident">ExtractionApiResponse</span></span>
<span>(</span><span>request: requests.models.Request, response: requests.models.Response, extraction_config: <a title="scrapfly.extraction_config.ExtractionConfig" href="extraction_config.html#scrapfly.extraction_config.ExtractionConfig">ExtractionConfig</a>, api_result: Optional[bytes] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtractionApiResponse(ApiResponse):
    def __init__(self, request: Request, response: Response, extraction_config: ExtractionConfig, api_result: Optional[bytes] = None):
        super().__init__(request, response)
        self.extraction_config = extraction_config
        self.result = self.handle_api_result(api_result)

    @property
    def extraction_result(self) -&gt; Optional[Dict]:
        extraction_result = self.result.get(&#39;result&#39;, None)
        if not extraction_result:  # handle empty extraction responses
            return {&#39;data&#39;: None, &#39;content_type&#39;: None}
        else:
            return extraction_result

    @property
    def data(self) -&gt; Union[Dict, List, str]:  # depends on the LLM prompt
        if self.error is None:
            return self.extraction_result[&#39;data&#39;]

        return None

    @property
    def content_type(self) -&gt; Optional[str]:
        if self.error is None:
            return self.extraction_result[&#39;content_type&#39;]

        return None

    @property
    def extraction_success(self) -&gt; bool:
        extraction_result = self.extraction_result
        if extraction_result is None or extraction_result[&#39;data&#39;] is None:
            return False

        return True

    @property
    def error(self) -&gt; Optional[Dict]:
        if self.extraction_result is None:
            return self.result

        return None

    def _is_api_error(self, api_result: Dict) -&gt; bool:
        if api_result is None:
            return True

        return &#39;error_id&#39; in api_result

    def handle_api_result(self, api_result: bytes) -&gt; FrozenDict:
        if self._is_api_error(api_result=api_result) is True:
            return FrozenDict(api_result)

        return FrozenDict({&#39;result&#39;: api_result})

    def raise_for_result(self, raise_on_upstream_error=True, error_class=ExtractionAPIError):
        super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="scrapfly.ExtractionApiResponse.content_type"><code class="name">var <span class="ident">content_type</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def content_type(self) -&gt; Optional[str]:
    if self.error is None:
        return self.extraction_result[&#39;content_type&#39;]

    return None</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionApiResponse.data"><code class="name">var <span class="ident">data</span> : Union[Dict, List, str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def data(self) -&gt; Union[Dict, List, str]:  # depends on the LLM prompt
    if self.error is None:
        return self.extraction_result[&#39;data&#39;]

    return None</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionApiResponse.error"><code class="name">var <span class="ident">error</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def error(self) -&gt; Optional[Dict]:
    if self.extraction_result is None:
        return self.result

    return None</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionApiResponse.extraction_result"><code class="name">var <span class="ident">extraction_result</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def extraction_result(self) -&gt; Optional[Dict]:
    extraction_result = self.result.get(&#39;result&#39;, None)
    if not extraction_result:  # handle empty extraction responses
        return {&#39;data&#39;: None, &#39;content_type&#39;: None}
    else:
        return extraction_result</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionApiResponse.extraction_success"><code class="name">var <span class="ident">extraction_success</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def extraction_success(self) -&gt; bool:
    extraction_result = self.extraction_result
    if extraction_result is None or extraction_result[&#39;data&#39;] is None:
        return False

    return True</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ExtractionApiResponse.handle_api_result"><code class="name flex">
<span>def <span class="ident">handle_api_result</span></span>(<span>self, api_result: bytes) ‑> <a title="scrapfly.frozen_dict.FrozenDict" href="frozen_dict.html#scrapfly.frozen_dict.FrozenDict">FrozenDict</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_api_result(self, api_result: bytes) -&gt; FrozenDict:
    if self._is_api_error(api_result=api_result) is True:
        return FrozenDict(api_result)

    return FrozenDict({&#39;result&#39;: api_result})</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionApiResponse.raise_for_result"><code class="name flex">
<span>def <span class="ident">raise_for_result</span></span>(<span>self, raise_on_upstream_error=True, error_class=scrapfly.errors.ExtractionAPIError)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def raise_for_result(self, raise_on_upstream_error=True, error_class=ExtractionAPIError):
    super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></b></code>:
<ul class="hlist">
<li><code><a title="scrapfly.api_response.ApiResponse.status_code" href="api_response.html#scrapfly.api_response.ApiResponse.status_code">status_code</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="scrapfly.ExtractionConfig"><code class="flex name class">
<span>class <span class="ident">ExtractionConfig</span></span>
<span>(</span><span>body: Union[str, bytes], content_type: str, url: Optional[str] = None, charset: Optional[str] = None, extraction_template: Optional[str] = None, extraction_ephemeral_template: Optional[Dict] = None, extraction_prompt: Optional[str] = None, extraction_model: Optional[str] = None, is_document_compressed: Optional[bool] = None, document_compression_format: Optional[<a title="scrapfly.extraction_config.CompressionFormat" href="extraction_config.html#scrapfly.extraction_config.CompressionFormat">CompressionFormat</a>] = None, webhook: Optional[str] = None, raise_on_upstream_error: bool = True, template: Optional[str] = None, ephemeral_template: Optional[Dict] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ExtractionConfig(BaseApiConfig):
    body: Union[str, bytes]
    content_type: str
    url: Optional[str] = None
    charset: Optional[str] = None
    extraction_template: Optional[str] = None  # a saved template name
    extraction_ephemeral_template: Optional[Dict]  # ephemeraly declared json template
    extraction_prompt: Optional[str] = None
    extraction_model: Optional[str] = None
    is_document_compressed: Optional[bool] = None
    document_compression_format: Optional[CompressionFormat] = None
    webhook: Optional[str] = None
    raise_on_upstream_error: bool = True

    # deprecated options
    template: Optional[str] = None
    ephemeral_template: Optional[Dict] = None

    def __init__(
        self,
        body: Union[str, bytes],
        content_type: str,
        url: Optional[str] = None,
        charset: Optional[str] = None,
        extraction_template: Optional[str] = None,  # a saved template name
        extraction_ephemeral_template: Optional[Dict] = None,  # ephemeraly declared json template
        extraction_prompt: Optional[str] = None,
        extraction_model: Optional[str] = None,
        is_document_compressed: Optional[bool] = None,
        document_compression_format: Optional[CompressionFormat] = None,
        webhook: Optional[str] = None,
        raise_on_upstream_error: bool = True,

        # deprecated options
        template: Optional[str] = None,
        ephemeral_template: Optional[Dict] = None     
    ):
        if template:
            print(&#34;WARNGING&#34;)
            warnings.warn(
                &#34;Deprecation warning: &#39;template&#39; is deprecated. Use &#39;extraction_template&#39; instead.&#34;
            )
            extraction_template = template

        if ephemeral_template:
            warnings.warn(
                &#34;Deprecation warning: &#39;ephemeral_template&#39; is deprecated. Use &#39;extraction_ephemeral_template&#39; instead.&#34;
            )
            extraction_ephemeral_template = ephemeral_template

        self.key = None
        self.body = body
        self.content_type = content_type
        self.url = url
        self.charset = charset
        self.extraction_template = extraction_template
        self.extraction_ephemeral_template = extraction_ephemeral_template
        self.extraction_prompt = extraction_prompt
        self.extraction_model = extraction_model
        self.is_document_compressed = is_document_compressed
        self.document_compression_format = CompressionFormat(document_compression_format) if document_compression_format else None
        self.webhook = webhook
        self.raise_on_upstream_error = raise_on_upstream_error

        if isinstance(body, bytes) or document_compression_format:
            compression_format = detect_compression_format(body)

            if compression_format is not None:
                self.is_document_compressed = True

                if self.document_compression_format and compression_format != self.document_compression_format:
                    raise ExtractionConfigError(
                        f&#39;The detected compression format `{compression_format}` does not match declared format `{self.document_compression_format}`. &#39;
                        f&#39;You must pass the compression format or disable compression.&#39;
                    )
                
                self.document_compression_format = compression_format
            
            else:
                self.is_document_compressed = False

            if self.is_document_compressed is False:
                compression_foramt = CompressionFormat(self.document_compression_format) if self.document_compression_format else None
                
                if isinstance(self.body, str) and compression_foramt:
                    self.body = self.body.encode(&#39;utf-8&#39;)

                if compression_foramt == CompressionFormat.GZIP:
                    import gzip
                    self.body = gzip.compress(self.body)

                elif compression_foramt == CompressionFormat.ZSTD:
                    try:
                        import zstandard as zstd
                    except ImportError:
                        raise ExtractionConfigError(
                            f&#39;zstandard is not installed. You must run pip install zstandard&#39;
                            f&#39; to auto compress into zstd or use compression formats.&#39;
                        )
                    self.body = zstd.compress(self.body)
                
                elif compression_foramt == CompressionFormat.DEFLATE:
                    import zlib
                    compressor = zlib.compressobj(wbits=-zlib.MAX_WBITS) # raw deflate compression
                    self.body = compressor.compress(self.body) + compressor.flush()

    def to_api_params(self, key: str) -&gt; Dict:
        params = {
            &#39;key&#39;: self.key or key,
            &#39;content_type&#39;: self.content_type
        }

        if self.url:
            params[&#39;url&#39;] = self.url

        if self.charset:
            params[&#39;charset&#39;] = self.charset

        if self.extraction_template and self.extraction_ephemeral_template:
            raise ExtractionConfigError(&#39;You cannot pass both parameters extraction_template and extraction_ephemeral_template. You must choose&#39;)

        if self.extraction_template:
            params[&#39;extraction_template&#39;] = self.extraction_template

        if self.extraction_ephemeral_template:
            self.extraction_ephemeral_template = json.dumps(self.extraction_ephemeral_template)
            params[&#39;extraction_template&#39;] = &#39;ephemeral:&#39; + urlsafe_b64encode(self.extraction_ephemeral_template.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.extraction_prompt:
            params[&#39;extraction_prompt&#39;] = quote_plus(self.extraction_prompt)

        if self.extraction_model:
            params[&#39;extraction_model&#39;] = self.extraction_model

        if self.webhook:
            params[&#39;webhook_name&#39;] = self.webhook

        return params

    def to_dict(self) -&gt; Dict:
        &#34;&#34;&#34;
        Export the ExtractionConfig instance to a plain dictionary.
        &#34;&#34;&#34;

        if self.is_document_compressed is True:
                compression_foramt = CompressionFormat(self.document_compression_format) if self.document_compression_format else None

                if compression_foramt == CompressionFormat.GZIP:
                    import gzip
                    self.body = gzip.decompress(self.body)
                    
                elif compression_foramt == CompressionFormat.ZSTD:
                    import zstandard as zstd
                    self.body = zstd.decompress(self.body)

                elif compression_foramt == CompressionFormat.DEFLATE:
                    import zlib
                    decompressor = zlib.decompressobj(wbits=-zlib.MAX_WBITS)
                    self.body = decompressor.decompress(self.body) + decompressor.flush()

                if isinstance(self.body, bytes):
                    self.body = self.body.decode(&#39;utf-8&#39;)
                    self.is_document_compressed = False

        return {
            &#39;body&#39;: self.body,
            &#39;content_type&#39;: self.content_type,
            &#39;url&#39;: self.url,
            &#39;charset&#39;: self.charset,
            &#39;extraction_template&#39;: self.extraction_template,
            &#39;extraction_ephemeral_template&#39;: self.extraction_ephemeral_template,
            &#39;extraction_prompt&#39;: self.extraction_prompt,
            &#39;extraction_model&#39;: self.extraction_model,
            &#39;is_document_compressed&#39;: self.is_document_compressed,
            &#39;document_compression_format&#39;: CompressionFormat(self.document_compression_format).value if self.document_compression_format else None,
            &#39;webhook&#39;: self.webhook,
            &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error,
        }
    
    @staticmethod
    def from_dict(extraction_config_dict: Dict) -&gt; &#39;ExtractionConfig&#39;:
        &#34;&#34;&#34;Create an ExtractionConfig instance from a dictionary.&#34;&#34;&#34;
        body = extraction_config_dict.get(&#39;body&#39;, None)
        content_type = extraction_config_dict.get(&#39;content_type&#39;, None)
        url = extraction_config_dict.get(&#39;url&#39;, None)
        charset = extraction_config_dict.get(&#39;charset&#39;, None)
        extraction_template = extraction_config_dict.get(&#39;extraction_template&#39;, None)
        extraction_ephemeral_template = extraction_config_dict.get(&#39;extraction_ephemeral_template&#39;, None)
        extraction_prompt = extraction_config_dict.get(&#39;extraction_prompt&#39;, None)
        extraction_model = extraction_config_dict.get(&#39;extraction_model&#39;, None)
        is_document_compressed = extraction_config_dict.get(&#39;is_document_compressed&#39;, None)

        document_compression_format = extraction_config_dict.get(&#39;document_compression_format&#39;, None)
        document_compression_format = CompressionFormat(document_compression_format) if document_compression_format else None
        
        webhook = extraction_config_dict.get(&#39;webhook&#39;, None)
        raise_on_upstream_error = extraction_config_dict.get(&#39;raise_on_upstream_error&#39;, True)

        return ExtractionConfig(
            body=body,
            content_type=content_type,
            url=url,
            charset=charset,
            extraction_template=extraction_template,
            extraction_ephemeral_template=extraction_ephemeral_template,
            extraction_prompt=extraction_prompt,
            extraction_model=extraction_model,
            is_document_compressed=is_document_compressed,
            document_compression_format=document_compression_format,
            webhook=webhook,
            raise_on_upstream_error=raise_on_upstream_error
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_config.BaseApiConfig" href="api_config.html#scrapfly.api_config.BaseApiConfig">BaseApiConfig</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ExtractionConfig.body"><code class="name">var <span class="ident">body</span> : Union[str, bytes]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.charset"><code class="name">var <span class="ident">charset</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.content_type"><code class="name">var <span class="ident">content_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.document_compression_format"><code class="name">var <span class="ident">document_compression_format</span> : Optional[<a title="scrapfly.extraction_config.CompressionFormat" href="extraction_config.html#scrapfly.extraction_config.CompressionFormat">CompressionFormat</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.ephemeral_template"><code class="name">var <span class="ident">ephemeral_template</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.extraction_ephemeral_template"><code class="name">var <span class="ident">extraction_ephemeral_template</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.extraction_model"><code class="name">var <span class="ident">extraction_model</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.extraction_prompt"><code class="name">var <span class="ident">extraction_prompt</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.extraction_template"><code class="name">var <span class="ident">extraction_template</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.is_document_compressed"><code class="name">var <span class="ident">is_document_compressed</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.raise_on_upstream_error"><code class="name">var <span class="ident">raise_on_upstream_error</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.template"><code class="name">var <span class="ident">template</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.url"><code class="name">var <span class="ident">url</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ExtractionConfig.webhook"><code class="name">var <span class="ident">webhook</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scrapfly.ExtractionConfig.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>extraction_config_dict: Dict) ‑> <a title="scrapfly.extraction_config.ExtractionConfig" href="extraction_config.html#scrapfly.extraction_config.ExtractionConfig">ExtractionConfig</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create an ExtractionConfig instance from a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(extraction_config_dict: Dict) -&gt; &#39;ExtractionConfig&#39;:
    &#34;&#34;&#34;Create an ExtractionConfig instance from a dictionary.&#34;&#34;&#34;
    body = extraction_config_dict.get(&#39;body&#39;, None)
    content_type = extraction_config_dict.get(&#39;content_type&#39;, None)
    url = extraction_config_dict.get(&#39;url&#39;, None)
    charset = extraction_config_dict.get(&#39;charset&#39;, None)
    extraction_template = extraction_config_dict.get(&#39;extraction_template&#39;, None)
    extraction_ephemeral_template = extraction_config_dict.get(&#39;extraction_ephemeral_template&#39;, None)
    extraction_prompt = extraction_config_dict.get(&#39;extraction_prompt&#39;, None)
    extraction_model = extraction_config_dict.get(&#39;extraction_model&#39;, None)
    is_document_compressed = extraction_config_dict.get(&#39;is_document_compressed&#39;, None)

    document_compression_format = extraction_config_dict.get(&#39;document_compression_format&#39;, None)
    document_compression_format = CompressionFormat(document_compression_format) if document_compression_format else None
    
    webhook = extraction_config_dict.get(&#39;webhook&#39;, None)
    raise_on_upstream_error = extraction_config_dict.get(&#39;raise_on_upstream_error&#39;, True)

    return ExtractionConfig(
        body=body,
        content_type=content_type,
        url=url,
        charset=charset,
        extraction_template=extraction_template,
        extraction_ephemeral_template=extraction_ephemeral_template,
        extraction_prompt=extraction_prompt,
        extraction_model=extraction_model,
        is_document_compressed=is_document_compressed,
        document_compression_format=document_compression_format,
        webhook=webhook,
        raise_on_upstream_error=raise_on_upstream_error
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ExtractionConfig.to_api_params"><code class="name flex">
<span>def <span class="ident">to_api_params</span></span>(<span>self, key: str) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_api_params(self, key: str) -&gt; Dict:
    params = {
        &#39;key&#39;: self.key or key,
        &#39;content_type&#39;: self.content_type
    }

    if self.url:
        params[&#39;url&#39;] = self.url

    if self.charset:
        params[&#39;charset&#39;] = self.charset

    if self.extraction_template and self.extraction_ephemeral_template:
        raise ExtractionConfigError(&#39;You cannot pass both parameters extraction_template and extraction_ephemeral_template. You must choose&#39;)

    if self.extraction_template:
        params[&#39;extraction_template&#39;] = self.extraction_template

    if self.extraction_ephemeral_template:
        self.extraction_ephemeral_template = json.dumps(self.extraction_ephemeral_template)
        params[&#39;extraction_template&#39;] = &#39;ephemeral:&#39; + urlsafe_b64encode(self.extraction_ephemeral_template.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

    if self.extraction_prompt:
        params[&#39;extraction_prompt&#39;] = quote_plus(self.extraction_prompt)

    if self.extraction_model:
        params[&#39;extraction_model&#39;] = self.extraction_model

    if self.webhook:
        params[&#39;webhook_name&#39;] = self.webhook

    return params</code></pre>
</details>
</dd>
<dt id="scrapfly.ExtractionConfig.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Export the ExtractionConfig instance to a plain dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; Dict:
    &#34;&#34;&#34;
    Export the ExtractionConfig instance to a plain dictionary.
    &#34;&#34;&#34;

    if self.is_document_compressed is True:
            compression_foramt = CompressionFormat(self.document_compression_format) if self.document_compression_format else None

            if compression_foramt == CompressionFormat.GZIP:
                import gzip
                self.body = gzip.decompress(self.body)
                
            elif compression_foramt == CompressionFormat.ZSTD:
                import zstandard as zstd
                self.body = zstd.decompress(self.body)

            elif compression_foramt == CompressionFormat.DEFLATE:
                import zlib
                decompressor = zlib.decompressobj(wbits=-zlib.MAX_WBITS)
                self.body = decompressor.decompress(self.body) + decompressor.flush()

            if isinstance(self.body, bytes):
                self.body = self.body.decode(&#39;utf-8&#39;)
                self.is_document_compressed = False

    return {
        &#39;body&#39;: self.body,
        &#39;content_type&#39;: self.content_type,
        &#39;url&#39;: self.url,
        &#39;charset&#39;: self.charset,
        &#39;extraction_template&#39;: self.extraction_template,
        &#39;extraction_ephemeral_template&#39;: self.extraction_ephemeral_template,
        &#39;extraction_prompt&#39;: self.extraction_prompt,
        &#39;extraction_model&#39;: self.extraction_model,
        &#39;is_document_compressed&#39;: self.is_document_compressed,
        &#39;document_compression_format&#39;: CompressionFormat(self.document_compression_format).value if self.document_compression_format else None,
        &#39;webhook&#39;: self.webhook,
        &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error,
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.HttpError"><code class="flex name class">
<span>class <span class="ident">HttpError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HttpError(ScrapflyError):

    def __init__(self, request:Request, response:Optional[Response]=None, **kwargs):
        self.request = request
        self.response = response
        super().__init__(**kwargs)

    def __str__(self) -&gt; str:
        if isinstance(self, UpstreamHttpError):
            return f&#34;Target website responded with {self.api_response.scrape_result[&#39;status_code&#39;]} - {self.api_response.scrape_result[&#39;reason&#39;]}&#34;

        if self.api_response is not None:
            return self.api_response.error_message

        text = f&#34;{self.response.status_code} - {self.response.reason}&#34;

        if isinstance(self, (ApiHttpClientError, ApiHttpServerError)):
            text += &#34; - &#34; + self.message

        return text</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.ApiHttpClientError" href="errors.html#scrapfly.errors.ApiHttpClientError">ApiHttpClientError</a></li>
<li>scrapfly.errors.ExtractionAPIError</li>
<li>scrapfly.errors.QuotaLimitReached</li>
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.ScreenshotAPIError</li>
<li>scrapfly.errors.TooManyConcurrentRequest</li>
<li>scrapfly.errors.UpstreamHttpError</li>
</ul>
</dd>
<dt id="scrapfly.ResponseBodyHandler"><code class="flex name class">
<span>class <span class="ident">ResponseBodyHandler</span></span>
<span>(</span><span>use_brotli: bool = False, signing_secrets: Optional[Tuple[str]] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResponseBodyHandler:

    SUPPORTED_COMPRESSION = [&#39;gzip&#39;, &#39;deflate&#39;]
    SUPPORTED_CONTENT_TYPES = [&#39;application/msgpack&#39;, &#39;application/json&#39;]

    class JSONDateTimeDecoder(JSONDecoder):
        def __init__(self, *args, **kargs):
            JSONDecoder.__init__(self, *args, object_hook=_date_parser, **kargs)

    # brotli under perform at same gzip level and upper level destroy the cpu so
    # the trade off do not worth it for most of usage
    def __init__(self, use_brotli: bool = False, signing_secrets: Optional[Tuple[str]] = None):
        if use_brotli is True and &#39;br&#39; not in self.SUPPORTED_COMPRESSION:
            try:
                try:
                    import brotlicffi as brotli
                    self.SUPPORTED_COMPRESSION.insert(0, &#39;br&#39;)
                except ImportError:
                    import brotli
                    self.SUPPORTED_COMPRESSION.insert(0, &#39;br&#39;)
            except ImportError:
                pass

        try:
            import zstd
            self.SUPPORTED_COMPRESSION.append(&#39;zstd&#39;)
        except ImportError:
            pass

        self.content_encoding: str = &#39;, &#39;.join(self.SUPPORTED_COMPRESSION)
        self._signing_secret: Optional[Tuple[str]] = None

        if signing_secrets:
            _secrets = set()

            for signing_secret in signing_secrets:
                _secrets.add(binascii.unhexlify(signing_secret))

            self._signing_secret = tuple(_secrets)

        try:  # automatically use msgpack if available https://msgpack.org/
            import msgpack
            self.accept = &#39;application/msgpack;charset=utf-8&#39;
            self.content_type = &#39;application/msgpack;charset=utf-8&#39;
            self.content_loader = partial(msgpack.loads, object_hook=_date_parser, strict_map_key=False)
        except ImportError:
            self.accept = &#39;application/json;charset=utf-8&#39;
            self.content_type = &#39;application/json;charset=utf-8&#39;
            self.content_loader = partial(loads, cls=self.JSONDateTimeDecoder)

    def support(self, headers: Dict) -&gt; bool:
        if &#39;content-type&#39; not in headers:
            return False

        for content_type in self.SUPPORTED_CONTENT_TYPES:
            if headers[&#39;content-type&#39;].find(content_type) != -1:
                return True

        return False

    def verify(self, message: bytes, signature: str) -&gt; bool:
        for signing_secret in self._signing_secret:
            if hmac.new(signing_secret, message, hashlib.sha256).hexdigest().upper() == signature:
                return True

        return False

    def read(self, content: bytes, content_encoding: str, content_type: str, signature: Optional[str]) -&gt; Dict:
        if content_encoding == &#39;gzip&#39; or content_encoding == &#39;gz&#39;:
            import gzip
            content = gzip.decompress(content)
        elif content_encoding == &#39;deflate&#39;:
            import zlib
            content = zlib.decompress(content)
        elif content_encoding == &#39;brotli&#39; or content_encoding == &#39;br&#39;:
            import brotli
            content = brotli.decompress(content)
        elif content_encoding == &#39;zstd&#39;:
            import zstd
            content = zstd.decompress(content)

        if self._signing_secret is not None and signature is not None:
            if not self.verify(content, signature):
                raise WebhookSignatureMissMatch()

        if content_type.startswith(&#39;application/json&#39;):
            content = loads(content, cls=self.JSONDateTimeDecoder)
        elif content_type.startswith(&#39;application/msgpack&#39;):
            import msgpack
            content = msgpack.loads(content, object_hook=_date_parser, strict_map_key=False)

        return content

    def __call__(self, content: bytes, content_type: str) -&gt; Union[str, Dict]:
        content_loader = None

        if content_type.find(&#39;application/json&#39;) != -1:
            content_loader = partial(loads, cls=self.JSONDateTimeDecoder)
        elif content_type.find(&#39;application/msgpack&#39;) != -1:
            import msgpack
            content_loader = partial(msgpack.loads, object_hook=_date_parser, strict_map_key=False)

        if content_loader is None:
            raise Exception(&#39;Unsupported content type&#39;)

        try:
            return content_loader(content)
        except Exception as e:
            try:
                raise EncoderError(content=content.decode(&#39;utf-8&#39;)) from e
            except UnicodeError:
                raise EncoderError(content=base64.b64encode(content).decode(&#39;utf-8&#39;)) from e</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ResponseBodyHandler.JSONDateTimeDecoder"><code class="name">var <span class="ident">JSONDateTimeDecoder</span></code></dt>
<dd>
<div class="desc"><p>Simple JSON <a href="http://json.org">http://json.org</a> decoder</p>
<p>Performs the following translations in decoding by default:</p>
<p>+---------------+-------------------+
| JSON
| Python
|
+===============+===================+
| object
| dict
|
+---------------+-------------------+
| array
| list
|
+---------------+-------------------+
| string
| str
|
+---------------+-------------------+
| number (int)
| int
|
+---------------+-------------------+
| number (real) | float
|
+---------------+-------------------+
| true
| True
|
+---------------+-------------------+
| false
| False
|
+---------------+-------------------+
| null
| None
|
+---------------+-------------------+</p>
<p>It also understands <code>NaN</code>, <code>Infinity</code>, and <code>-Infinity</code> as
their corresponding <code>float</code> values, which is outside the JSON spec.</p></div>
</dd>
<dt id="scrapfly.ResponseBodyHandler.SUPPORTED_COMPRESSION"><code class="name">var <span class="ident">SUPPORTED_COMPRESSION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ResponseBodyHandler.SUPPORTED_CONTENT_TYPES"><code class="name">var <span class="ident">SUPPORTED_CONTENT_TYPES</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ResponseBodyHandler.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, content: bytes, content_encoding: str, content_type: str, signature: Optional[str]) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read(self, content: bytes, content_encoding: str, content_type: str, signature: Optional[str]) -&gt; Dict:
    if content_encoding == &#39;gzip&#39; or content_encoding == &#39;gz&#39;:
        import gzip
        content = gzip.decompress(content)
    elif content_encoding == &#39;deflate&#39;:
        import zlib
        content = zlib.decompress(content)
    elif content_encoding == &#39;brotli&#39; or content_encoding == &#39;br&#39;:
        import brotli
        content = brotli.decompress(content)
    elif content_encoding == &#39;zstd&#39;:
        import zstd
        content = zstd.decompress(content)

    if self._signing_secret is not None and signature is not None:
        if not self.verify(content, signature):
            raise WebhookSignatureMissMatch()

    if content_type.startswith(&#39;application/json&#39;):
        content = loads(content, cls=self.JSONDateTimeDecoder)
    elif content_type.startswith(&#39;application/msgpack&#39;):
        import msgpack
        content = msgpack.loads(content, object_hook=_date_parser, strict_map_key=False)

    return content</code></pre>
</details>
</dd>
<dt id="scrapfly.ResponseBodyHandler.support"><code class="name flex">
<span>def <span class="ident">support</span></span>(<span>self, headers: Dict) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def support(self, headers: Dict) -&gt; bool:
    if &#39;content-type&#39; not in headers:
        return False

    for content_type in self.SUPPORTED_CONTENT_TYPES:
        if headers[&#39;content-type&#39;].find(content_type) != -1:
            return True

    return False</code></pre>
</details>
</dd>
<dt id="scrapfly.ResponseBodyHandler.verify"><code class="name flex">
<span>def <span class="ident">verify</span></span>(<span>self, message: bytes, signature: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verify(self, message: bytes, signature: str) -&gt; bool:
    for signing_secret in self._signing_secret:
        if hmac.new(signing_secret, message, hashlib.sha256).hexdigest().upper() == signature:
            return True

    return False</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.ScrapeApiResponse"><code class="flex name class">
<span>class <span class="ident">ScrapeApiResponse</span></span>
<span>(</span><span>request: requests.models.Request, response: requests.models.Response, scrape_config: <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a>, api_result: Optional[Dict] = None, large_object_handler: Optional[Callable] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapeApiResponse(ApiResponse):
    scrape_config:ScrapeConfig
    large_object_handler:Callable

    def __init__(self, request: Request, response: Response, scrape_config: ScrapeConfig, api_result: Optional[Dict] = None, large_object_handler:Optional[Callable]=None):
        super().__init__(request, response)
        self.scrape_config = scrape_config
        self.large_object_handler = large_object_handler

        if self.scrape_config.method == &#39;HEAD&#39;:
            api_result = {
                &#39;result&#39;: {
                    &#39;request_headers&#39;: {},
                    &#39;status&#39;: &#39;DONE&#39;,
                    &#39;success&#39;: 200 &gt;= self.response.status_code &lt; 300,
                    &#39;response_headers&#39;: self.response.headers,
                    &#39;status_code&#39;: self.response.status_code,
                    &#39;reason&#39;: self.response.reason,
                    &#39;format&#39;: &#39;text&#39;,
                    &#39;content&#39;: &#39;&#39;
                },
                &#39;context&#39;: {},
                &#39;config&#39;: self.scrape_config.__dict__
            }

            if &#39;X-Scrapfly-Reject-Code&#39; in self.response.headers:
                api_result[&#39;result&#39;][&#39;error&#39;] = {
                    &#39;code&#39;: self.response.headers[&#39;X-Scrapfly-Reject-Code&#39;],
                    &#39;http_code&#39;: int(self.response.headers[&#39;X-Scrapfly-Reject-Http-Code&#39;]),
                    &#39;message&#39;: self.response.headers[&#39;X-Scrapfly-Reject-Description&#39;],
                    &#39;error_id&#39;: self.response.headers[&#39;X-Scrapfly-Reject-ID&#39;],
                    &#39;retryable&#39;: True if self.response.headers[&#39;X-Scrapfly-Reject-Retryable&#39;] == &#39;yes&#39; else False,
                    &#39;doc_url&#39;: &#39;&#39;,
                    &#39;links&#39;: {}
                }

                if &#39;X-Scrapfly-Reject-Doc&#39; in self.response.headers:
                    api_result[&#39;result&#39;][&#39;error&#39;][&#39;doc_url&#39;] = self.response.headers[&#39;X-Scrapfly-Reject-Doc&#39;]
                    api_result[&#39;result&#39;][&#39;error&#39;][&#39;links&#39;][&#39;Related Docs&#39;] = self.response.headers[&#39;X-Scrapfly-Reject-Doc&#39;]

        if isinstance(api_result, str):
            raise HttpError(
                request=request,
                response=response,
                message=&#39;Bad gateway&#39;,
                code=502,
                http_status_code=502,
                is_retryable=True
            )

        self.result = self.handle_api_result(api_result=api_result)

    @property
    def scrape_result(self) -&gt; Optional[Dict]:
        return self.result.get(&#39;result&#39;, None)

    @property
    def scrape_result(self) -&gt; Optional[Dict]:
        return self.result.get(&#39;result&#39;, None)

    @property
    def config(self) -&gt; Optional[Dict]:
        if self.scrape_result is None:
            return None

        return self.result[&#39;config&#39;]

    @property
    def context(self) -&gt; Optional[Dict]:
        if self.scrape_result is None:
            return None

        return self.result[&#39;context&#39;]

    @property
    def content(self) -&gt; str:
        if self.scrape_result is None:
            return &#39;&#39;

        return self.scrape_result[&#39;content&#39;]

    @property
    def success(self) -&gt; bool:
        &#34;&#34;&#34;
            /!\ Success means Scrapfly api reply correctly to the call, but the scrape can be unsuccessful if the upstream reply with error status code
        &#34;&#34;&#34;
        return 200 &gt;= self.response.status_code &lt;= 299

    @property
    def scrape_success(self) -&gt; bool:
        scrape_result = self.scrape_result

        if not scrape_result:
            return False

        return self.scrape_result[&#39;success&#39;]

    @property
    def error(self) -&gt; Optional[Dict]:
        if self.scrape_result is None:
            return None

        if self.scrape_success is False:
            return self.scrape_result[&#39;error&#39;]

    @property
    def upstream_status_code(self) -&gt; Optional[int]:
        if self.scrape_result is None:
            return None

        if &#39;status_code&#39; in self.scrape_result:
            return self.scrape_result[&#39;status_code&#39;]

        return None

    @cached_property
    def soup(self) -&gt; &#39;BeautifulSoup&#39;:
        if self.scrape_result[&#39;format&#39;] != &#39;text&#39;:
            raise ContentError(&#34;Unable to cast into beautiful soup, the format of data is binary - must be text content&#34;)

        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(self.content, &#34;lxml&#34;)
            return soup
        except ImportError as e:
            logger.error(&#39;You must install scrapfly[parser] to enable this feature&#39;)

    @cached_property
    def selector(self) -&gt; &#39;Selector&#39;:
        if self.scrape_result[&#39;format&#39;] != &#39;text&#39;:
            raise ContentError(&#34;Unable to cast into beautiful soup, the format of data is binary - must be text content&#34;)

        try:
            from parsel import Selector
            return Selector(text=self.content)
        except ImportError as e:
            logger.error(&#39;You must install parsel or scrapy package to enable this feature&#39;)
            raise e

    def handle_api_result(self, api_result: Dict) -&gt; Optional[FrozenDict]:
        if self._is_api_error(api_result=api_result) is True:
            return FrozenDict(api_result)

        try:
            if isinstance(api_result[&#39;config&#39;][&#39;headers&#39;], list):
                api_result[&#39;config&#39;][&#39;headers&#39;] = {}
        except TypeError:
            logger.info(api_result)
            raise

        with suppress(KeyError):
            api_result[&#39;result&#39;][&#39;request_headers&#39;] = CaseInsensitiveDict(api_result[&#39;result&#39;][&#39;request_headers&#39;])
            api_result[&#39;result&#39;][&#39;response_headers&#39;] = CaseInsensitiveDict(api_result[&#39;result&#39;][&#39;response_headers&#39;])

        if self.large_object_handler is not None and api_result[&#39;result&#39;][&#39;content&#39;]:
            content_format = api_result[&#39;result&#39;][&#39;format&#39;]

            if content_format in [&#39;clob&#39;, &#39;blob&#39;]:
                api_result[&#39;result&#39;][&#39;content&#39;], api_result[&#39;result&#39;][&#39;format&#39;] = self.large_object_handler(callback_url=api_result[&#39;result&#39;][&#39;content&#39;], format=content_format)
            elif content_format == &#39;binary&#39;:
                base64_payload = api_result[&#39;result&#39;][&#39;content&#39;]

                if isinstance(base64_payload, bytes):
                    base64_payload = base64_payload.decode(&#39;utf-8&#39;)

                api_result[&#39;result&#39;][&#39;content&#39;] = BytesIO(b64decode(base64_payload))

        return FrozenDict(api_result)

    def _is_api_error(self, api_result: Dict) -&gt; bool:
        if self.scrape_config.method == &#39;HEAD&#39;:
            if &#39;X-Reject-Reason&#39; in self.response.headers:
                return True
            return False

        if api_result is None:
            return True

        return &#39;error_id&#39; in api_result

    def upstream_result_into_response(self, _class=Response) -&gt; Optional[Response]:
        if _class != Response:
            raise RuntimeError(&#39;only Response from requests package is supported at the moment&#39;)

        if self.result is None:
            return None

        if self.response.status_code != 200:
            return None

        response = Response()
        response.status_code = self.scrape_result[&#39;status_code&#39;]
        response.reason = self.scrape_result[&#39;reason&#39;]

        if self.scrape_result[&#39;content&#39;]:
            if isinstance(self.scrape_result[&#39;content&#39;], BytesIO):
                response._content = self.scrape_result[&#39;content&#39;].getvalue()
            elif isinstance(self.scrape_result[&#39;content&#39;], bytes):
                response._content = self.scrape_result[&#39;content&#39;]
            elif isinstance(self.scrape_result[&#39;content&#39;], str):
                response._content = self.scrape_result[&#39;content&#39;].encode(&#39;utf-8&#39;)
        else:
            response._content = None

        response.headers.update(self.scrape_result[&#39;response_headers&#39;])
        response.url = self.scrape_result[&#39;url&#39;]

        response.request = Request(
            method=self.config[&#39;method&#39;],
            url=self.config[&#39;url&#39;],
            headers=self.scrape_result[&#39;request_headers&#39;],
            data=self.config[&#39;body&#39;] if self.config[&#39;body&#39;] else None
        )

        if &#39;set-cookie&#39; in response.headers:
            for raw_cookie in response.headers[&#39;set-cookie&#39;]:
                for name, cookie in SimpleCookie(raw_cookie).items():
                    expires = cookie.get(&#39;expires&#39;)

                    if expires == &#39;&#39;:
                        expires = None

                    if expires:
                        try:
                            expires = parse(expires).timestamp()
                        except ValueError:
                            expires = None

                    if type(expires) == str:
                        if &#39;.&#39; in expires:
                            expires = float(expires)
                        else:
                            expires = int(expires)

                    response.cookies.set_cookie(Cookie(
                        version=cookie.get(&#39;version&#39;) if cookie.get(&#39;version&#39;) else None,
                        name=name,
                        value=cookie.value,
                        path=cookie.get(&#39;path&#39;, &#39;&#39;),
                        expires=expires,
                        comment=cookie.get(&#39;comment&#39;),
                        domain=cookie.get(&#39;domain&#39;, &#39;&#39;),
                        secure=cookie.get(&#39;secure&#39;),
                        port=None,
                        port_specified=False,
                        domain_specified=cookie.get(&#39;domain&#39;) is not None and cookie.get(&#39;domain&#39;) != &#39;&#39;,
                        domain_initial_dot=bool(cookie.get(&#39;domain&#39;).startswith(&#39;.&#39;)) if cookie.get(&#39;domain&#39;) is not None else False,
                        path_specified=cookie.get(&#39;path&#39;) != &#39;&#39; and cookie.get(&#39;path&#39;) is not None,
                        discard=False,
                        comment_url=None,
                        rest={
                            &#39;httponly&#39;: cookie.get(&#39;httponly&#39;),
                            &#39;samesite&#39;: cookie.get(&#39;samesite&#39;),
                            &#39;max-age&#39;: cookie.get(&#39;max-age&#39;)
                        }
                    ))

        return response

    def sink(self, path: Optional[str] = None, name: Optional[str] = None, file: Optional[Union[TextIO, BytesIO]] = None, content: Optional[Union[str, bytes]] = None):
        file_content = content or self.scrape_result[&#39;content&#39;]
        file_path = None
        file_extension = None

        if name:
            name_parts = name.split(&#39;.&#39;)
            if len(name_parts) &gt; 1:
                file_extension = name_parts[-1]

        if not file:
            if file_extension is None:
                try:
                    mime_type = self.scrape_result[&#39;response_headers&#39;][&#39;content-type&#39;]
                except KeyError:
                    mime_type = &#39;application/octet-stream&#39;

                if &#39;;&#39; in mime_type:
                    mime_type = mime_type.split(&#39;;&#39;)[0]

                file_extension = &#39;.&#39; + mime_type.split(&#39;/&#39;)[1]

            if not name:
                name = self.config[&#39;url&#39;].split(&#39;/&#39;)[-1]

            if name.find(file_extension) == -1:
                name += file_extension

            file_path = path + &#39;/&#39; + name if path is not None else name

            if file_path == file_extension:
                url = re.sub(r&#39;(https|http)?://&#39;, &#39;&#39;, self.config[&#39;url&#39;]).replace(&#39;/&#39;, &#39;-&#39;)

                if url[-1] == &#39;-&#39;:
                    url = url[:-1]

                url += file_extension

                file_path = url

            file = open(file_path, &#39;wb&#39;)

        if isinstance(file_content, str):
            file_content = BytesIO(file_content.encode(&#39;utf-8&#39;))
        elif isinstance(file_content, bytes):
            file_content = BytesIO(file_content)

        file_content.seek(0)
        with file as f:
            shutil.copyfileobj(file_content, f, length=131072)

        logger.info(&#39;file %s created&#39; % file_path)

    def raise_for_result(self, raise_on_upstream_error=True, error_class=ApiHttpClientError):
        super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)
        if self.result[&#39;result&#39;][&#39;status&#39;] == &#39;DONE&#39; and self.scrape_success is False:
            error = ErrorFactory.create(api_response=self)
            if error:
                if isinstance(error, UpstreamHttpError):
                    if raise_on_upstream_error is True:
                        raise error
                else:
                    raise error</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScrapeApiResponse.large_object_handler"><code class="name">var <span class="ident">large_object_handler</span> : Callable</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeApiResponse.scrape_config"><code class="name">var <span class="ident">scrape_config</span> : <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="scrapfly.ScrapeApiResponse.config"><code class="name">var <span class="ident">config</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def config(self) -&gt; Optional[Dict]:
    if self.scrape_result is None:
        return None

    return self.result[&#39;config&#39;]</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.content"><code class="name">var <span class="ident">content</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def content(self) -&gt; str:
    if self.scrape_result is None:
        return &#39;&#39;

    return self.scrape_result[&#39;content&#39;]</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.context"><code class="name">var <span class="ident">context</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def context(self) -&gt; Optional[Dict]:
    if self.scrape_result is None:
        return None

    return self.result[&#39;context&#39;]</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.error"><code class="name">var <span class="ident">error</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def error(self) -&gt; Optional[Dict]:
    if self.scrape_result is None:
        return None

    if self.scrape_success is False:
        return self.scrape_result[&#39;error&#39;]</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.scrape_result"><code class="name">var <span class="ident">scrape_result</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scrape_result(self) -&gt; Optional[Dict]:
    return self.result.get(&#39;result&#39;, None)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.scrape_success"><code class="name">var <span class="ident">scrape_success</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scrape_success(self) -&gt; bool:
    scrape_result = self.scrape_result

    if not scrape_result:
        return False

    return self.scrape_result[&#39;success&#39;]</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.selector"><code class="name">var <span class="ident">selector</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __get__(self, instance, owner=None):
    if instance is None:
        return self
    if self.attrname is None:
        raise TypeError(
            &#34;Cannot use cached_property instance without calling __set_name__ on it.&#34;)
    try:
        cache = instance.__dict__
    except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)
        msg = (
            f&#34;No &#39;__dict__&#39; attribute on {type(instance).__name__!r} &#34;
            f&#34;instance to cache {self.attrname!r} property.&#34;
        )
        raise TypeError(msg) from None
    val = cache.get(self.attrname, _NOT_FOUND)
    if val is _NOT_FOUND:
        with self.lock:
            # check if another thread filled cache while we awaited lock
            val = cache.get(self.attrname, _NOT_FOUND)
            if val is _NOT_FOUND:
                val = self.func(instance)
                try:
                    cache[self.attrname] = val
                except TypeError:
                    msg = (
                        f&#34;The &#39;__dict__&#39; attribute on {type(instance).__name__!r} instance &#34;
                        f&#34;does not support item assignment for caching {self.attrname!r} property.&#34;
                    )
                    raise TypeError(msg) from None
    return val</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.soup"><code class="name">var <span class="ident">soup</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __get__(self, instance, owner=None):
    if instance is None:
        return self
    if self.attrname is None:
        raise TypeError(
            &#34;Cannot use cached_property instance without calling __set_name__ on it.&#34;)
    try:
        cache = instance.__dict__
    except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)
        msg = (
            f&#34;No &#39;__dict__&#39; attribute on {type(instance).__name__!r} &#34;
            f&#34;instance to cache {self.attrname!r} property.&#34;
        )
        raise TypeError(msg) from None
    val = cache.get(self.attrname, _NOT_FOUND)
    if val is _NOT_FOUND:
        with self.lock:
            # check if another thread filled cache while we awaited lock
            val = cache.get(self.attrname, _NOT_FOUND)
            if val is _NOT_FOUND:
                val = self.func(instance)
                try:
                    cache[self.attrname] = val
                except TypeError:
                    msg = (
                        f&#34;The &#39;__dict__&#39; attribute on {type(instance).__name__!r} instance &#34;
                        f&#34;does not support item assignment for caching {self.attrname!r} property.&#34;
                    )
                    raise TypeError(msg) from None
    return val</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.success"><code class="name">var <span class="ident">success</span> : bool</code></dt>
<dd>
<div class="desc"><p>/!\ Success means Scrapfly api reply correctly to the call, but the scrape can be unsuccessful if the upstream reply with error status code</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def success(self) -&gt; bool:
    &#34;&#34;&#34;
        /!\ Success means Scrapfly api reply correctly to the call, but the scrape can be unsuccessful if the upstream reply with error status code
    &#34;&#34;&#34;
    return 200 &gt;= self.response.status_code &lt;= 299</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.upstream_status_code"><code class="name">var <span class="ident">upstream_status_code</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def upstream_status_code(self) -&gt; Optional[int]:
    if self.scrape_result is None:
        return None

    if &#39;status_code&#39; in self.scrape_result:
        return self.scrape_result[&#39;status_code&#39;]

    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ScrapeApiResponse.handle_api_result"><code class="name flex">
<span>def <span class="ident">handle_api_result</span></span>(<span>self, api_result: Dict) ‑> Optional[<a title="scrapfly.frozen_dict.FrozenDict" href="frozen_dict.html#scrapfly.frozen_dict.FrozenDict">FrozenDict</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_api_result(self, api_result: Dict) -&gt; Optional[FrozenDict]:
    if self._is_api_error(api_result=api_result) is True:
        return FrozenDict(api_result)

    try:
        if isinstance(api_result[&#39;config&#39;][&#39;headers&#39;], list):
            api_result[&#39;config&#39;][&#39;headers&#39;] = {}
    except TypeError:
        logger.info(api_result)
        raise

    with suppress(KeyError):
        api_result[&#39;result&#39;][&#39;request_headers&#39;] = CaseInsensitiveDict(api_result[&#39;result&#39;][&#39;request_headers&#39;])
        api_result[&#39;result&#39;][&#39;response_headers&#39;] = CaseInsensitiveDict(api_result[&#39;result&#39;][&#39;response_headers&#39;])

    if self.large_object_handler is not None and api_result[&#39;result&#39;][&#39;content&#39;]:
        content_format = api_result[&#39;result&#39;][&#39;format&#39;]

        if content_format in [&#39;clob&#39;, &#39;blob&#39;]:
            api_result[&#39;result&#39;][&#39;content&#39;], api_result[&#39;result&#39;][&#39;format&#39;] = self.large_object_handler(callback_url=api_result[&#39;result&#39;][&#39;content&#39;], format=content_format)
        elif content_format == &#39;binary&#39;:
            base64_payload = api_result[&#39;result&#39;][&#39;content&#39;]

            if isinstance(base64_payload, bytes):
                base64_payload = base64_payload.decode(&#39;utf-8&#39;)

            api_result[&#39;result&#39;][&#39;content&#39;] = BytesIO(b64decode(base64_payload))

    return FrozenDict(api_result)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.raise_for_result"><code class="name flex">
<span>def <span class="ident">raise_for_result</span></span>(<span>self, raise_on_upstream_error=True, error_class=scrapfly.errors.ApiHttpClientError)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def raise_for_result(self, raise_on_upstream_error=True, error_class=ApiHttpClientError):
    super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)
    if self.result[&#39;result&#39;][&#39;status&#39;] == &#39;DONE&#39; and self.scrape_success is False:
        error = ErrorFactory.create(api_response=self)
        if error:
            if isinstance(error, UpstreamHttpError):
                if raise_on_upstream_error is True:
                    raise error
            else:
                raise error</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.sink"><code class="name flex">
<span>def <span class="ident">sink</span></span>(<span>self, path: Optional[str] = None, name: Optional[str] = None, file: Union[TextIO, _io.BytesIO, ForwardRef(None)] = None, content: Union[str, bytes, ForwardRef(None)] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sink(self, path: Optional[str] = None, name: Optional[str] = None, file: Optional[Union[TextIO, BytesIO]] = None, content: Optional[Union[str, bytes]] = None):
    file_content = content or self.scrape_result[&#39;content&#39;]
    file_path = None
    file_extension = None

    if name:
        name_parts = name.split(&#39;.&#39;)
        if len(name_parts) &gt; 1:
            file_extension = name_parts[-1]

    if not file:
        if file_extension is None:
            try:
                mime_type = self.scrape_result[&#39;response_headers&#39;][&#39;content-type&#39;]
            except KeyError:
                mime_type = &#39;application/octet-stream&#39;

            if &#39;;&#39; in mime_type:
                mime_type = mime_type.split(&#39;;&#39;)[0]

            file_extension = &#39;.&#39; + mime_type.split(&#39;/&#39;)[1]

        if not name:
            name = self.config[&#39;url&#39;].split(&#39;/&#39;)[-1]

        if name.find(file_extension) == -1:
            name += file_extension

        file_path = path + &#39;/&#39; + name if path is not None else name

        if file_path == file_extension:
            url = re.sub(r&#39;(https|http)?://&#39;, &#39;&#39;, self.config[&#39;url&#39;]).replace(&#39;/&#39;, &#39;-&#39;)

            if url[-1] == &#39;-&#39;:
                url = url[:-1]

            url += file_extension

            file_path = url

        file = open(file_path, &#39;wb&#39;)

    if isinstance(file_content, str):
        file_content = BytesIO(file_content.encode(&#39;utf-8&#39;))
    elif isinstance(file_content, bytes):
        file_content = BytesIO(file_content)

    file_content.seek(0)
    with file as f:
        shutil.copyfileobj(file_content, f, length=131072)

    logger.info(&#39;file %s created&#39; % file_path)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeApiResponse.upstream_result_into_response"><code class="name flex">
<span>def <span class="ident">upstream_result_into_response</span></span>(<span>self) ‑> Optional[requests.models.Response]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upstream_result_into_response(self, _class=Response) -&gt; Optional[Response]:
    if _class != Response:
        raise RuntimeError(&#39;only Response from requests package is supported at the moment&#39;)

    if self.result is None:
        return None

    if self.response.status_code != 200:
        return None

    response = Response()
    response.status_code = self.scrape_result[&#39;status_code&#39;]
    response.reason = self.scrape_result[&#39;reason&#39;]

    if self.scrape_result[&#39;content&#39;]:
        if isinstance(self.scrape_result[&#39;content&#39;], BytesIO):
            response._content = self.scrape_result[&#39;content&#39;].getvalue()
        elif isinstance(self.scrape_result[&#39;content&#39;], bytes):
            response._content = self.scrape_result[&#39;content&#39;]
        elif isinstance(self.scrape_result[&#39;content&#39;], str):
            response._content = self.scrape_result[&#39;content&#39;].encode(&#39;utf-8&#39;)
    else:
        response._content = None

    response.headers.update(self.scrape_result[&#39;response_headers&#39;])
    response.url = self.scrape_result[&#39;url&#39;]

    response.request = Request(
        method=self.config[&#39;method&#39;],
        url=self.config[&#39;url&#39;],
        headers=self.scrape_result[&#39;request_headers&#39;],
        data=self.config[&#39;body&#39;] if self.config[&#39;body&#39;] else None
    )

    if &#39;set-cookie&#39; in response.headers:
        for raw_cookie in response.headers[&#39;set-cookie&#39;]:
            for name, cookie in SimpleCookie(raw_cookie).items():
                expires = cookie.get(&#39;expires&#39;)

                if expires == &#39;&#39;:
                    expires = None

                if expires:
                    try:
                        expires = parse(expires).timestamp()
                    except ValueError:
                        expires = None

                if type(expires) == str:
                    if &#39;.&#39; in expires:
                        expires = float(expires)
                    else:
                        expires = int(expires)

                response.cookies.set_cookie(Cookie(
                    version=cookie.get(&#39;version&#39;) if cookie.get(&#39;version&#39;) else None,
                    name=name,
                    value=cookie.value,
                    path=cookie.get(&#39;path&#39;, &#39;&#39;),
                    expires=expires,
                    comment=cookie.get(&#39;comment&#39;),
                    domain=cookie.get(&#39;domain&#39;, &#39;&#39;),
                    secure=cookie.get(&#39;secure&#39;),
                    port=None,
                    port_specified=False,
                    domain_specified=cookie.get(&#39;domain&#39;) is not None and cookie.get(&#39;domain&#39;) != &#39;&#39;,
                    domain_initial_dot=bool(cookie.get(&#39;domain&#39;).startswith(&#39;.&#39;)) if cookie.get(&#39;domain&#39;) is not None else False,
                    path_specified=cookie.get(&#39;path&#39;) != &#39;&#39; and cookie.get(&#39;path&#39;) is not None,
                    discard=False,
                    comment_url=None,
                    rest={
                        &#39;httponly&#39;: cookie.get(&#39;httponly&#39;),
                        &#39;samesite&#39;: cookie.get(&#39;samesite&#39;),
                        &#39;max-age&#39;: cookie.get(&#39;max-age&#39;)
                    }
                ))

    return response</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></b></code>:
<ul class="hlist">
<li><code><a title="scrapfly.api_response.ApiResponse.status_code" href="api_response.html#scrapfly.api_response.ApiResponse.status_code">status_code</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="scrapfly.ScrapeConfig"><code class="flex name class">
<span>class <span class="ident">ScrapeConfig</span></span>
<span>(</span><span>url: str, retry: bool = True, method: str = 'GET', country: Optional[str] = None, render_js: bool = False, cache: bool = False, cache_clear: bool = False, ssl: bool = False, dns: bool = False, asp: bool = False, debug: bool = False, raise_on_upstream_error: bool = True, cache_ttl: Optional[int] = None, proxy_pool: Optional[str] = None, session: Optional[str] = None, tags: Union[List[str], Set[str], ForwardRef(None)] = None, format: Optional[<a title="scrapfly.scrape_config.Format" href="scrape_config.html#scrapfly.scrape_config.Format">Format</a>] = None, format_options: Optional[List[<a title="scrapfly.scrape_config.FormatOption" href="scrape_config.html#scrapfly.scrape_config.FormatOption">FormatOption</a>]] = None, extraction_template: Optional[str] = None, extraction_ephemeral_template: Optional[Dict] = None, extraction_prompt: Optional[str] = None, extraction_model: Optional[str] = None, correlation_id: Optional[str] = None, cookies: Optional[requests.structures.CaseInsensitiveDict] = None, body: Optional[str] = None, data: Optional[Dict] = None, headers: Union[requests.structures.CaseInsensitiveDict, Dict[str, str], ForwardRef(None)] = None, js: str = None, rendering_wait: int = None, wait_for_selector: Optional[str] = None, screenshots: Optional[Dict] = None, screenshot_flags: Optional[List[<a title="scrapfly.scrape_config.ScreenshotFlag" href="scrape_config.html#scrapfly.scrape_config.ScreenshotFlag">ScreenshotFlag</a>]] = None, session_sticky_proxy: Optional[bool] = None, webhook: Optional[str] = None, timeout: Optional[int] = None, js_scenario: Optional[List] = None, extract: Optional[Dict] = None, os: Optional[str] = None, lang: Optional[List[str]] = None, auto_scroll: Optional[bool] = None, cost_budget: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapeConfig(BaseApiConfig):

    PUBLIC_DATACENTER_POOL = &#39;public_datacenter_pool&#39;
    PUBLIC_RESIDENTIAL_POOL = &#39;public_residential_pool&#39;

    url: str
    retry: bool = True
    method: str = &#39;GET&#39;
    country: Optional[str] = None
    render_js: bool = False
    cache: bool = False
    cache_clear:bool = False
    ssl:bool = False
    dns:bool = False
    asp:bool = False
    debug: bool = False
    raise_on_upstream_error:bool = True
    cache_ttl:Optional[int] = None
    proxy_pool:Optional[str] = None
    session: Optional[str] = None
    tags: Optional[List[str]] = None
    format: Optional[Format] = None, # raw(unchanged)
    format_options: Optional[List[FormatOption]] 
    extraction_template: Optional[str] = None  # a saved template name
    extraction_ephemeral_template: Optional[Dict]  # ephemeraly declared json template
    extraction_prompt: Optional[str] = None
    extraction_model: Optional[str] = None    
    correlation_id: Optional[str] = None
    cookies: Optional[CaseInsensitiveDict] = None
    body: Optional[str] = None
    data: Optional[Dict] = None
    headers: Optional[CaseInsensitiveDict] = None
    js: str = None
    rendering_wait: int = None
    wait_for_selector: Optional[str] = None
    session_sticky_proxy:bool = True
    screenshots:Optional[Dict]=None
    screenshot_flags: Optional[List[ScreenshotFlag]] = None,
    webhook:Optional[str]=None
    timeout:Optional[int]=None # in milliseconds
    js_scenario: Dict = None
    extract: Dict = None
    lang:Optional[List[str]] = None
    os:Optional[str] = None
    auto_scroll:Optional[bool] = None
    cost_budget:Optional[int] = None

    def __init__(
        self,
        url: str,
        retry: bool = True,
        method: str = &#39;GET&#39;,
        country: Optional[str] = None,
        render_js: bool = False,
        cache: bool = False,
        cache_clear:bool = False,
        ssl:bool = False,
        dns:bool = False,
        asp:bool = False,
        debug: bool = False,
        raise_on_upstream_error:bool = True,
        cache_ttl:Optional[int] = None,
        proxy_pool:Optional[str] = None,
        session: Optional[str] = None,
        tags: Optional[Union[List[str], Set[str]]] = None,
        format: Optional[Format] = None, # raw(unchanged)
        format_options: Optional[List[FormatOption]] = None, # raw(unchanged)
        extraction_template: Optional[str] = None,  # a saved template name
        extraction_ephemeral_template: Optional[Dict] = None,  # ephemeraly declared json template
        extraction_prompt: Optional[str] = None,
        extraction_model: Optional[str] = None,        
        correlation_id: Optional[str] = None,
        cookies: Optional[CaseInsensitiveDict] = None,
        body: Optional[str] = None,
        data: Optional[Dict] = None,
        headers: Optional[Union[CaseInsensitiveDict, Dict[str, str]]] = None,
        js: str = None,
        rendering_wait: int = None,
        wait_for_selector: Optional[str] = None,
        screenshots:Optional[Dict]=None,
        screenshot_flags: Optional[List[ScreenshotFlag]] = None,
        session_sticky_proxy:Optional[bool] = None,
        webhook:Optional[str] = None,
        timeout:Optional[int] = None, # in milliseconds
        js_scenario:Optional[List] = None,
        extract:Optional[Dict] = None,
        os:Optional[str] = None,
        lang:Optional[List[str]] = None,
        auto_scroll:Optional[bool] = None,
        cost_budget:Optional[int] = None
    ):
        assert(type(url) is str)

        if isinstance(tags, List):
            tags = set(tags)

        cookies = cookies or {}
        headers = headers or {}

        self.cookies = CaseInsensitiveDict(cookies)
        self.headers = CaseInsensitiveDict(headers)
        self.url = url
        self.retry = retry
        self.method = method
        self.country = country
        self.session_sticky_proxy = session_sticky_proxy
        self.render_js = render_js
        self.cache = cache
        self.cache_clear = cache_clear
        self.asp = asp
        self.webhook = webhook
        self.session = session
        self.debug = debug
        self.cache_ttl = cache_ttl
        self.proxy_pool = proxy_pool
        self.tags = tags or set()
        self.format = format
        self.format_options = format_options
        self.extraction_template = extraction_template
        self.extraction_ephemeral_template = extraction_ephemeral_template
        self.extraction_prompt = extraction_prompt
        self.extraction_model = extraction_model        
        self.correlation_id = correlation_id
        self.wait_for_selector = wait_for_selector
        self.body = body
        self.data = data
        self.js = js
        self.rendering_wait = rendering_wait
        self.raise_on_upstream_error = raise_on_upstream_error
        self.screenshots = screenshots
        self.screenshot_flags = screenshot_flags
        self.key = None
        self.dns = dns
        self.ssl = ssl
        self.js_scenario = js_scenario
        self.timeout = timeout
        self.extract = extract
        self.lang = lang
        self.os = os
        self.auto_scroll = auto_scroll
        self.cost_budget = cost_budget

        if cookies:
            _cookies = []

            for name, value in cookies.items():
                _cookies.append(name + &#39;=&#39; + value)

            if &#39;cookie&#39; in self.headers:
                if self.headers[&#39;cookie&#39;][-1] != &#39;;&#39;:
                    self.headers[&#39;cookie&#39;] += &#39;;&#39;
            else:
                self.headers[&#39;cookie&#39;] = &#39;&#39;

            self.headers[&#39;cookie&#39;] += &#39;; &#39;.join(_cookies)

        if self.body and self.data:
            raise ScrapeConfigError(&#39;You cannot pass both parameters body and data. You must choose&#39;)

        if method in [&#39;POST&#39;, &#39;PUT&#39;, &#39;PATCH&#39;]:
            if self.body is None and self.data is not None:
                if &#39;content-type&#39; not in self.headers:
                    self.headers[&#39;content-type&#39;] = &#39;application/x-www-form-urlencoded&#39;
                    self.body = urlencode(data)
                else:
                    if self.headers[&#39;content-type&#39;].find(&#39;application/json&#39;) != -1:
                        self.body = json.dumps(data)
                    elif self.headers[&#39;content-type&#39;].find(&#39;application/x-www-form-urlencoded&#39;) != -1:
                        self.body = urlencode(data)
                    else:
                        raise ScrapeConfigError(&#39;Content-Type &#34;%s&#34; not supported, use body parameter to pass pre encoded body according to your content type&#39; % self.headers[&#39;content-type&#39;])
            elif self.body is None and self.data is None:
                self.headers[&#39;content-type&#39;] = &#39;text/plain&#39;

    def to_api_params(self, key:str) -&gt; Dict:
        params = {
            &#39;key&#39;: self.key or key,
            &#39;url&#39;: self.url
        }

        if self.country is not None:
            params[&#39;country&#39;] = self.country

        for name, value in self.headers.items():
            params[&#39;headers[%s]&#39; % name] = value

        if self.webhook is not None:
            params[&#39;webhook_name&#39;] = self.webhook

        if self.timeout is not None:
            params[&#39;timeout&#39;] = self.timeout

        if self.extract is not None:
            params[&#39;extract&#39;] = base64.urlsafe_b64encode(json.dumps(self.extract).encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.cost_budget is not None:
            params[&#39;cost_budget&#39;] = self.cost_budget

        if self.render_js is True:
            params[&#39;render_js&#39;] = self._bool_to_http(self.render_js)

            if self.wait_for_selector is not None:
                params[&#39;wait_for_selector&#39;] = self.wait_for_selector

            if self.js:
                params[&#39;js&#39;] = base64.urlsafe_b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

            if self.js_scenario:
                params[&#39;js_scenario&#39;] = base64.urlsafe_b64encode(json.dumps(self.js_scenario).encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

            if self.rendering_wait:
                params[&#39;rendering_wait&#39;] = self.rendering_wait

            if self.screenshots is not None:
                for name, element in self.screenshots.items():
                    params[&#39;screenshots[%s]&#39; % name] = element

            if self.screenshot_flags is not None:
                self.screenshot_flags = [ScreenshotFlag(flag) for flag in self.screenshot_flags]
                params[&#34;screenshot_flags&#34;] = &#34;,&#34;.join(flag.value for flag in self.screenshot_flags)
            else:
                if self.screenshot_flags is not None:
                    logging.warning(&#39;Params &#34;screenshot_flags&#34; is ignored. Works only if screenshots is enabled&#39;)

            if self.auto_scroll is True:
                params[&#39;auto_scroll&#39;] = self._bool_to_http(self.auto_scroll)
        else:
            if self.wait_for_selector is not None:
                logging.warning(&#39;Params &#34;wait_for_selector&#34; is ignored. Works only if render_js is enabled&#39;)

            if self.screenshots:
                logging.warning(&#39;Params &#34;screenshots&#34; is ignored. Works only if render_js is enabled&#39;)

            if self.js_scenario:
                logging.warning(&#39;Params &#34;js_scenario&#34; is ignored. Works only if render_js is enabled&#39;)

            if self.js:
                logging.warning(&#39;Params &#34;js&#34; is ignored. Works only if render_js is enabled&#39;)

            if self.rendering_wait:
                logging.warning(&#39;Params &#34;rendering_wait&#34; is ignored. Works only if render_js is enabled&#39;)

        if self.asp is True:
            params[&#39;asp&#39;] = self._bool_to_http(self.asp)

        if self.retry is False:
            params[&#39;retry&#39;] = self._bool_to_http(self.retry)

        if self.cache is True:
            params[&#39;cache&#39;] = self._bool_to_http(self.cache)

            if self.cache_clear is True:
                params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

            if self.cache_ttl is not None:
                params[&#39;cache_ttl&#39;] = self.cache_ttl
        else:
            if self.cache_clear is True:
                logging.warning(&#39;Params &#34;cache_clear&#34; is ignored. Works only if cache is enabled&#39;)

            if self.cache_ttl is not None:
                logging.warning(&#39;Params &#34;cache_ttl&#34; is ignored. Works only if cache is enabled&#39;)

        if self.dns is True:
            params[&#39;dns&#39;] = self._bool_to_http(self.dns)

        if self.ssl is True:
            params[&#39;ssl&#39;] = self._bool_to_http(self.ssl)

        if self.tags:
            params[&#39;tags&#39;] = &#39;,&#39;.join(self.tags)

        if self.format:
            params[&#39;format&#39;] = Format(self.format).value
            if self.format_options:
                params[&#39;format&#39;] += &#39;:&#39; + &#39;,&#39;.join(FormatOption(option).value for option in self.format_options)

        if self.extraction_template and self.extraction_ephemeral_template:
            raise ScrapeConfigError(&#39;You cannot pass both parameters extraction_template and extraction_ephemeral_template. You must choose&#39;)

        if self.extraction_template:
            params[&#39;extraction_template&#39;] = self.extraction_template

        if self.extraction_ephemeral_template:
            self.extraction_ephemeral_template = json.dumps(self.extraction_ephemeral_template)
            params[&#39;extraction_template&#39;] = &#39;ephemeral:&#39; + urlsafe_b64encode(self.extraction_ephemeral_template.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.extraction_prompt:
            params[&#39;extraction_prompt&#39;] = quote_plus(self.extraction_prompt)

        if self.extraction_model:
            params[&#39;extraction_model&#39;] = self.extraction_model

        if self.correlation_id:
            params[&#39;correlation_id&#39;] = self.correlation_id

        if self.session:
            params[&#39;session&#39;] = self.session

            if self.session_sticky_proxy is True: # false by default
                params[&#39;session_sticky_proxy&#39;] = self._bool_to_http(self.session_sticky_proxy)
        else:
            if self.session_sticky_proxy:
                logging.warning(&#39;Params &#34;session_sticky_proxy&#34; is ignored. Works only if session is enabled&#39;)

        if self.debug is True:
            params[&#39;debug&#39;] = self._bool_to_http(self.debug)

        if self.proxy_pool is not None:
            params[&#39;proxy_pool&#39;] = self.proxy_pool

        if self.lang is not None:
            params[&#39;lang&#39;] = &#39;,&#39;.join(self.lang)

        if self.os is not None:
            params[&#39;os&#39;] = self.os

        return params

    @staticmethod
    def from_exported_config(config:str) -&gt; &#39;ScrapeConfig&#39;:
        try:
            from msgpack import loads as msgpack_loads
        except ImportError as e:
            print(&#39;You must install msgpack package - run: pip install &#34;scrapfly-sdk[seepdup] or pip install msgpack&#39;)
            raise

        data = msgpack_loads(base64.b64decode(config))

        headers = {}

        for name, value in data[&#39;headers&#39;].items():
            if isinstance(value, Iterable):
                headers[name] = &#39;; &#39;.join(value)
            else:
                headers[name] = value

        return ScrapeConfig(
            url=data[&#39;url&#39;],
            retry=data[&#39;retry&#39;],
            headers=headers,
            session=data[&#39;session&#39;],
            session_sticky_proxy=data[&#39;session_sticky_proxy&#39;],
            cache=data[&#39;cache&#39;],
            cache_ttl=data[&#39;cache_ttl&#39;],
            cache_clear=data[&#39;cache_clear&#39;],
            render_js=data[&#39;render_js&#39;],
            method=data[&#39;method&#39;],
            asp=data[&#39;asp&#39;],
            body=data[&#39;body&#39;],
            ssl=data[&#39;ssl&#39;],
            dns=data[&#39;dns&#39;],
            country=data[&#39;country&#39;],
            debug=data[&#39;debug&#39;],
            correlation_id=data[&#39;correlation_id&#39;],
            tags=data[&#39;tags&#39;],
            format=data[&#39;format&#39;],
            js=data[&#39;js&#39;],
            rendering_wait=data[&#39;rendering_wait&#39;],
            screenshots=data[&#39;screenshots&#39;] or {},
            screenshot_flags=data[&#39;screenshot_flags&#39;],
            proxy_pool=data[&#39;proxy_pool&#39;],
            auto_scroll=data[&#39;auto_scroll&#39;],
            cost_budget=data[&#39;cost_budget&#39;]
        )

    def to_dict(self) -&gt; Dict:
        &#34;&#34;&#34;
        Export the ScrapeConfig instance to a plain dictionary. 
        Useful for JSON-serialization or other external storage.
        &#34;&#34;&#34;
        
        return {
            &#39;url&#39;: self.url,
            &#39;retry&#39;: self.retry,
            &#39;method&#39;: self.method,
            &#39;country&#39;: self.country,
            &#39;render_js&#39;: self.render_js,
            &#39;cache&#39;: self.cache,
            &#39;cache_clear&#39;: self.cache_clear,
            &#39;ssl&#39;: self.ssl,
            &#39;dns&#39;: self.dns,
            &#39;asp&#39;: self.asp,
            &#39;debug&#39;: self.debug,
            &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error,
            &#39;cache_ttl&#39;: self.cache_ttl,
            &#39;proxy_pool&#39;: self.proxy_pool,
            &#39;session&#39;: self.session,
            &#39;tags&#39;: list(self.tags),
            &#39;format&#39;: Format(self.format).value if self.format else None,
            &#39;format_options&#39;: [FormatOption(option).value for option in self.format_options] if self.format_options else None,
            &#39;extraction_template&#39;: self.extraction_template,
            &#39;extraction_ephemeral_template&#39;: self.extraction_ephemeral_template,
            &#39;extraction_prompt&#39;: self.extraction_prompt,
            &#39;extraction_model&#39;: self.extraction_model,
            &#39;correlation_id&#39;: self.correlation_id,
            &#39;cookies&#39;: CaseInsensitiveDict(self.cookies),
            &#39;body&#39;: self.body,
            &#39;data&#39;: None if self.body else self.data,
            &#39;headers&#39;: CaseInsensitiveDict(self.headers),
            &#39;js&#39;: self.js,
            &#39;rendering_wait&#39;: self.rendering_wait,
            &#39;wait_for_selector&#39;: self.wait_for_selector,
            &#39;session_sticky_proxy&#39;: self.session_sticky_proxy,
            &#39;screenshots&#39;: self.screenshots,
            &#39;screenshot_flags&#39;: [ScreenshotFlag(flag).value for flag in self.screenshot_flags] if self.screenshot_flags else None,
            &#39;webhook&#39;: self.webhook,
            &#39;timeout&#39;: self.timeout,
            &#39;js_scenario&#39;: self.js_scenario,
            &#39;extract&#39;: self.extract,
            &#39;lang&#39;: self.lang,
            &#39;os&#39;: self.os,
            &#39;auto_scroll&#39;: self.auto_scroll,
            &#39;cost_budget&#39;: self.cost_budget,
        }

    @staticmethod
    def from_dict(scrape_config_dict: Dict) -&gt; &#39;ScrapeConfig&#39;:
        &#34;&#34;&#34;Create a ScrapeConfig instance from a dictionary.&#34;&#34;&#34;
        url = scrape_config_dict.get(&#39;url&#39;, None)
        retry = scrape_config_dict.get(&#39;retry&#39;, False)
        method = scrape_config_dict.get(&#39;method&#39;, &#39;GET&#39;)
        country = scrape_config_dict.get(&#39;country&#39;, None)
        render_js = scrape_config_dict.get(&#39;render_js&#39;, False)
        cache = scrape_config_dict.get(&#39;cache&#39;, False)
        cache_clear = scrape_config_dict.get(&#39;cache_clear&#39;, False)
        ssl = scrape_config_dict.get(&#39;ssl&#39;, False)
        dns = scrape_config_dict.get(&#39;dns&#39;, False)
        asp = scrape_config_dict.get(&#39;asp&#39;, False)
        debug = scrape_config_dict.get(&#39;debug&#39;, False)
        raise_on_upstream_error = scrape_config_dict.get(&#39;raise_on_upstream_error&#39;, True)
        cache_ttl = scrape_config_dict.get(&#39;cache_ttl&#39;, None)
        proxy_pool = scrape_config_dict.get(&#39;proxy_pool&#39;, None)
        session = scrape_config_dict.get(&#39;session&#39;, None)
        tags = scrape_config_dict.get(&#39;tags&#39;, [])

        format = scrape_config_dict.get(&#39;format&#39;, None)
        format = Format(format) if format else None

        format_options = scrape_config_dict.get(&#39;format_options&#39;, None)
        format_options = [FormatOption(option) for option in format_options] if format_options else None

        extraction_template = scrape_config_dict.get(&#39;extraction_template&#39;, None)
        extraction_ephemeral_template = scrape_config_dict.get(&#39;extraction_ephemeral_template&#39;, None)
        extraction_prompt = scrape_config_dict.get(&#39;extraction_prompt&#39;, None)
        extraction_model = scrape_config_dict.get(&#39;extraction_model&#39;, None)
        correlation_id = scrape_config_dict.get(&#39;correlation_id&#39;, None)
        cookies = scrape_config_dict.get(&#39;cookies&#39;, {})
        body = scrape_config_dict.get(&#39;body&#39;, None)
        data = scrape_config_dict.get(&#39;data&#39;, None)
        headers = scrape_config_dict.get(&#39;headers&#39;, {})
        js = scrape_config_dict.get(&#39;js&#39;, None)
        rendering_wait = scrape_config_dict.get(&#39;rendering_wait&#39;, None)
        wait_for_selector = scrape_config_dict.get(&#39;wait_for_selector&#39;, None)
        screenshots = scrape_config_dict.get(&#39;screenshots&#39;, [])
        
        screenshot_flags = scrape_config_dict.get(&#39;screenshot_flags&#39;, [])
        screenshot_flags = [ScreenshotFlag(flag) for flag in screenshot_flags] if screenshot_flags else None

        session_sticky_proxy = scrape_config_dict.get(&#39;session_sticky_proxy&#39;, False)
        webhook = scrape_config_dict.get(&#39;webhook&#39;, None)
        timeout = scrape_config_dict.get(&#39;timeout&#39;, None)
        js_scenario = scrape_config_dict.get(&#39;js_scenario&#39;, None)
        extract = scrape_config_dict.get(&#39;extract&#39;, None)
        os = scrape_config_dict.get(&#39;os&#39;, None)
        lang = scrape_config_dict.get(&#39;lang&#39;, None)
        auto_scroll = scrape_config_dict.get(&#39;auto_scroll&#39;, None)
        cost_budget = scrape_config_dict.get(&#39;cost_budget&#39;, None)

        return ScrapeConfig(
            url=url,
            retry=retry,
            method=method,
            country=country,
            render_js=render_js,
            cache=cache,
            cache_clear=cache_clear,
            ssl=ssl,
            dns=dns,
            asp=asp,
            debug=debug,
            raise_on_upstream_error=raise_on_upstream_error,
            cache_ttl=cache_ttl,
            proxy_pool=proxy_pool,
            session=session,
            tags=tags,
            format=format,
            format_options=format_options,
            extraction_template=extraction_template,
            extraction_ephemeral_template=extraction_ephemeral_template,
            extraction_prompt=extraction_prompt,
            extraction_model=extraction_model,
            correlation_id=correlation_id,
            cookies=cookies,
            body=body,
            data=data,
            headers=headers,
            js=js,
            rendering_wait=rendering_wait,
            wait_for_selector=wait_for_selector,
            screenshots=screenshots,
            screenshot_flags=screenshot_flags,
            session_sticky_proxy=session_sticky_proxy,
            webhook=webhook,
            timeout=timeout,
            js_scenario=js_scenario,
            extract=extract,
            os=os,
            lang=lang,
            auto_scroll=auto_scroll,
            cost_budget=cost_budget,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_config.BaseApiConfig" href="api_config.html#scrapfly.api_config.BaseApiConfig">BaseApiConfig</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScrapeConfig.PUBLIC_DATACENTER_POOL"><code class="name">var <span class="ident">PUBLIC_DATACENTER_POOL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.PUBLIC_RESIDENTIAL_POOL"><code class="name">var <span class="ident">PUBLIC_RESIDENTIAL_POOL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.asp"><code class="name">var <span class="ident">asp</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.auto_scroll"><code class="name">var <span class="ident">auto_scroll</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.body"><code class="name">var <span class="ident">body</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.cache"><code class="name">var <span class="ident">cache</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.cache_clear"><code class="name">var <span class="ident">cache_clear</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.cache_ttl"><code class="name">var <span class="ident">cache_ttl</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.cookies"><code class="name">var <span class="ident">cookies</span> : Optional[requests.structures.CaseInsensitiveDict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.correlation_id"><code class="name">var <span class="ident">correlation_id</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.cost_budget"><code class="name">var <span class="ident">cost_budget</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.country"><code class="name">var <span class="ident">country</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.data"><code class="name">var <span class="ident">data</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.debug"><code class="name">var <span class="ident">debug</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.dns"><code class="name">var <span class="ident">dns</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.extract"><code class="name">var <span class="ident">extract</span> : Dict</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.extraction_ephemeral_template"><code class="name">var <span class="ident">extraction_ephemeral_template</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.extraction_model"><code class="name">var <span class="ident">extraction_model</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.extraction_prompt"><code class="name">var <span class="ident">extraction_prompt</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.extraction_template"><code class="name">var <span class="ident">extraction_template</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.format"><code class="name">var <span class="ident">format</span> : Optional[<a title="scrapfly.scrape_config.Format" href="scrape_config.html#scrapfly.scrape_config.Format">Format</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.format_options"><code class="name">var <span class="ident">format_options</span> : Optional[List[<a title="scrapfly.scrape_config.FormatOption" href="scrape_config.html#scrapfly.scrape_config.FormatOption">FormatOption</a>]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.headers"><code class="name">var <span class="ident">headers</span> : Optional[requests.structures.CaseInsensitiveDict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.js"><code class="name">var <span class="ident">js</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.js_scenario"><code class="name">var <span class="ident">js_scenario</span> : Dict</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.lang"><code class="name">var <span class="ident">lang</span> : Optional[List[str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.method"><code class="name">var <span class="ident">method</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.os"><code class="name">var <span class="ident">os</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.proxy_pool"><code class="name">var <span class="ident">proxy_pool</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.raise_on_upstream_error"><code class="name">var <span class="ident">raise_on_upstream_error</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.render_js"><code class="name">var <span class="ident">render_js</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.rendering_wait"><code class="name">var <span class="ident">rendering_wait</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.retry"><code class="name">var <span class="ident">retry</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.screenshot_flags"><code class="name">var <span class="ident">screenshot_flags</span> : Optional[List[<a title="scrapfly.scrape_config.ScreenshotFlag" href="scrape_config.html#scrapfly.scrape_config.ScreenshotFlag">ScreenshotFlag</a>]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.screenshots"><code class="name">var <span class="ident">screenshots</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.session"><code class="name">var <span class="ident">session</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.session_sticky_proxy"><code class="name">var <span class="ident">session_sticky_proxy</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.ssl"><code class="name">var <span class="ident">ssl</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.tags"><code class="name">var <span class="ident">tags</span> : Optional[List[str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.timeout"><code class="name">var <span class="ident">timeout</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.url"><code class="name">var <span class="ident">url</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.wait_for_selector"><code class="name">var <span class="ident">wait_for_selector</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapeConfig.webhook"><code class="name">var <span class="ident">webhook</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scrapfly.ScrapeConfig.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>scrape_config_dict: Dict) ‑> <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a ScrapeConfig instance from a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(scrape_config_dict: Dict) -&gt; &#39;ScrapeConfig&#39;:
    &#34;&#34;&#34;Create a ScrapeConfig instance from a dictionary.&#34;&#34;&#34;
    url = scrape_config_dict.get(&#39;url&#39;, None)
    retry = scrape_config_dict.get(&#39;retry&#39;, False)
    method = scrape_config_dict.get(&#39;method&#39;, &#39;GET&#39;)
    country = scrape_config_dict.get(&#39;country&#39;, None)
    render_js = scrape_config_dict.get(&#39;render_js&#39;, False)
    cache = scrape_config_dict.get(&#39;cache&#39;, False)
    cache_clear = scrape_config_dict.get(&#39;cache_clear&#39;, False)
    ssl = scrape_config_dict.get(&#39;ssl&#39;, False)
    dns = scrape_config_dict.get(&#39;dns&#39;, False)
    asp = scrape_config_dict.get(&#39;asp&#39;, False)
    debug = scrape_config_dict.get(&#39;debug&#39;, False)
    raise_on_upstream_error = scrape_config_dict.get(&#39;raise_on_upstream_error&#39;, True)
    cache_ttl = scrape_config_dict.get(&#39;cache_ttl&#39;, None)
    proxy_pool = scrape_config_dict.get(&#39;proxy_pool&#39;, None)
    session = scrape_config_dict.get(&#39;session&#39;, None)
    tags = scrape_config_dict.get(&#39;tags&#39;, [])

    format = scrape_config_dict.get(&#39;format&#39;, None)
    format = Format(format) if format else None

    format_options = scrape_config_dict.get(&#39;format_options&#39;, None)
    format_options = [FormatOption(option) for option in format_options] if format_options else None

    extraction_template = scrape_config_dict.get(&#39;extraction_template&#39;, None)
    extraction_ephemeral_template = scrape_config_dict.get(&#39;extraction_ephemeral_template&#39;, None)
    extraction_prompt = scrape_config_dict.get(&#39;extraction_prompt&#39;, None)
    extraction_model = scrape_config_dict.get(&#39;extraction_model&#39;, None)
    correlation_id = scrape_config_dict.get(&#39;correlation_id&#39;, None)
    cookies = scrape_config_dict.get(&#39;cookies&#39;, {})
    body = scrape_config_dict.get(&#39;body&#39;, None)
    data = scrape_config_dict.get(&#39;data&#39;, None)
    headers = scrape_config_dict.get(&#39;headers&#39;, {})
    js = scrape_config_dict.get(&#39;js&#39;, None)
    rendering_wait = scrape_config_dict.get(&#39;rendering_wait&#39;, None)
    wait_for_selector = scrape_config_dict.get(&#39;wait_for_selector&#39;, None)
    screenshots = scrape_config_dict.get(&#39;screenshots&#39;, [])
    
    screenshot_flags = scrape_config_dict.get(&#39;screenshot_flags&#39;, [])
    screenshot_flags = [ScreenshotFlag(flag) for flag in screenshot_flags] if screenshot_flags else None

    session_sticky_proxy = scrape_config_dict.get(&#39;session_sticky_proxy&#39;, False)
    webhook = scrape_config_dict.get(&#39;webhook&#39;, None)
    timeout = scrape_config_dict.get(&#39;timeout&#39;, None)
    js_scenario = scrape_config_dict.get(&#39;js_scenario&#39;, None)
    extract = scrape_config_dict.get(&#39;extract&#39;, None)
    os = scrape_config_dict.get(&#39;os&#39;, None)
    lang = scrape_config_dict.get(&#39;lang&#39;, None)
    auto_scroll = scrape_config_dict.get(&#39;auto_scroll&#39;, None)
    cost_budget = scrape_config_dict.get(&#39;cost_budget&#39;, None)

    return ScrapeConfig(
        url=url,
        retry=retry,
        method=method,
        country=country,
        render_js=render_js,
        cache=cache,
        cache_clear=cache_clear,
        ssl=ssl,
        dns=dns,
        asp=asp,
        debug=debug,
        raise_on_upstream_error=raise_on_upstream_error,
        cache_ttl=cache_ttl,
        proxy_pool=proxy_pool,
        session=session,
        tags=tags,
        format=format,
        format_options=format_options,
        extraction_template=extraction_template,
        extraction_ephemeral_template=extraction_ephemeral_template,
        extraction_prompt=extraction_prompt,
        extraction_model=extraction_model,
        correlation_id=correlation_id,
        cookies=cookies,
        body=body,
        data=data,
        headers=headers,
        js=js,
        rendering_wait=rendering_wait,
        wait_for_selector=wait_for_selector,
        screenshots=screenshots,
        screenshot_flags=screenshot_flags,
        session_sticky_proxy=session_sticky_proxy,
        webhook=webhook,
        timeout=timeout,
        js_scenario=js_scenario,
        extract=extract,
        os=os,
        lang=lang,
        auto_scroll=auto_scroll,
        cost_budget=cost_budget,
    )</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeConfig.from_exported_config"><code class="name flex">
<span>def <span class="ident">from_exported_config</span></span>(<span>config: str) ‑> <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_exported_config(config:str) -&gt; &#39;ScrapeConfig&#39;:
    try:
        from msgpack import loads as msgpack_loads
    except ImportError as e:
        print(&#39;You must install msgpack package - run: pip install &#34;scrapfly-sdk[seepdup] or pip install msgpack&#39;)
        raise

    data = msgpack_loads(base64.b64decode(config))

    headers = {}

    for name, value in data[&#39;headers&#39;].items():
        if isinstance(value, Iterable):
            headers[name] = &#39;; &#39;.join(value)
        else:
            headers[name] = value

    return ScrapeConfig(
        url=data[&#39;url&#39;],
        retry=data[&#39;retry&#39;],
        headers=headers,
        session=data[&#39;session&#39;],
        session_sticky_proxy=data[&#39;session_sticky_proxy&#39;],
        cache=data[&#39;cache&#39;],
        cache_ttl=data[&#39;cache_ttl&#39;],
        cache_clear=data[&#39;cache_clear&#39;],
        render_js=data[&#39;render_js&#39;],
        method=data[&#39;method&#39;],
        asp=data[&#39;asp&#39;],
        body=data[&#39;body&#39;],
        ssl=data[&#39;ssl&#39;],
        dns=data[&#39;dns&#39;],
        country=data[&#39;country&#39;],
        debug=data[&#39;debug&#39;],
        correlation_id=data[&#39;correlation_id&#39;],
        tags=data[&#39;tags&#39;],
        format=data[&#39;format&#39;],
        js=data[&#39;js&#39;],
        rendering_wait=data[&#39;rendering_wait&#39;],
        screenshots=data[&#39;screenshots&#39;] or {},
        screenshot_flags=data[&#39;screenshot_flags&#39;],
        proxy_pool=data[&#39;proxy_pool&#39;],
        auto_scroll=data[&#39;auto_scroll&#39;],
        cost_budget=data[&#39;cost_budget&#39;]
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ScrapeConfig.to_api_params"><code class="name flex">
<span>def <span class="ident">to_api_params</span></span>(<span>self, key: str) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_api_params(self, key:str) -&gt; Dict:
    params = {
        &#39;key&#39;: self.key or key,
        &#39;url&#39;: self.url
    }

    if self.country is not None:
        params[&#39;country&#39;] = self.country

    for name, value in self.headers.items():
        params[&#39;headers[%s]&#39; % name] = value

    if self.webhook is not None:
        params[&#39;webhook_name&#39;] = self.webhook

    if self.timeout is not None:
        params[&#39;timeout&#39;] = self.timeout

    if self.extract is not None:
        params[&#39;extract&#39;] = base64.urlsafe_b64encode(json.dumps(self.extract).encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

    if self.cost_budget is not None:
        params[&#39;cost_budget&#39;] = self.cost_budget

    if self.render_js is True:
        params[&#39;render_js&#39;] = self._bool_to_http(self.render_js)

        if self.wait_for_selector is not None:
            params[&#39;wait_for_selector&#39;] = self.wait_for_selector

        if self.js:
            params[&#39;js&#39;] = base64.urlsafe_b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.js_scenario:
            params[&#39;js_scenario&#39;] = base64.urlsafe_b64encode(json.dumps(self.js_scenario).encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.rendering_wait:
            params[&#39;rendering_wait&#39;] = self.rendering_wait

        if self.screenshots is not None:
            for name, element in self.screenshots.items():
                params[&#39;screenshots[%s]&#39; % name] = element

        if self.screenshot_flags is not None:
            self.screenshot_flags = [ScreenshotFlag(flag) for flag in self.screenshot_flags]
            params[&#34;screenshot_flags&#34;] = &#34;,&#34;.join(flag.value for flag in self.screenshot_flags)
        else:
            if self.screenshot_flags is not None:
                logging.warning(&#39;Params &#34;screenshot_flags&#34; is ignored. Works only if screenshots is enabled&#39;)

        if self.auto_scroll is True:
            params[&#39;auto_scroll&#39;] = self._bool_to_http(self.auto_scroll)
    else:
        if self.wait_for_selector is not None:
            logging.warning(&#39;Params &#34;wait_for_selector&#34; is ignored. Works only if render_js is enabled&#39;)

        if self.screenshots:
            logging.warning(&#39;Params &#34;screenshots&#34; is ignored. Works only if render_js is enabled&#39;)

        if self.js_scenario:
            logging.warning(&#39;Params &#34;js_scenario&#34; is ignored. Works only if render_js is enabled&#39;)

        if self.js:
            logging.warning(&#39;Params &#34;js&#34; is ignored. Works only if render_js is enabled&#39;)

        if self.rendering_wait:
            logging.warning(&#39;Params &#34;rendering_wait&#34; is ignored. Works only if render_js is enabled&#39;)

    if self.asp is True:
        params[&#39;asp&#39;] = self._bool_to_http(self.asp)

    if self.retry is False:
        params[&#39;retry&#39;] = self._bool_to_http(self.retry)

    if self.cache is True:
        params[&#39;cache&#39;] = self._bool_to_http(self.cache)

        if self.cache_clear is True:
            params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

        if self.cache_ttl is not None:
            params[&#39;cache_ttl&#39;] = self.cache_ttl
    else:
        if self.cache_clear is True:
            logging.warning(&#39;Params &#34;cache_clear&#34; is ignored. Works only if cache is enabled&#39;)

        if self.cache_ttl is not None:
            logging.warning(&#39;Params &#34;cache_ttl&#34; is ignored. Works only if cache is enabled&#39;)

    if self.dns is True:
        params[&#39;dns&#39;] = self._bool_to_http(self.dns)

    if self.ssl is True:
        params[&#39;ssl&#39;] = self._bool_to_http(self.ssl)

    if self.tags:
        params[&#39;tags&#39;] = &#39;,&#39;.join(self.tags)

    if self.format:
        params[&#39;format&#39;] = Format(self.format).value
        if self.format_options:
            params[&#39;format&#39;] += &#39;:&#39; + &#39;,&#39;.join(FormatOption(option).value for option in self.format_options)

    if self.extraction_template and self.extraction_ephemeral_template:
        raise ScrapeConfigError(&#39;You cannot pass both parameters extraction_template and extraction_ephemeral_template. You must choose&#39;)

    if self.extraction_template:
        params[&#39;extraction_template&#39;] = self.extraction_template

    if self.extraction_ephemeral_template:
        self.extraction_ephemeral_template = json.dumps(self.extraction_ephemeral_template)
        params[&#39;extraction_template&#39;] = &#39;ephemeral:&#39; + urlsafe_b64encode(self.extraction_ephemeral_template.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

    if self.extraction_prompt:
        params[&#39;extraction_prompt&#39;] = quote_plus(self.extraction_prompt)

    if self.extraction_model:
        params[&#39;extraction_model&#39;] = self.extraction_model

    if self.correlation_id:
        params[&#39;correlation_id&#39;] = self.correlation_id

    if self.session:
        params[&#39;session&#39;] = self.session

        if self.session_sticky_proxy is True: # false by default
            params[&#39;session_sticky_proxy&#39;] = self._bool_to_http(self.session_sticky_proxy)
    else:
        if self.session_sticky_proxy:
            logging.warning(&#39;Params &#34;session_sticky_proxy&#34; is ignored. Works only if session is enabled&#39;)

    if self.debug is True:
        params[&#39;debug&#39;] = self._bool_to_http(self.debug)

    if self.proxy_pool is not None:
        params[&#39;proxy_pool&#39;] = self.proxy_pool

    if self.lang is not None:
        params[&#39;lang&#39;] = &#39;,&#39;.join(self.lang)

    if self.os is not None:
        params[&#39;os&#39;] = self.os

    return params</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapeConfig.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Export the ScrapeConfig instance to a plain dictionary.
Useful for JSON-serialization or other external storage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; Dict:
    &#34;&#34;&#34;
    Export the ScrapeConfig instance to a plain dictionary. 
    Useful for JSON-serialization or other external storage.
    &#34;&#34;&#34;
    
    return {
        &#39;url&#39;: self.url,
        &#39;retry&#39;: self.retry,
        &#39;method&#39;: self.method,
        &#39;country&#39;: self.country,
        &#39;render_js&#39;: self.render_js,
        &#39;cache&#39;: self.cache,
        &#39;cache_clear&#39;: self.cache_clear,
        &#39;ssl&#39;: self.ssl,
        &#39;dns&#39;: self.dns,
        &#39;asp&#39;: self.asp,
        &#39;debug&#39;: self.debug,
        &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error,
        &#39;cache_ttl&#39;: self.cache_ttl,
        &#39;proxy_pool&#39;: self.proxy_pool,
        &#39;session&#39;: self.session,
        &#39;tags&#39;: list(self.tags),
        &#39;format&#39;: Format(self.format).value if self.format else None,
        &#39;format_options&#39;: [FormatOption(option).value for option in self.format_options] if self.format_options else None,
        &#39;extraction_template&#39;: self.extraction_template,
        &#39;extraction_ephemeral_template&#39;: self.extraction_ephemeral_template,
        &#39;extraction_prompt&#39;: self.extraction_prompt,
        &#39;extraction_model&#39;: self.extraction_model,
        &#39;correlation_id&#39;: self.correlation_id,
        &#39;cookies&#39;: CaseInsensitiveDict(self.cookies),
        &#39;body&#39;: self.body,
        &#39;data&#39;: None if self.body else self.data,
        &#39;headers&#39;: CaseInsensitiveDict(self.headers),
        &#39;js&#39;: self.js,
        &#39;rendering_wait&#39;: self.rendering_wait,
        &#39;wait_for_selector&#39;: self.wait_for_selector,
        &#39;session_sticky_proxy&#39;: self.session_sticky_proxy,
        &#39;screenshots&#39;: self.screenshots,
        &#39;screenshot_flags&#39;: [ScreenshotFlag(flag).value for flag in self.screenshot_flags] if self.screenshot_flags else None,
        &#39;webhook&#39;: self.webhook,
        &#39;timeout&#39;: self.timeout,
        &#39;js_scenario&#39;: self.js_scenario,
        &#39;extract&#39;: self.extract,
        &#39;lang&#39;: self.lang,
        &#39;os&#39;: self.os,
        &#39;auto_scroll&#39;: self.auto_scroll,
        &#39;cost_budget&#39;: self.cost_budget,
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.ScraperAPI"><code class="flex name class">
<span>class <span class="ident">ScraperAPI</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScraperAPI:

    MONITORING_DATA_FORMAT_STRUCTURED = &#39;structured&#39;
    MONITORING_DATA_FORMAT_PROMETHEUS = &#39;prometheus&#39;

    MONITORING_PERIOD_SUBSCRIPTION = &#39;subscription&#39;
    MONITORING_PERIOD_LAST_7D = &#39;last7d&#39;
    MONITORING_PERIOD_LAST_24H = &#39;last24h&#39;
    MONITORING_PERIOD_LAST_1H = &#39;last1h&#39;
    MONITORING_PERIOD_LAST_5m = &#39;last5m&#39;

    MONITORING_ACCOUNT_AGGREGATION = &#39;account&#39;
    MONITORING_PROJECT_AGGREGATION = &#39;project&#39;
    MONITORING_TARGET_AGGREGATION = &#39;target&#39;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScraperAPI.MONITORING_ACCOUNT_AGGREGATION"><code class="name">var <span class="ident">MONITORING_ACCOUNT_AGGREGATION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_PROMETHEUS"><code class="name">var <span class="ident">MONITORING_DATA_FORMAT_PROMETHEUS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_STRUCTURED"><code class="name">var <span class="ident">MONITORING_DATA_FORMAT_STRUCTURED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_1H"><code class="name">var <span class="ident">MONITORING_PERIOD_LAST_1H</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_24H"><code class="name">var <span class="ident">MONITORING_PERIOD_LAST_24H</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_5m"><code class="name">var <span class="ident">MONITORING_PERIOD_LAST_5m</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_7D"><code class="name">var <span class="ident">MONITORING_PERIOD_LAST_7D</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PERIOD_SUBSCRIPTION"><code class="name">var <span class="ident">MONITORING_PERIOD_SUBSCRIPTION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_PROJECT_AGGREGATION"><code class="name">var <span class="ident">MONITORING_PROJECT_AGGREGATION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScraperAPI.MONITORING_TARGET_AGGREGATION"><code class="name">var <span class="ident">MONITORING_TARGET_AGGREGATION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="scrapfly.ScrapflyAspError"><code class="flex name class">
<span>class <span class="ident">ScrapflyAspError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyAspError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflyClient"><code class="flex name class">
<span>class <span class="ident">ScrapflyClient</span></span>
<span>(</span><span>key: str, host: Optional[str] = 'https://api.scrapfly.io', verify=True, debug: bool = False, max_concurrency: int = 1, connect_timeout: int = 30, web_scraping_api_read_timeout: int = 160, extraction_api_read_timeout: int = 35, screenshot_api_read_timeout: int = 60, read_timeout: int = 30, default_read_timeout: int = 30, reporter: Optional[Callable] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyClient:

    HOST = &#39;https://api.scrapfly.io&#39;
    DEFAULT_CONNECT_TIMEOUT = 30
    DEFAULT_READ_TIMEOUT = 30

    DEFAULT_WEBSCRAPING_API_READ_TIMEOUT = 160 # 155 real
    DEFAULT_SCREENSHOT_API_READ_TIMEOUT = 60  # 30 real
    DEFAULT_EXTRACTION_API_READ_TIMEOUT = 35 # 30 real

    host:str
    key:str
    max_concurrency:int
    verify:bool
    debug:bool
    distributed_mode:bool
    connect_timeout:int
    web_scraping_api_read_timeout:int
    screenshot_api_read_timeout:int
    extraction_api_read_timeout:int
    monitoring_api_read_timeout:int
    default_read_timeout:int
    brotli: bool
    reporter:Reporter
    version:str

    # @deprecated
    read_timeout:int

    CONCURRENCY_AUTO = &#39;auto&#39; # retrieve the allowed concurrency from your account
    DATETIME_FORMAT = &#39;%Y-%m-%d %H:%M:%S&#39;

    def __init__(
        self,
        key: str,
        host: Optional[str] = HOST,
        verify=True,
        debug: bool = False,
        max_concurrency:int=1,
        connect_timeout:int = DEFAULT_CONNECT_TIMEOUT,
        web_scraping_api_read_timeout: int = DEFAULT_WEBSCRAPING_API_READ_TIMEOUT,
        extraction_api_read_timeout: int = DEFAULT_EXTRACTION_API_READ_TIMEOUT,
        screenshot_api_read_timeout: int = DEFAULT_SCREENSHOT_API_READ_TIMEOUT,

        # @deprecated
        read_timeout:int = DEFAULT_READ_TIMEOUT,
        default_read_timeout:int = DEFAULT_READ_TIMEOUT,
        reporter:Optional[Callable]=None,
        **kwargs
    ):
        if host[-1] == &#39;/&#39;:  # remove last &#39;/&#39; if exists
            host = host[:-1]

        if &#39;distributed_mode&#39; in kwargs:
            warnings.warn(&#34;distributed mode is deprecated and will be remove the next version -&#34;
              &#34; user should handle themself the session name based on the concurrency&#34;,
              DeprecationWarning,
              stacklevel=2
            )

        if &#39;brotli&#39; in kwargs:
            warnings.warn(&#34;brotli arg is deprecated and will be remove the next version - &#34;
                &#34;brotli is disabled by default&#34;,
                DeprecationWarning,
                stacklevel=2
            )

        self.version = __version__
        self.host = host
        self.key = key
        self.verify = verify
        self.debug = debug
        self.connect_timeout = connect_timeout
        self.web_scraping_api_read_timeout = web_scraping_api_read_timeout
        self.screenshot_api_read_timeout = screenshot_api_read_timeout
        self.extraction_api_read_timeout = extraction_api_read_timeout
        self.monitoring_api_read_timeout = default_read_timeout
        self.default_read_timeout = default_read_timeout

        # @deprecated
        self.read_timeout = default_read_timeout

        self.max_concurrency = max_concurrency
        self.body_handler = ResponseBodyHandler(use_brotli=False)
        self.async_executor = ThreadPoolExecutor()
        self.http_session = None

        if not self.verify and not self.HOST.endswith(&#39;.local&#39;):
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        if self.debug is True:
            http.client.HTTPConnection.debuglevel = 5

        if reporter is None:
            from .reporter import NoopReporter

            reporter = NoopReporter()

        self.reporter = Reporter(reporter)

    @property
    def ua(self) -&gt; str:
        return &#39;ScrapflySDK/%s (Python %s, %s, %s)&#39; % (
            self.version,
            platform.python_version(),
            platform.uname().system,
            platform.uname().machine
        )

    @cached_property
    def _http_handler(self):
        return partial(self.http_session.request if self.http_session else requests.request)

    @property
    def http(self):
        return self._http_handler

    def _scrape_request(self, scrape_config:ScrapeConfig):
        return {
            &#39;method&#39;: scrape_config.method,
            &#39;url&#39;: self.host + &#39;/scrape&#39;,
            &#39;data&#39;: scrape_config.body,
            &#39;verify&#39;: self.verify,
            &#39;timeout&#39;: (self.connect_timeout, self.web_scraping_api_read_timeout),
            &#39;headers&#39;: {
                &#39;content-type&#39;: scrape_config.headers[&#39;content-type&#39;] if scrape_config.method in [&#39;POST&#39;, &#39;PUT&#39;, &#39;PATCH&#39;] else self.body_handler.content_type,
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },
            &#39;params&#39;: scrape_config.to_api_params(key=self.key)
        }
    
    def _screenshot_request(self, screenshot_config:ScreenshotConfig):
        return {
            &#39;method&#39;: &#39;GET&#39;,
            &#39;url&#39;: self.host + &#39;/screenshot&#39;,
            &#39;timeout&#39;: (self.connect_timeout, self.screenshot_api_read_timeout),
            &#39;headers&#39;: {
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },            
            &#39;params&#39;: screenshot_config.to_api_params(key=self.key)
        }        

    def _extraction_request(self, extraction_config:ExtractionConfig):
        headers = {
                &#39;content-type&#39;: extraction_config.content_type,
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;content-encoding&#39;: extraction_config.document_compression_format if extraction_config.document_compression_format else None,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
        }

        if extraction_config.document_compression_format:
            headers[&#39;content-encoding&#39;] = extraction_config.document_compression_format.value

        return {
            &#39;method&#39;: &#39;POST&#39;,
            &#39;url&#39;: self.host + &#39;/extraction&#39;,
            &#39;data&#39;: extraction_config.body,
            &#39;timeout&#39;: (self.connect_timeout, self.extraction_api_read_timeout),
            &#39;headers&#39;: headers,
            &#39;params&#39;: extraction_config.to_api_params(key=self.key)
        }


    def account(self) -&gt; Union[str, Dict]:
        response = self._http_handler(
            method=&#39;GET&#39;,
            url=self.host + &#39;/account&#39;,
            params={&#39;key&#39;: self.key},
            verify=self.verify,
            headers={
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },
        )

        response.raise_for_status()

        if self.body_handler.support(response.headers):
            return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

        return response.content.decode(&#39;utf-8&#39;)

    def get_monitoring_metrics(self, format:str=ScraperAPI.MONITORING_DATA_FORMAT_STRUCTURED, period:Optional[str]=None, aggregation:Optional[List[MonitoringAggregation]]=None):
        params = {&#39;key&#39;: self.key, &#39;format&#39;: format}

        if period is not None:
            params[&#39;period&#39;] = period

        if aggregation is not None:
            params[&#39;aggregation&#39;] = &#39;,&#39;.join(aggregation)

        response = self._http_handler(
            method=&#39;GET&#39;,
            url=self.host + &#39;/scrape/monitoring/metrics&#39;,
            params=params,
            timeout=(self.connect_timeout, self.monitoring_api_read_timeout),
            verify=self.verify,
            headers={
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },
        )

        response.raise_for_status()

        if self.body_handler.support(response.headers):
            return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

        return response.content.decode(&#39;utf-8&#39;)

    def get_monitoring_target_metrics(
            self,
            domain:str,
            group_subdomain:bool=False,
            period:Optional[MonitoringTargetPeriod]=ScraperAPI.MONITORING_PERIOD_LAST_24H,
            start:Optional[datetime.datetime]=None,
            end:Optional[datetime.datetime]=None,
    ):
        params = {
            &#39;key&#39;: self.key,
            &#39;domain&#39;: domain,
            &#39;group_subdomain&#39;: group_subdomain
        }

        if (start is not None and end is None) or (start is None and end is not None):
            raise ValueError(&#39;You must provide both start and end date&#39;)

        if start is not None and end is not None:
            params[&#39;start&#39;] = start.strftime(self.DATETIME_FORMAT)
            params[&#39;end&#39;] = end.strftime(self.DATETIME_FORMAT)
            period = None

        params[&#39;period&#39;] = period

        response = self._http_handler(
            method=&#39;GET&#39;,
            url=self.host + &#39;/scrape/monitoring/metrics/target&#39;,
            timeout=(self.connect_timeout, self.monitoring_api_read_timeout),
            params=params,
            verify=self.verify,
            headers={
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },
        )

        response.raise_for_status()

        if self.body_handler.support(response.headers):
            return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

        return response.content.decode(&#39;utf-8&#39;)


    def resilient_scrape(
        self,
        scrape_config:ScrapeConfig,
        retry_on_errors:Set[Exception]={ScrapflyError},
        retry_on_status_code:Optional[List[int]]=None,
        tries: int = 5,
        delay: int = 20,
    ) -&gt; ScrapeApiResponse:
        assert retry_on_errors is not None, &#39;Retry on error is None&#39;
        assert isinstance(retry_on_errors, set), &#39;retry_on_errors is not a set()&#39;

        @backoff.on_exception(backoff.expo, exception=tuple(retry_on_errors), max_tries=tries, max_time=delay)
        def inner() -&gt; ScrapeApiResponse:

            try:
                return self.scrape(scrape_config=scrape_config)
            except (UpstreamHttpClientError, UpstreamHttpServerError) as e:
                if retry_on_status_code is not None and e.api_response:
                    if e.api_response.upstream_status_code in retry_on_status_code:
                        raise e
                    else:
                        return e.api_response

                raise e

        return inner()

    def open(self):
        if self.http_session is None:
            self.http_session = Session()
            self.http_session.verify = self.verify
            self.http_session.timeout = (self.connect_timeout, self.default_read_timeout)
            self.http_session.params[&#39;key&#39;] = self.key
            self.http_session.headers[&#39;accept-encoding&#39;] = self.body_handler.content_encoding
            self.http_session.headers[&#39;accept&#39;] = self.body_handler.accept
            self.http_session.headers[&#39;user-agent&#39;] = self.ua

    def close(self):
        self.http_session.close()
        self.http_session = None

    def __enter__(self) -&gt; &#39;ScrapflyClient&#39;:
        self.open()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    async def async_scrape(self, scrape_config:ScrapeConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ScrapeApiResponse:
        if loop is None:
            loop = asyncio.get_running_loop()

        return await loop.run_in_executor(self.async_executor, self.scrape, scrape_config)

    async def concurrent_scrape(self, scrape_configs:List[ScrapeConfig], concurrency:Optional[int]=None):
        if concurrency is None:
            concurrency = self.max_concurrency
        elif concurrency == self.CONCURRENCY_AUTO:
            concurrency = self.account()[&#39;subscription&#39;][&#39;max_concurrency&#39;]

        loop = asyncio.get_running_loop()
        processing_tasks = []
        results = []
        processed_tasks = 0
        expected_tasks = len(scrape_configs)

        def scrape_done_callback(task:Task):
            nonlocal processed_tasks

            try:
                if task.cancelled() is True:
                    return

                error = task.exception()

                if error is not None:
                    results.append(error)
                else:
                    results.append(task.result())
            finally:
                processing_tasks.remove(task)
                processed_tasks += 1

        while scrape_configs or results or processing_tasks:
            logger.info(&#34;Scrape %d/%d - %d running&#34; % (processed_tasks, expected_tasks, len(processing_tasks)))

            if scrape_configs:
                if len(processing_tasks) &lt; concurrency:
                    # @todo handle backpressure
                    for _ in range(0, concurrency - len(processing_tasks)):
                        try:
                            scrape_config = scrape_configs.pop()
                        except:
                            break

                        scrape_config.raise_on_upstream_error = False
                        task = loop.create_task(self.async_scrape(scrape_config=scrape_config, loop=loop))
                        processing_tasks.append(task)
                        task.add_done_callback(scrape_done_callback)

            for _ in results:
                result = results.pop()
                yield result

            await asyncio.sleep(.5)

        logger.debug(&#34;Scrape %d/%d - %d running&#34; % (processed_tasks, expected_tasks, len(processing_tasks)))

    @backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
    def scrape(self, scrape_config:ScrapeConfig, no_raise:bool=False) -&gt; ScrapeApiResponse:
        &#34;&#34;&#34;
        Scrape a website
        :param scrape_config: ScrapeConfig
        :param no_raise: bool - if True, do not raise exception on error while the api response is a ScrapflyError for seamless integration
        :return: ScrapeApiResponse

        If you use no_raise=True, make sure to check the api_response.scrape_result.error attribute to handle the error.
        If the error is not none, you will get the following structure for example

        &#39;error&#39;: {
            &#39;code&#39;: &#39;ERR::ASP::SHIELD_PROTECTION_FAILED&#39;,
            &#39;message&#39;: &#39;The ASP shield failed to solve the challenge against the anti scrapping protection - heuristic_engine bypass failed, please retry in few seconds&#39;,
            &#39;retryable&#39;: False,
            &#39;http_code&#39;: 422,
            &#39;links&#39;: {
                &#39;Checkout ASP documentation&#39;: &#39;https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate&#39;, &#39;Related Error Doc&#39;: &#39;https://scrapfly.io/docs/scrape-api/error/ERR::ASP::SHIELD_PROTECTION_FAILED&#39;
            }
        }
        &#34;&#34;&#34;

        try:
            logger.debug(&#39;--&gt; %s Scrapping %s&#39; % (scrape_config.method, scrape_config.url))
            request_data = self._scrape_request(scrape_config=scrape_config)
            response = self._http_handler(**request_data)
            scrape_api_response = self._handle_response(response=response, scrape_config=scrape_config)

            self.reporter.report(scrape_api_response=scrape_api_response)

            return scrape_api_response
        except BaseException as e:
            self.reporter.report(error=e)

            if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
                return e.api_response

            raise e

    async def async_screenshot(self, screenshot_config:ScreenshotConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ScreenshotApiResponse:
        if loop is None:
            loop = asyncio.get_running_loop()

        return await loop.run_in_executor(self.async_executor, self.screenshot, screenshot_config)

    @backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
    def screenshot(self, screenshot_config:ScreenshotConfig, no_raise:bool=False) -&gt; ScreenshotApiResponse:
        &#34;&#34;&#34;
        Take a screenshot
        :param screenshot_config: ScrapeConfig
        :param no_raise: bool - if True, do not raise exception on error while the screenshot api response is a ScrapflyError for seamless integration
        :return: str

        If you use no_raise=True, make sure to check the screenshot_api_response.error attribute to handle the error.
        If the error is not none, you will get the following structure for example

        &#39;error&#39;: {
            &#39;code&#39;: &#39;ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT&#39;,
            &#39;message&#39;: &#39;For some reason we were unable to take the screenshot&#39;,
            &#39;http_code&#39;: 422,
            &#39;links&#39;: {
                &#39;Checkout the related doc: https://scrapfly.io/docs/screenshot-api/error/ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT&#39;
            }
        }
        &#34;&#34;&#34;

        try:
            logger.debug(&#39;--&gt; %s Screenshoting&#39; % (screenshot_config.url))
            request_data = self._screenshot_request(screenshot_config=screenshot_config)
            response = self._http_handler(**request_data)
            screenshot_api_response = self._handle_screenshot_response(response=response, screenshot_config=screenshot_config)
            return screenshot_api_response
        except BaseException as e:
            self.reporter.report(error=e)

            if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
                return e.api_response

            raise e

    async def async_extraction(self, extraction_config:ExtractionConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ExtractionApiResponse:
        if loop is None:
            loop = asyncio.get_running_loop()

        return await loop.run_in_executor(self.async_executor, self.extract, extraction_config)

    @backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
    def extract(self, extraction_config:ExtractionConfig, no_raise:bool=False) -&gt; ExtractionApiResponse:
        &#34;&#34;&#34;
        Extract structured data from text content
        :param extraction_config: ExtractionConfig
        :param no_raise: bool - if True, do not raise exception on error while the extraction api response is a ScrapflyError for seamless integration
        :return: str

        If you use no_raise=True, make sure to check the extraction_api_response.error attribute to handle the error.
        If the error is not none, you will get the following structure for example

        &#39;error&#39;: {
            &#39;code&#39;: &#39;ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED&#39;,
            &#39;message&#39;: &#39;The content type of the response is not supported for extraction&#39;,
            &#39;http_code&#39;: 422,
            &#39;links&#39;: {
                &#39;Checkout the related doc: https://scrapfly.io/docs/extraction-api/error/ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED&#39;
            }
        }
        &#34;&#34;&#34;

        try:
            logger.debug(&#39;--&gt; %s Extracting data from&#39; % (extraction_config.content_type))
            request_data = self._extraction_request(extraction_config=extraction_config)
            response = self._http_handler(**request_data)
            extraction_api_response = self._handle_extraction_response(response=response, extraction_config=extraction_config)
            return extraction_api_response
        except BaseException as e:
            self.reporter.report(error=e)

            if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
                return e.api_response

            raise e

    def _handle_response(self, response:Response, scrape_config:ScrapeConfig) -&gt; ScrapeApiResponse:
        try:
            api_response = self._handle_api_response(
                response=response,
                scrape_config=scrape_config,
                raise_on_upstream_error=scrape_config.raise_on_upstream_error
            )

            if scrape_config.method == &#39;HEAD&#39;:
                logger.debug(&#39;&lt;-- [%s %s] %s | %ss&#39; % (
                    api_response.response.status_code,
                    api_response.response.reason,
                    api_response.response.request.url,
                    0
                ))
            else:
                logger.debug(&#39;&lt;-- [%s %s] %s | %ss&#39; % (
                    api_response.result[&#39;result&#39;][&#39;status_code&#39;],
                    api_response.result[&#39;result&#39;][&#39;reason&#39;],
                    api_response.result[&#39;config&#39;][&#39;url&#39;],
                    api_response.result[&#39;result&#39;][&#39;duration&#39;])
                )

                logger.debug(&#39;Log url: %s&#39; % api_response.result[&#39;result&#39;][&#39;log_url&#39;])

            return api_response
        except UpstreamHttpError as e:
            logger.critical(e.api_response.error_message)
            raise
        except HttpError as e:
            if e.api_response is not None:
                logger.critical(e.api_response.error_message)
            else:
                logger.critical(e.message)
            raise
        except ScrapflyError as e:
            logger.critical(&#39;&lt;-- %s | Docs: %s&#39; % (str(e), e.documentation_url))
            raise

    def _handle_screenshot_response(self, response:Response, screenshot_config:ScreenshotConfig) -&gt; ScreenshotApiResponse:    
        try:
            api_response = self._handle_screenshot_api_response(
                response=response,
                screenshot_config=screenshot_config,
                raise_on_upstream_error=screenshot_config.raise_on_upstream_error
            )
            return api_response
        except UpstreamHttpError as e:
            logger.critical(e.api_response.error_message)
            raise
        except HttpError as e:
            if e.api_response is not None:
                logger.critical(e.api_response.error_message)
            else:
                logger.critical(e.message)
            raise
        except ScrapflyError as e:
            logger.critical(&#39;&lt;-- %s | Docs: %s&#39; % (str(e), e.documentation_url))
            raise         

    def _handle_extraction_response(self, response:Response, extraction_config:ExtractionConfig) -&gt; ExtractionApiResponse:
        try:
            api_response = self._handle_extraction_api_response(
                response=response,
                extraction_config=extraction_config,
                raise_on_upstream_error=extraction_config.raise_on_upstream_error
            )
            return api_response
        except UpstreamHttpError as e:
            logger.critical(e.api_response.error_message)
            raise
        except HttpError as e:
            if e.api_response is not None:
                logger.critical(e.api_response.error_message)
            else:
                logger.critical(e.message)
            raise
        except ScrapflyError as e:
            logger.critical(&#39;&lt;-- %s | Docs: %s&#39; % (str(e), e.documentation_url))
            raise    

    def save_screenshot(self, screenshot_api_response:ScreenshotApiResponse, name:str, path:Optional[str]=None):
        &#34;&#34;&#34;
        Save a screenshot from a screenshot API response
        :param api_response: ScreenshotApiResponse
        :param name: str - name of the screenshot to save as
        :param path: Optional[str]
        &#34;&#34;&#34;

        if screenshot_api_response.screenshot_success is not True:
            raise RuntimeError(&#39;Screenshot was not successful&#39;)

        if not screenshot_api_response.image:
            raise RuntimeError(&#39;Screenshot binary does not exist&#39;)

        content = screenshot_api_response.image
        extension_name = screenshot_api_response.metadata[&#39;extension_name&#39;]

        if path:
            os.makedirs(path, exist_ok=True)
            file_path = os.path.join(path, f&#39;{name}.{extension_name}&#39;)
        else:
            file_path = f&#39;{name}.{extension_name}&#39;

        if isinstance(content, bytes):
            content = BytesIO(content)

        with open(file_path, &#39;wb&#39;) as f:
            shutil.copyfileobj(content, f, length=131072)

    def save_scrape_screenshot(self, api_response:ScrapeApiResponse, name:str, path:Optional[str]=None):
        &#34;&#34;&#34;
        Save a screenshot from a scrape result
        :param api_response: ScrapeApiResponse
        :param name: str - name of the screenshot given in the scrape config
        :param path: Optional[str]
        &#34;&#34;&#34;

        if not api_response.scrape_result[&#39;screenshots&#39;]:
            raise RuntimeError(&#39;Screenshot %s do no exists&#39; % name)

        try:
            api_response.scrape_result[&#39;screenshots&#39;][name]
        except KeyError:
            raise RuntimeError(&#39;Screenshot %s do no exists&#39; % name)

        screenshot_response = self._http_handler(
            method=&#39;GET&#39;,
            url=api_response.scrape_result[&#39;screenshots&#39;][name][&#39;url&#39;],
            params={&#39;key&#39;: self.key},
            verify=self.verify
        )

        screenshot_response.raise_for_status()

        if not name.endswith(&#39;.jpg&#39;):
            name += &#39;.jpg&#39;

        api_response.sink(path=path, name=name, content=screenshot_response.content)

    def sink(self, api_response:ScrapeApiResponse, content:Optional[Union[str, bytes]]=None, path: Optional[str] = None, name: Optional[str] = None, file: Optional[Union[TextIO, BytesIO]] = None) -&gt; str:
        scrape_result = api_response.result[&#39;result&#39;]
        scrape_config = api_response.result[&#39;config&#39;]

        file_content = content or scrape_result[&#39;content&#39;]
        file_path = None
        file_extension = None

        if name:
            name_parts = name.split(&#39;.&#39;)
            if len(name_parts) &gt; 1:
                file_extension = name_parts[-1]

        if not file:
            if file_extension is None:
                try:
                    mime_type = scrape_result[&#39;response_headers&#39;][&#39;content-type&#39;]
                except KeyError:
                    mime_type = &#39;application/octet-stream&#39;

                if &#39;;&#39; in mime_type:
                    mime_type = mime_type.split(&#39;;&#39;)[0]

                file_extension = &#39;.&#39; + mime_type.split(&#39;/&#39;)[1]

            if not name:
                name = scrape_config[&#39;url&#39;].split(&#39;/&#39;)[-1]

            if name.find(file_extension) == -1:
                name += file_extension

            file_path = path + &#39;/&#39; + name if path else name

            if file_path == file_extension:
                url = re.sub(r&#39;(https|http)?://&#39;, &#39;&#39;, api_response.config[&#39;url&#39;]).replace(&#39;/&#39;, &#39;-&#39;)

                if url[-1] == &#39;-&#39;:
                    url = url[:-1]

                url += file_extension

                file_path = url

            file = open(file_path, &#39;wb&#39;)

        if isinstance(file_content, str):
            file_content = BytesIO(file_content.encode(&#39;utf-8&#39;))
        elif isinstance(file_content, bytes):
            file_content = BytesIO(file_content)

        file_content.seek(0)
        with file as f:
            shutil.copyfileobj(file_content, f, length=131072)

        logger.info(&#39;file %s created&#39; % file_path)
        return file_path

    def _handle_scrape_large_objects(
        self,
        callback_url:str,
        format: Literal[&#39;clob&#39;, &#39;blob&#39;]
    ) -&gt; Tuple[Union[BytesIO, str], str]:
        if format not in [&#39;clob&#39;, &#39;blob&#39;]:
            raise ContentError(&#39;Large objects handle can handles format format [blob, clob], given: %s&#39; % format)

        response = self._http_handler(**{
            &#39;method&#39;: &#39;GET&#39;,
            &#39;url&#39;: callback_url,
            &#39;verify&#39;: self.verify,
            &#39;timeout&#39;: (self.connect_timeout, self.default_read_timeout),
            &#39;headers&#39;: {
                &#39;accept-encoding&#39;: self.body_handler.content_encoding,
                &#39;accept&#39;: self.body_handler.accept,
                &#39;user-agent&#39;: self.ua
            },
            &#39;params&#39;: {&#39;key&#39;: self.key}
        })

        if self.body_handler.support(headers=response.headers):
            content = self.body_handler(content=response.content, content_type=response.headers[&#39;content-type&#39;])
        else:
            content = response.content

        if format == &#39;clob&#39;:
            return content.decode(&#39;utf-8&#39;), &#39;text&#39;

        return BytesIO(content), &#39;binary&#39;

    def _handle_api_response(
        self,
        response: Response,
        scrape_config:ScrapeConfig,
        raise_on_upstream_error: Optional[bool] = True
    ) -&gt; ScrapeApiResponse:

        if scrape_config.method == &#39;HEAD&#39;:
            body = None
        else:
            if self.body_handler.support(headers=response.headers):
                body = self.body_handler(content=response.content, content_type=response.headers[&#39;content-type&#39;])
            else:
                body = response.content.decode(&#39;utf-8&#39;)

        api_response:ScrapeApiResponse = ScrapeApiResponse(
            response=response,
            request=response.request,
            api_result=body,
            scrape_config=scrape_config,
            large_object_handler=self._handle_scrape_large_objects
        )

        api_response.raise_for_result(raise_on_upstream_error=raise_on_upstream_error)

        return api_response

    def _handle_screenshot_api_response(
        self,
        response: Response,
        screenshot_config:ScreenshotConfig,
        raise_on_upstream_error: Optional[bool] = True
    ) -&gt; ScreenshotApiResponse:

        if self.body_handler.support(headers=response.headers):
            body = self.body_handler(content=response.content, content_type=response.headers[&#39;content-type&#39;])
        else:
            body = {&#39;result&#39;: response.content}

        api_response:ScreenshotApiResponse = ScreenshotApiResponse(
            response=response,
            request=response.request,
            api_result=body,
            screenshot_config=screenshot_config
        )

        api_response.raise_for_result(raise_on_upstream_error=raise_on_upstream_error)

        return api_response

    def _handle_extraction_api_response(
        self,
        response: Response,
        extraction_config:ExtractionConfig,
        raise_on_upstream_error: Optional[bool] = True
    ) -&gt; ExtractionApiResponse:
        
        if self.body_handler.support(headers=response.headers):
            body = self.body_handler(content=response.content, content_type=response.headers[&#39;content-type&#39;])
        else:
            body = response.content.decode(&#39;utf-8&#39;)

        api_response:ExtractionApiResponse = ExtractionApiResponse(
            response=response,
            request=response.request,
            api_result=body,
            extraction_config=extraction_config
        )

        api_response.raise_for_result(raise_on_upstream_error=raise_on_upstream_error)

        return api_response</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScrapflyClient.CONCURRENCY_AUTO"><code class="name">var <span class="ident">CONCURRENCY_AUTO</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DATETIME_FORMAT"><code class="name">var <span class="ident">DATETIME_FORMAT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DEFAULT_CONNECT_TIMEOUT"><code class="name">var <span class="ident">DEFAULT_CONNECT_TIMEOUT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DEFAULT_EXTRACTION_API_READ_TIMEOUT"><code class="name">var <span class="ident">DEFAULT_EXTRACTION_API_READ_TIMEOUT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DEFAULT_READ_TIMEOUT"><code class="name">var <span class="ident">DEFAULT_READ_TIMEOUT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DEFAULT_SCREENSHOT_API_READ_TIMEOUT"><code class="name">var <span class="ident">DEFAULT_SCREENSHOT_API_READ_TIMEOUT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.DEFAULT_WEBSCRAPING_API_READ_TIMEOUT"><code class="name">var <span class="ident">DEFAULT_WEBSCRAPING_API_READ_TIMEOUT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.HOST"><code class="name">var <span class="ident">HOST</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.brotli"><code class="name">var <span class="ident">brotli</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.connect_timeout"><code class="name">var <span class="ident">connect_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.debug"><code class="name">var <span class="ident">debug</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.default_read_timeout"><code class="name">var <span class="ident">default_read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.distributed_mode"><code class="name">var <span class="ident">distributed_mode</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.extraction_api_read_timeout"><code class="name">var <span class="ident">extraction_api_read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.host"><code class="name">var <span class="ident">host</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.key"><code class="name">var <span class="ident">key</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.max_concurrency"><code class="name">var <span class="ident">max_concurrency</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.monitoring_api_read_timeout"><code class="name">var <span class="ident">monitoring_api_read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.read_timeout"><code class="name">var <span class="ident">read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.reporter"><code class="name">var <span class="ident">reporter</span> : scrapfly.reporter.Reporter</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.screenshot_api_read_timeout"><code class="name">var <span class="ident">screenshot_api_read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.verify"><code class="name">var <span class="ident">verify</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.version"><code class="name">var <span class="ident">version</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyClient.web_scraping_api_read_timeout"><code class="name">var <span class="ident">web_scraping_api_read_timeout</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="scrapfly.ScrapflyClient.http"><code class="name">var <span class="ident">http</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def http(self):
    return self._http_handler</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.ua"><code class="name">var <span class="ident">ua</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ua(self) -&gt; str:
    return &#39;ScrapflySDK/%s (Python %s, %s, %s)&#39; % (
        self.version,
        platform.python_version(),
        platform.uname().system,
        platform.uname().machine
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ScrapflyClient.account"><code class="name flex">
<span>def <span class="ident">account</span></span>(<span>self) ‑> Union[str, Dict]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def account(self) -&gt; Union[str, Dict]:
    response = self._http_handler(
        method=&#39;GET&#39;,
        url=self.host + &#39;/account&#39;,
        params={&#39;key&#39;: self.key},
        verify=self.verify,
        headers={
            &#39;accept-encoding&#39;: self.body_handler.content_encoding,
            &#39;accept&#39;: self.body_handler.accept,
            &#39;user-agent&#39;: self.ua
        },
    )

    response.raise_for_status()

    if self.body_handler.support(response.headers):
        return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

    return response.content.decode(&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.async_extraction"><code class="name flex">
<span>async def <span class="ident">async_extraction</span></span>(<span>self, extraction_config: <a title="scrapfly.extraction_config.ExtractionConfig" href="extraction_config.html#scrapfly.extraction_config.ExtractionConfig">ExtractionConfig</a>, loop: Optional[asyncio.events.AbstractEventLoop] = None) ‑> <a title="scrapfly.api_response.ExtractionApiResponse" href="api_response.html#scrapfly.api_response.ExtractionApiResponse">ExtractionApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_extraction(self, extraction_config:ExtractionConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ExtractionApiResponse:
    if loop is None:
        loop = asyncio.get_running_loop()

    return await loop.run_in_executor(self.async_executor, self.extract, extraction_config)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.async_scrape"><code class="name flex">
<span>async def <span class="ident">async_scrape</span></span>(<span>self, scrape_config: <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a>, loop: Optional[asyncio.events.AbstractEventLoop] = None) ‑> <a title="scrapfly.api_response.ScrapeApiResponse" href="api_response.html#scrapfly.api_response.ScrapeApiResponse">ScrapeApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_scrape(self, scrape_config:ScrapeConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ScrapeApiResponse:
    if loop is None:
        loop = asyncio.get_running_loop()

    return await loop.run_in_executor(self.async_executor, self.scrape, scrape_config)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.async_screenshot"><code class="name flex">
<span>async def <span class="ident">async_screenshot</span></span>(<span>self, screenshot_config: <a title="scrapfly.screenshot_config.ScreenshotConfig" href="screenshot_config.html#scrapfly.screenshot_config.ScreenshotConfig">ScreenshotConfig</a>, loop: Optional[asyncio.events.AbstractEventLoop] = None) ‑> <a title="scrapfly.api_response.ScreenshotApiResponse" href="api_response.html#scrapfly.api_response.ScreenshotApiResponse">ScreenshotApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_screenshot(self, screenshot_config:ScreenshotConfig, loop:Optional[AbstractEventLoop]=None) -&gt; ScreenshotApiResponse:
    if loop is None:
        loop = asyncio.get_running_loop()

    return await loop.run_in_executor(self.async_executor, self.screenshot, screenshot_config)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    self.http_session.close()
    self.http_session = None</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.concurrent_scrape"><code class="name flex">
<span>async def <span class="ident">concurrent_scrape</span></span>(<span>self, scrape_configs: List[<a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a>], concurrency: Optional[int] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def concurrent_scrape(self, scrape_configs:List[ScrapeConfig], concurrency:Optional[int]=None):
    if concurrency is None:
        concurrency = self.max_concurrency
    elif concurrency == self.CONCURRENCY_AUTO:
        concurrency = self.account()[&#39;subscription&#39;][&#39;max_concurrency&#39;]

    loop = asyncio.get_running_loop()
    processing_tasks = []
    results = []
    processed_tasks = 0
    expected_tasks = len(scrape_configs)

    def scrape_done_callback(task:Task):
        nonlocal processed_tasks

        try:
            if task.cancelled() is True:
                return

            error = task.exception()

            if error is not None:
                results.append(error)
            else:
                results.append(task.result())
        finally:
            processing_tasks.remove(task)
            processed_tasks += 1

    while scrape_configs or results or processing_tasks:
        logger.info(&#34;Scrape %d/%d - %d running&#34; % (processed_tasks, expected_tasks, len(processing_tasks)))

        if scrape_configs:
            if len(processing_tasks) &lt; concurrency:
                # @todo handle backpressure
                for _ in range(0, concurrency - len(processing_tasks)):
                    try:
                        scrape_config = scrape_configs.pop()
                    except:
                        break

                    scrape_config.raise_on_upstream_error = False
                    task = loop.create_task(self.async_scrape(scrape_config=scrape_config, loop=loop))
                    processing_tasks.append(task)
                    task.add_done_callback(scrape_done_callback)

        for _ in results:
            result = results.pop()
            yield result

        await asyncio.sleep(.5)

    logger.debug(&#34;Scrape %d/%d - %d running&#34; % (processed_tasks, expected_tasks, len(processing_tasks)))</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.extract"><code class="name flex">
<span>def <span class="ident">extract</span></span>(<span>self, extraction_config: <a title="scrapfly.extraction_config.ExtractionConfig" href="extraction_config.html#scrapfly.extraction_config.ExtractionConfig">ExtractionConfig</a>, no_raise: bool = False) ‑> <a title="scrapfly.api_response.ExtractionApiResponse" href="api_response.html#scrapfly.api_response.ExtractionApiResponse">ExtractionApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"><p>Extract structured data from text content
:param extraction_config: ExtractionConfig
:param no_raise: bool - if True, do not raise exception on error while the extraction api response is a ScrapflyError for seamless integration
:return: str</p>
<p>If you use no_raise=True, make sure to check the extraction_api_response.error attribute to handle the error.
If the error is not none, you will get the following structure for example</p>
<p>'error': {
'code': 'ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED',
'message': 'The content type of the response is not supported for extraction',
'http_code': 422,
'links': {
'Checkout the related doc: <a href="https://scrapfly.io/docs/extraction-api/error/ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED'">https://scrapfly.io/docs/extraction-api/error/ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED'</a>
}
}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
def extract(self, extraction_config:ExtractionConfig, no_raise:bool=False) -&gt; ExtractionApiResponse:
    &#34;&#34;&#34;
    Extract structured data from text content
    :param extraction_config: ExtractionConfig
    :param no_raise: bool - if True, do not raise exception on error while the extraction api response is a ScrapflyError for seamless integration
    :return: str

    If you use no_raise=True, make sure to check the extraction_api_response.error attribute to handle the error.
    If the error is not none, you will get the following structure for example

    &#39;error&#39;: {
        &#39;code&#39;: &#39;ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED&#39;,
        &#39;message&#39;: &#39;The content type of the response is not supported for extraction&#39;,
        &#39;http_code&#39;: 422,
        &#39;links&#39;: {
            &#39;Checkout the related doc: https://scrapfly.io/docs/extraction-api/error/ERR::EXTRACTION::CONTENT_TYPE_NOT_SUPPORTED&#39;
        }
    }
    &#34;&#34;&#34;

    try:
        logger.debug(&#39;--&gt; %s Extracting data from&#39; % (extraction_config.content_type))
        request_data = self._extraction_request(extraction_config=extraction_config)
        response = self._http_handler(**request_data)
        extraction_api_response = self._handle_extraction_response(response=response, extraction_config=extraction_config)
        return extraction_api_response
    except BaseException as e:
        self.reporter.report(error=e)

        if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
            return e.api_response

        raise e</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.get_monitoring_metrics"><code class="name flex">
<span>def <span class="ident">get_monitoring_metrics</span></span>(<span>self, format: str = 'structured', period: Optional[str] = None, aggregation: Optional[List[Literal['account', 'project', 'target']]] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_monitoring_metrics(self, format:str=ScraperAPI.MONITORING_DATA_FORMAT_STRUCTURED, period:Optional[str]=None, aggregation:Optional[List[MonitoringAggregation]]=None):
    params = {&#39;key&#39;: self.key, &#39;format&#39;: format}

    if period is not None:
        params[&#39;period&#39;] = period

    if aggregation is not None:
        params[&#39;aggregation&#39;] = &#39;,&#39;.join(aggregation)

    response = self._http_handler(
        method=&#39;GET&#39;,
        url=self.host + &#39;/scrape/monitoring/metrics&#39;,
        params=params,
        timeout=(self.connect_timeout, self.monitoring_api_read_timeout),
        verify=self.verify,
        headers={
            &#39;accept-encoding&#39;: self.body_handler.content_encoding,
            &#39;accept&#39;: self.body_handler.accept,
            &#39;user-agent&#39;: self.ua
        },
    )

    response.raise_for_status()

    if self.body_handler.support(response.headers):
        return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

    return response.content.decode(&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.get_monitoring_target_metrics"><code class="name flex">
<span>def <span class="ident">get_monitoring_target_metrics</span></span>(<span>self, domain: str, group_subdomain: bool = False, period: Optional[Literal['subscription', 'last7d', 'last24h', 'last1h', 'last5m']] = 'last24h', start: Optional[datetime.datetime] = None, end: Optional[datetime.datetime] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_monitoring_target_metrics(
        self,
        domain:str,
        group_subdomain:bool=False,
        period:Optional[MonitoringTargetPeriod]=ScraperAPI.MONITORING_PERIOD_LAST_24H,
        start:Optional[datetime.datetime]=None,
        end:Optional[datetime.datetime]=None,
):
    params = {
        &#39;key&#39;: self.key,
        &#39;domain&#39;: domain,
        &#39;group_subdomain&#39;: group_subdomain
    }

    if (start is not None and end is None) or (start is None and end is not None):
        raise ValueError(&#39;You must provide both start and end date&#39;)

    if start is not None and end is not None:
        params[&#39;start&#39;] = start.strftime(self.DATETIME_FORMAT)
        params[&#39;end&#39;] = end.strftime(self.DATETIME_FORMAT)
        period = None

    params[&#39;period&#39;] = period

    response = self._http_handler(
        method=&#39;GET&#39;,
        url=self.host + &#39;/scrape/monitoring/metrics/target&#39;,
        timeout=(self.connect_timeout, self.monitoring_api_read_timeout),
        params=params,
        verify=self.verify,
        headers={
            &#39;accept-encoding&#39;: self.body_handler.content_encoding,
            &#39;accept&#39;: self.body_handler.accept,
            &#39;user-agent&#39;: self.ua
        },
    )

    response.raise_for_status()

    if self.body_handler.support(response.headers):
        return self.body_handler(response.content, response.headers[&#39;content-type&#39;])

    return response.content.decode(&#39;utf-8&#39;)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.open"><code class="name flex">
<span>def <span class="ident">open</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def open(self):
    if self.http_session is None:
        self.http_session = Session()
        self.http_session.verify = self.verify
        self.http_session.timeout = (self.connect_timeout, self.default_read_timeout)
        self.http_session.params[&#39;key&#39;] = self.key
        self.http_session.headers[&#39;accept-encoding&#39;] = self.body_handler.content_encoding
        self.http_session.headers[&#39;accept&#39;] = self.body_handler.accept
        self.http_session.headers[&#39;user-agent&#39;] = self.ua</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.resilient_scrape"><code class="name flex">
<span>def <span class="ident">resilient_scrape</span></span>(<span>self, scrape_config: <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a>, retry_on_errors: Set[Exception] = {&lt;class &#x27;scrapfly.errors.ScrapflyError&#x27;&gt;}, retry_on_status_code: Optional[List[int]] = None, tries: int = 5, delay: int = 20) ‑> <a title="scrapfly.api_response.ScrapeApiResponse" href="api_response.html#scrapfly.api_response.ScrapeApiResponse">ScrapeApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resilient_scrape(
    self,
    scrape_config:ScrapeConfig,
    retry_on_errors:Set[Exception]={ScrapflyError},
    retry_on_status_code:Optional[List[int]]=None,
    tries: int = 5,
    delay: int = 20,
) -&gt; ScrapeApiResponse:
    assert retry_on_errors is not None, &#39;Retry on error is None&#39;
    assert isinstance(retry_on_errors, set), &#39;retry_on_errors is not a set()&#39;

    @backoff.on_exception(backoff.expo, exception=tuple(retry_on_errors), max_tries=tries, max_time=delay)
    def inner() -&gt; ScrapeApiResponse:

        try:
            return self.scrape(scrape_config=scrape_config)
        except (UpstreamHttpClientError, UpstreamHttpServerError) as e:
            if retry_on_status_code is not None and e.api_response:
                if e.api_response.upstream_status_code in retry_on_status_code:
                    raise e
                else:
                    return e.api_response

            raise e

    return inner()</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.save_scrape_screenshot"><code class="name flex">
<span>def <span class="ident">save_scrape_screenshot</span></span>(<span>self, api_response: <a title="scrapfly.api_response.ScrapeApiResponse" href="api_response.html#scrapfly.api_response.ScrapeApiResponse">ScrapeApiResponse</a>, name: str, path: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Save a screenshot from a scrape result
:param api_response: ScrapeApiResponse
:param name: str - name of the screenshot given in the scrape config
:param path: Optional[str]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_scrape_screenshot(self, api_response:ScrapeApiResponse, name:str, path:Optional[str]=None):
    &#34;&#34;&#34;
    Save a screenshot from a scrape result
    :param api_response: ScrapeApiResponse
    :param name: str - name of the screenshot given in the scrape config
    :param path: Optional[str]
    &#34;&#34;&#34;

    if not api_response.scrape_result[&#39;screenshots&#39;]:
        raise RuntimeError(&#39;Screenshot %s do no exists&#39; % name)

    try:
        api_response.scrape_result[&#39;screenshots&#39;][name]
    except KeyError:
        raise RuntimeError(&#39;Screenshot %s do no exists&#39; % name)

    screenshot_response = self._http_handler(
        method=&#39;GET&#39;,
        url=api_response.scrape_result[&#39;screenshots&#39;][name][&#39;url&#39;],
        params={&#39;key&#39;: self.key},
        verify=self.verify
    )

    screenshot_response.raise_for_status()

    if not name.endswith(&#39;.jpg&#39;):
        name += &#39;.jpg&#39;

    api_response.sink(path=path, name=name, content=screenshot_response.content)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.save_screenshot"><code class="name flex">
<span>def <span class="ident">save_screenshot</span></span>(<span>self, screenshot_api_response: <a title="scrapfly.api_response.ScreenshotApiResponse" href="api_response.html#scrapfly.api_response.ScreenshotApiResponse">ScreenshotApiResponse</a>, name: str, path: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Save a screenshot from a screenshot API response
:param api_response: ScreenshotApiResponse
:param name: str - name of the screenshot to save as
:param path: Optional[str]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_screenshot(self, screenshot_api_response:ScreenshotApiResponse, name:str, path:Optional[str]=None):
    &#34;&#34;&#34;
    Save a screenshot from a screenshot API response
    :param api_response: ScreenshotApiResponse
    :param name: str - name of the screenshot to save as
    :param path: Optional[str]
    &#34;&#34;&#34;

    if screenshot_api_response.screenshot_success is not True:
        raise RuntimeError(&#39;Screenshot was not successful&#39;)

    if not screenshot_api_response.image:
        raise RuntimeError(&#39;Screenshot binary does not exist&#39;)

    content = screenshot_api_response.image
    extension_name = screenshot_api_response.metadata[&#39;extension_name&#39;]

    if path:
        os.makedirs(path, exist_ok=True)
        file_path = os.path.join(path, f&#39;{name}.{extension_name}&#39;)
    else:
        file_path = f&#39;{name}.{extension_name}&#39;

    if isinstance(content, bytes):
        content = BytesIO(content)

    with open(file_path, &#39;wb&#39;) as f:
        shutil.copyfileobj(content, f, length=131072)</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.scrape"><code class="name flex">
<span>def <span class="ident">scrape</span></span>(<span>self, scrape_config: <a title="scrapfly.scrape_config.ScrapeConfig" href="scrape_config.html#scrapfly.scrape_config.ScrapeConfig">ScrapeConfig</a>, no_raise: bool = False) ‑> <a title="scrapfly.api_response.ScrapeApiResponse" href="api_response.html#scrapfly.api_response.ScrapeApiResponse">ScrapeApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"><p>Scrape a website
:param scrape_config: ScrapeConfig
:param no_raise: bool - if True, do not raise exception on error while the api response is a ScrapflyError for seamless integration
:return: ScrapeApiResponse</p>
<p>If you use no_raise=True, make sure to check the api_response.scrape_result.error attribute to handle the error.
If the error is not none, you will get the following structure for example</p>
<p>'error': {
'code': 'ERR::ASP::SHIELD_PROTECTION_FAILED',
'message': 'The ASP shield failed to solve the challenge against the anti scrapping protection - heuristic_engine bypass failed, please retry in few seconds',
'retryable': False,
'http_code': 422,
'links': {
'Checkout ASP documentation': 'https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate', 'Related Error Doc': 'https://scrapfly.io/docs/scrape-api/error/ERR::ASP::SHIELD_PROTECTION_FAILED'
}
}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
def scrape(self, scrape_config:ScrapeConfig, no_raise:bool=False) -&gt; ScrapeApiResponse:
    &#34;&#34;&#34;
    Scrape a website
    :param scrape_config: ScrapeConfig
    :param no_raise: bool - if True, do not raise exception on error while the api response is a ScrapflyError for seamless integration
    :return: ScrapeApiResponse

    If you use no_raise=True, make sure to check the api_response.scrape_result.error attribute to handle the error.
    If the error is not none, you will get the following structure for example

    &#39;error&#39;: {
        &#39;code&#39;: &#39;ERR::ASP::SHIELD_PROTECTION_FAILED&#39;,
        &#39;message&#39;: &#39;The ASP shield failed to solve the challenge against the anti scrapping protection - heuristic_engine bypass failed, please retry in few seconds&#39;,
        &#39;retryable&#39;: False,
        &#39;http_code&#39;: 422,
        &#39;links&#39;: {
            &#39;Checkout ASP documentation&#39;: &#39;https://scrapfly.io/docs/scrape-api/anti-scraping-protection#maximize_success_rate&#39;, &#39;Related Error Doc&#39;: &#39;https://scrapfly.io/docs/scrape-api/error/ERR::ASP::SHIELD_PROTECTION_FAILED&#39;
        }
    }
    &#34;&#34;&#34;

    try:
        logger.debug(&#39;--&gt; %s Scrapping %s&#39; % (scrape_config.method, scrape_config.url))
        request_data = self._scrape_request(scrape_config=scrape_config)
        response = self._http_handler(**request_data)
        scrape_api_response = self._handle_response(response=response, scrape_config=scrape_config)

        self.reporter.report(scrape_api_response=scrape_api_response)

        return scrape_api_response
    except BaseException as e:
        self.reporter.report(error=e)

        if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
            return e.api_response

        raise e</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.screenshot"><code class="name flex">
<span>def <span class="ident">screenshot</span></span>(<span>self, screenshot_config: <a title="scrapfly.screenshot_config.ScreenshotConfig" href="screenshot_config.html#scrapfly.screenshot_config.ScreenshotConfig">ScreenshotConfig</a>, no_raise: bool = False) ‑> <a title="scrapfly.api_response.ScreenshotApiResponse" href="api_response.html#scrapfly.api_response.ScreenshotApiResponse">ScreenshotApiResponse</a></span>
</code></dt>
<dd>
<div class="desc"><p>Take a screenshot
:param screenshot_config: ScrapeConfig
:param no_raise: bool - if True, do not raise exception on error while the screenshot api response is a ScrapflyError for seamless integration
:return: str</p>
<p>If you use no_raise=True, make sure to check the screenshot_api_response.error attribute to handle the error.
If the error is not none, you will get the following structure for example</p>
<p>'error': {
'code': 'ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT',
'message': 'For some reason we were unable to take the screenshot',
'http_code': 422,
'links': {
'Checkout the related doc: <a href="https://scrapfly.io/docs/screenshot-api/error/ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT'">https://scrapfly.io/docs/screenshot-api/error/ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT'</a>
}
}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@backoff.on_exception(backoff.expo, exception=NetworkError, max_tries=5)
def screenshot(self, screenshot_config:ScreenshotConfig, no_raise:bool=False) -&gt; ScreenshotApiResponse:
    &#34;&#34;&#34;
    Take a screenshot
    :param screenshot_config: ScrapeConfig
    :param no_raise: bool - if True, do not raise exception on error while the screenshot api response is a ScrapflyError for seamless integration
    :return: str

    If you use no_raise=True, make sure to check the screenshot_api_response.error attribute to handle the error.
    If the error is not none, you will get the following structure for example

    &#39;error&#39;: {
        &#39;code&#39;: &#39;ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT&#39;,
        &#39;message&#39;: &#39;For some reason we were unable to take the screenshot&#39;,
        &#39;http_code&#39;: 422,
        &#39;links&#39;: {
            &#39;Checkout the related doc: https://scrapfly.io/docs/screenshot-api/error/ERR::SCREENSHOT::UNABLE_TO_TAKE_SCREENSHOT&#39;
        }
    }
    &#34;&#34;&#34;

    try:
        logger.debug(&#39;--&gt; %s Screenshoting&#39; % (screenshot_config.url))
        request_data = self._screenshot_request(screenshot_config=screenshot_config)
        response = self._http_handler(**request_data)
        screenshot_api_response = self._handle_screenshot_response(response=response, screenshot_config=screenshot_config)
        return screenshot_api_response
    except BaseException as e:
        self.reporter.report(error=e)

        if no_raise and isinstance(e, ScrapflyError) and e.api_response is not None:
            return e.api_response

        raise e</code></pre>
</details>
</dd>
<dt id="scrapfly.ScrapflyClient.sink"><code class="name flex">
<span>def <span class="ident">sink</span></span>(<span>self, api_response: <a title="scrapfly.api_response.ScrapeApiResponse" href="api_response.html#scrapfly.api_response.ScrapeApiResponse">ScrapeApiResponse</a>, content: Union[str, bytes, ForwardRef(None)] = None, path: Optional[str] = None, name: Optional[str] = None, file: Union[TextIO, _io.BytesIO, ForwardRef(None)] = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sink(self, api_response:ScrapeApiResponse, content:Optional[Union[str, bytes]]=None, path: Optional[str] = None, name: Optional[str] = None, file: Optional[Union[TextIO, BytesIO]] = None) -&gt; str:
    scrape_result = api_response.result[&#39;result&#39;]
    scrape_config = api_response.result[&#39;config&#39;]

    file_content = content or scrape_result[&#39;content&#39;]
    file_path = None
    file_extension = None

    if name:
        name_parts = name.split(&#39;.&#39;)
        if len(name_parts) &gt; 1:
            file_extension = name_parts[-1]

    if not file:
        if file_extension is None:
            try:
                mime_type = scrape_result[&#39;response_headers&#39;][&#39;content-type&#39;]
            except KeyError:
                mime_type = &#39;application/octet-stream&#39;

            if &#39;;&#39; in mime_type:
                mime_type = mime_type.split(&#39;;&#39;)[0]

            file_extension = &#39;.&#39; + mime_type.split(&#39;/&#39;)[1]

        if not name:
            name = scrape_config[&#39;url&#39;].split(&#39;/&#39;)[-1]

        if name.find(file_extension) == -1:
            name += file_extension

        file_path = path + &#39;/&#39; + name if path else name

        if file_path == file_extension:
            url = re.sub(r&#39;(https|http)?://&#39;, &#39;&#39;, api_response.config[&#39;url&#39;]).replace(&#39;/&#39;, &#39;-&#39;)

            if url[-1] == &#39;-&#39;:
                url = url[:-1]

            url += file_extension

            file_path = url

        file = open(file_path, &#39;wb&#39;)

    if isinstance(file_content, str):
        file_content = BytesIO(file_content.encode(&#39;utf-8&#39;))
    elif isinstance(file_content, bytes):
        file_content = BytesIO(file_content)

    file_content.seek(0)
    with file as f:
        shutil.copyfileobj(file_content, f, length=131072)

    logger.info(&#39;file %s created&#39; % file_path)
    return file_path</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.ScrapflyError"><code class="flex name class">
<span>class <span class="ident">ScrapflyError</span></span>
<span>(</span><span>message: str, code: str, http_status_code: int, resource: Optional[str] = None, is_retryable: bool = False, retry_delay: Optional[int] = None, retry_times: Optional[int] = None, documentation_url: Optional[str] = None, api_response: Optional[ForwardRef('ApiResponse')] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyError(Exception):
    KIND_HTTP_BAD_RESPONSE = &#39;HTTP_BAD_RESPONSE&#39;
    KIND_SCRAPFLY_ERROR = &#39;SCRAPFLY_ERROR&#39;

    RESOURCE_PROXY = &#39;PROXY&#39;
    RESOURCE_THROTTLE = &#39;THROTTLE&#39;
    RESOURCE_SCRAPE = &#39;SCRAPE&#39;
    RESOURCE_ASP = &#39;ASP&#39;
    RESOURCE_SCHEDULE = &#39;SCHEDULE&#39;
    RESOURCE_WEBHOOK = &#39;WEBHOOK&#39;
    RESOURCE_SESSION = &#39;SESSION&#39;

    def __init__(
        self,
        message: str,
        code: str,
        http_status_code: int,
        resource: Optional[str]=None,
        is_retryable: bool = False,
        retry_delay: Optional[int] = None,
        retry_times: Optional[int] = None,
        documentation_url: Optional[str] = None,
        api_response: Optional[&#39;ApiResponse&#39;] = None
    ):
        self.message = message
        self.code = code
        self.retry_delay = retry_delay
        self.retry_times = retry_times
        self.resource = resource
        self.is_retryable = is_retryable
        self.documentation_url = documentation_url
        self.api_response = api_response
        self.http_status_code = http_status_code

        super().__init__(self.message, str(self.code))

    def __str__(self):
        message = self.message

        if self.documentation_url is not None:
            message += &#39;. Learn more: %s&#39; % self.documentation_url

        return message</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>scrapfly.errors.ExtraUsageForbidden</li>
<li>scrapfly.errors.HttpError</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScrapflyError.KIND_HTTP_BAD_RESPONSE"><code class="name">var <span class="ident">KIND_HTTP_BAD_RESPONSE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.KIND_SCRAPFLY_ERROR"><code class="name">var <span class="ident">KIND_SCRAPFLY_ERROR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_ASP"><code class="name">var <span class="ident">RESOURCE_ASP</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_PROXY"><code class="name">var <span class="ident">RESOURCE_PROXY</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_SCHEDULE"><code class="name">var <span class="ident">RESOURCE_SCHEDULE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_SCRAPE"><code class="name">var <span class="ident">RESOURCE_SCRAPE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_SESSION"><code class="name">var <span class="ident">RESOURCE_SESSION</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_THROTTLE"><code class="name">var <span class="ident">RESOURCE_THROTTLE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScrapflyError.RESOURCE_WEBHOOK"><code class="name">var <span class="ident">RESOURCE_WEBHOOK</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="scrapfly.ScrapflyProxyError"><code class="flex name class">
<span>class <span class="ident">ScrapflyProxyError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyProxyError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflyScheduleError"><code class="flex name class">
<span>class <span class="ident">ScrapflyScheduleError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyScheduleError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflyScrapeError"><code class="flex name class">
<span>class <span class="ident">ScrapflyScrapeError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyScrapeError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflySessionError"><code class="flex name class">
<span>class <span class="ident">ScrapflySessionError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflySessionError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflyThrottleError"><code class="flex name class">
<span>class <span class="ident">ScrapflyThrottleError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyThrottleError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScrapflyWebhookError"><code class="flex name class">
<span>class <span class="ident">ScrapflyWebhookError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScrapflyWebhookError(ScraperAPIError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.ScraperAPIError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScreenshotAPIError"><code class="flex name class">
<span>class <span class="ident">ScreenshotAPIError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScreenshotAPIError(HttpError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="scrapfly.ScreenshotApiResponse"><code class="flex name class">
<span>class <span class="ident">ScreenshotApiResponse</span></span>
<span>(</span><span>request: requests.models.Request, response: requests.models.Response, screenshot_config: <a title="scrapfly.screenshot_config.ScreenshotConfig" href="screenshot_config.html#scrapfly.screenshot_config.ScreenshotConfig">ScreenshotConfig</a>, api_result: Optional[bytes] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScreenshotApiResponse(ApiResponse):
    def __init__(self, request: Request, response: Response, screenshot_config: ScreenshotConfig, api_result: Optional[bytes] = None):
        super().__init__(request, response)
        self.screenshot_config = screenshot_config
        self.result = self.handle_api_result(api_result)

    @property
    def image(self) -&gt; Optional[str]:
        binary = self.result.get(&#39;result&#39;, None)
        if binary is None:
            return &#39;&#39;

        return binary

    @property
    def metadata(self) -&gt; Optional[Dict]:
        if not self.image:
            return {}

        content_type = self.response.headers.get(&#39;content-type&#39;)
        extension_name = content_type[content_type.find(&#39;/&#39;) + 1:].split(&#39;;&#39;)[0]

        return {
            &#39;extension_name&#39;: extension_name,
            &#39;upstream-status-code&#39;: self.response.headers.get(&#39;X-Scrapfly-Upstream-Http-Code&#39;),
            &#39;upstream-url&#39;: self.response.headers.get(&#39;X-Scrapfly-Upstream-Url&#39;)
        }

    @property
    def screenshot_success(self) -&gt; bool:
        if not self.image:
            return False

        return True

    @property
    def error(self) -&gt; Optional[Dict]:
        if self.image:
            return None

        if self.screenshot_success is False:
            return self.result

    def _is_api_error(self, api_result: Dict) -&gt; bool:
        if api_result is None:
            return True

        return &#39;error_id&#39; in api_result

    def handle_api_result(self, api_result: bytes) -&gt; FrozenDict:
        if self._is_api_error(api_result=api_result) is True:
            return FrozenDict(api_result)

        return api_result

    def raise_for_result(self, raise_on_upstream_error=True, error_class=ScreenshotAPIError):
        super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="scrapfly.ScreenshotApiResponse.error"><code class="name">var <span class="ident">error</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def error(self) -&gt; Optional[Dict]:
    if self.image:
        return None

    if self.screenshot_success is False:
        return self.result</code></pre>
</details>
</dd>
<dt id="scrapfly.ScreenshotApiResponse.image"><code class="name">var <span class="ident">image</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def image(self) -&gt; Optional[str]:
    binary = self.result.get(&#39;result&#39;, None)
    if binary is None:
        return &#39;&#39;

    return binary</code></pre>
</details>
</dd>
<dt id="scrapfly.ScreenshotApiResponse.metadata"><code class="name">var <span class="ident">metadata</span> : Optional[Dict]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def metadata(self) -&gt; Optional[Dict]:
    if not self.image:
        return {}

    content_type = self.response.headers.get(&#39;content-type&#39;)
    extension_name = content_type[content_type.find(&#39;/&#39;) + 1:].split(&#39;;&#39;)[0]

    return {
        &#39;extension_name&#39;: extension_name,
        &#39;upstream-status-code&#39;: self.response.headers.get(&#39;X-Scrapfly-Upstream-Http-Code&#39;),
        &#39;upstream-url&#39;: self.response.headers.get(&#39;X-Scrapfly-Upstream-Url&#39;)
    }</code></pre>
</details>
</dd>
<dt id="scrapfly.ScreenshotApiResponse.screenshot_success"><code class="name">var <span class="ident">screenshot_success</span> : bool</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def screenshot_success(self) -&gt; bool:
    if not self.image:
        return False

    return True</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ScreenshotApiResponse.handle_api_result"><code class="name flex">
<span>def <span class="ident">handle_api_result</span></span>(<span>self, api_result: bytes) ‑> <a title="scrapfly.frozen_dict.FrozenDict" href="frozen_dict.html#scrapfly.frozen_dict.FrozenDict">FrozenDict</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_api_result(self, api_result: bytes) -&gt; FrozenDict:
    if self._is_api_error(api_result=api_result) is True:
        return FrozenDict(api_result)

    return api_result</code></pre>
</details>
</dd>
<dt id="scrapfly.ScreenshotApiResponse.raise_for_result"><code class="name flex">
<span>def <span class="ident">raise_for_result</span></span>(<span>self, raise_on_upstream_error=True, error_class=scrapfly.errors.ScreenshotAPIError)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def raise_for_result(self, raise_on_upstream_error=True, error_class=ScreenshotAPIError):
    super().raise_for_result(raise_on_upstream_error=raise_on_upstream_error, error_class=error_class)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="scrapfly.api_response.ApiResponse" href="api_response.html#scrapfly.api_response.ApiResponse">ApiResponse</a></b></code>:
<ul class="hlist">
<li><code><a title="scrapfly.api_response.ApiResponse.status_code" href="api_response.html#scrapfly.api_response.ApiResponse.status_code">status_code</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="scrapfly.ScreenshotConfig"><code class="flex name class">
<span>class <span class="ident">ScreenshotConfig</span></span>
<span>(</span><span>url: str, format: Optional[<a title="scrapfly.screenshot_config.Format" href="screenshot_config.html#scrapfly.screenshot_config.Format">Format</a>] = None, capture: Optional[str] = None, resolution: Optional[str] = None, country: Optional[str] = None, timeout: Optional[int] = None, rendering_wait: Optional[int] = None, wait_for_selector: Optional[str] = None, options: Optional[List[<a title="scrapfly.screenshot_config.Options" href="screenshot_config.html#scrapfly.screenshot_config.Options">Options</a>]] = None, auto_scroll: Optional[bool] = None, js: Optional[str] = None, cache: Optional[bool] = None, cache_ttl: Optional[bool] = None, cache_clear: Optional[bool] = None, webhook: Optional[str] = None, raise_on_upstream_error: bool = True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScreenshotConfig(BaseApiConfig):
    url: str
    format: Optional[Format] = None
    capture: Optional[str] = None
    resolution: Optional[str] = None
    country: Optional[str] = None
    timeout: Optional[int] = None # in milliseconds
    rendering_wait: Optional[int] = None # in milliseconds
    wait_for_selector: Optional[str] = None
    options: Optional[List[Options]] = None
    auto_scroll: Optional[bool] = None
    js: Optional[str] = None
    cache: Optional[bool] = None
    cache_ttl: Optional[bool] = None
    cache_clear: Optional[bool] = None
    webhook: Optional[str] = None
    raise_on_upstream_error: bool = True

    def __init__(
        self,
        url: str,
        format: Optional[Format] = None,
        capture: Optional[str] = None,
        resolution: Optional[str] = None,
        country: Optional[str] = None,
        timeout: Optional[int] = None, # in milliseconds
        rendering_wait: Optional[int] = None, # in milliseconds
        wait_for_selector: Optional[str] = None,
        options: Optional[List[Options]] = None,
        auto_scroll: Optional[bool] = None,
        js: Optional[str] = None,
        cache: Optional[bool] = None,
        cache_ttl: Optional[bool] = None,
        cache_clear: Optional[bool] = None,
        webhook: Optional[str] = None,
        raise_on_upstream_error: bool = True
    ):
        assert(type(url) is str)

        self.url = url
        self.key = None
        self.format = format
        self.capture = capture
        self.resolution = resolution
        self.country = country
        self.timeout = timeout
        self.rendering_wait = rendering_wait
        self.wait_for_selector = wait_for_selector
        self.options = [Options(flag) for flag in options] if options else None
        self.auto_scroll = auto_scroll
        self.js = js
        self.cache = cache
        self.cache_ttl = cache_ttl
        self.cache_clear = cache_clear
        self.webhook = webhook
        self.raise_on_upstream_error = raise_on_upstream_error

    def to_api_params(self, key:str) -&gt; Dict:
        params = {
            &#39;key&#39;: self.key or key,
            &#39;url&#39;: self.url
        }

        if self.format:
            params[&#39;format&#39;] = Format(self.format).value

        if self.capture:
            params[&#39;capture&#39;] = self.capture

        if self.resolution:
            params[&#39;resolution&#39;] = self.resolution

        if self.country is not None:
            params[&#39;country&#39;] = self.country

        if self.timeout is not None:
            params[&#39;timeout&#39;] = self.timeout

        if self.rendering_wait is not None:
            params[&#39;rendering_wait&#39;] = self.rendering_wait

        if self.wait_for_selector is not None:
            params[&#39;wait_for_selector&#39;] = self.wait_for_selector            

        if self.options is not None:
            params[&#34;options&#34;] = &#34;,&#34;.join(flag.value for flag in self.options)

        if self.auto_scroll is not None:
            params[&#39;auto_scroll&#39;] = self._bool_to_http(self.auto_scroll)

        if self.js:
            params[&#39;js&#39;] = base64.urlsafe_b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

        if self.cache is not None:
            params[&#39;cache&#39;] = self._bool_to_http(self.cache)
            
            if self.cache_ttl is not None:
                params[&#39;cache_ttl&#39;] = self._bool_to_http(self.cache_ttl)

            if self.cache_clear is not None:
                params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

        else:
            if self.cache_ttl is not None:
                logging.warning(&#39;Params &#34;cache_ttl&#34; is ignored. Works only if cache is enabled&#39;)

            if self.cache_clear is not None:
                logging.warning(&#39;Params &#34;cache_clear&#34; is ignored. Works only if cache is enabled&#39;)

        if self.webhook is not None:
            params[&#39;webhook_name&#39;] = self.webhook

        return params

    def to_dict(self) -&gt; Dict:
        &#34;&#34;&#34;
        Export the ScreenshotConfig instance to a plain dictionary.
        &#34;&#34;&#34;
        return {
            &#39;url&#39;: self.url,
            &#39;format&#39;: Format(self.format).value if self.format else None,
            &#39;capture&#39;: self.capture,
            &#39;resolution&#39;: self.resolution,
            &#39;country&#39;: self.country,
            &#39;timeout&#39;: self.timeout,
            &#39;rendering_wait&#39;: self.rendering_wait,
            &#39;wait_for_selector&#39;: self.wait_for_selector,
            &#39;options&#39;: [Options(option).value for option in self.options] if self.options else None,
            &#39;auto_scroll&#39;: self.auto_scroll,
            &#39;js&#39;: self.js,
            &#39;cache&#39;: self.cache,
            &#39;cache_ttl&#39;: self.cache_ttl,
            &#39;cache_clear&#39;: self.cache_clear,
            &#39;webhook&#39;: self.webhook,
            &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error
        }
    
    @staticmethod
    def from_dict(screenshot_config_dict: Dict) -&gt; &#39;ScreenshotConfig&#39;:
        &#34;&#34;&#34;Create a ScreenshotConfig instance from a dictionary.&#34;&#34;&#34;
        url = screenshot_config_dict.get(&#39;url&#39;, None)

        format = screenshot_config_dict.get(&#39;format&#39;, None)
        format = Format(format) if format else None

        capture = screenshot_config_dict.get(&#39;capture&#39;, None)
        resolution = screenshot_config_dict.get(&#39;resolution&#39;, None)
        country = screenshot_config_dict.get(&#39;country&#39;, None)
        timeout = screenshot_config_dict.get(&#39;timeout&#39;, None)
        rendering_wait = screenshot_config_dict.get(&#39;rendering_wait&#39;, None)
        wait_for_selector = screenshot_config_dict.get(&#39;wait_for_selector&#39;, None)

        options = screenshot_config_dict.get(&#39;options&#39;, None)
        options = [Options(option) for option in options] if options else None

        auto_scroll = screenshot_config_dict.get(&#39;auto_scroll&#39;, None)
        js = screenshot_config_dict.get(&#39;js&#39;, None)
        cache = screenshot_config_dict.get(&#39;cache&#39;, None)
        cache_ttl = screenshot_config_dict.get(&#39;cache_ttl&#39;, None)
        cache_clear = screenshot_config_dict.get(&#39;cache_clear&#39;, None)
        webhook = screenshot_config_dict.get(&#39;webhook&#39;, None)
        raise_on_upstream_error = screenshot_config_dict.get(&#39;raise_on_upstream_error&#39;, True)

        return ScreenshotConfig(
            url=url,
            format=format,
            capture=capture,
            resolution=resolution,
            country=country,
            timeout=timeout,
            rendering_wait=rendering_wait,
            wait_for_selector=wait_for_selector,
            options=options,
            auto_scroll=auto_scroll,
            js=js,
            cache=cache,
            cache_ttl=cache_ttl,
            cache_clear=cache_clear,
            webhook=webhook,
            raise_on_upstream_error=raise_on_upstream_error
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.api_config.BaseApiConfig" href="api_config.html#scrapfly.api_config.BaseApiConfig">BaseApiConfig</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="scrapfly.ScreenshotConfig.auto_scroll"><code class="name">var <span class="ident">auto_scroll</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.cache"><code class="name">var <span class="ident">cache</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.cache_clear"><code class="name">var <span class="ident">cache_clear</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.cache_ttl"><code class="name">var <span class="ident">cache_ttl</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.capture"><code class="name">var <span class="ident">capture</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.country"><code class="name">var <span class="ident">country</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.format"><code class="name">var <span class="ident">format</span> : Optional[<a title="scrapfly.screenshot_config.Format" href="screenshot_config.html#scrapfly.screenshot_config.Format">Format</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.js"><code class="name">var <span class="ident">js</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.options"><code class="name">var <span class="ident">options</span> : Optional[List[<a title="scrapfly.screenshot_config.Options" href="screenshot_config.html#scrapfly.screenshot_config.Options">Options</a>]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.raise_on_upstream_error"><code class="name">var <span class="ident">raise_on_upstream_error</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.rendering_wait"><code class="name">var <span class="ident">rendering_wait</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.resolution"><code class="name">var <span class="ident">resolution</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.timeout"><code class="name">var <span class="ident">timeout</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.url"><code class="name">var <span class="ident">url</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.wait_for_selector"><code class="name">var <span class="ident">wait_for_selector</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="scrapfly.ScreenshotConfig.webhook"><code class="name">var <span class="ident">webhook</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="scrapfly.ScreenshotConfig.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>screenshot_config_dict: Dict) ‑> <a title="scrapfly.screenshot_config.ScreenshotConfig" href="screenshot_config.html#scrapfly.screenshot_config.ScreenshotConfig">ScreenshotConfig</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create a ScreenshotConfig instance from a dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(screenshot_config_dict: Dict) -&gt; &#39;ScreenshotConfig&#39;:
    &#34;&#34;&#34;Create a ScreenshotConfig instance from a dictionary.&#34;&#34;&#34;
    url = screenshot_config_dict.get(&#39;url&#39;, None)

    format = screenshot_config_dict.get(&#39;format&#39;, None)
    format = Format(format) if format else None

    capture = screenshot_config_dict.get(&#39;capture&#39;, None)
    resolution = screenshot_config_dict.get(&#39;resolution&#39;, None)
    country = screenshot_config_dict.get(&#39;country&#39;, None)
    timeout = screenshot_config_dict.get(&#39;timeout&#39;, None)
    rendering_wait = screenshot_config_dict.get(&#39;rendering_wait&#39;, None)
    wait_for_selector = screenshot_config_dict.get(&#39;wait_for_selector&#39;, None)

    options = screenshot_config_dict.get(&#39;options&#39;, None)
    options = [Options(option) for option in options] if options else None

    auto_scroll = screenshot_config_dict.get(&#39;auto_scroll&#39;, None)
    js = screenshot_config_dict.get(&#39;js&#39;, None)
    cache = screenshot_config_dict.get(&#39;cache&#39;, None)
    cache_ttl = screenshot_config_dict.get(&#39;cache_ttl&#39;, None)
    cache_clear = screenshot_config_dict.get(&#39;cache_clear&#39;, None)
    webhook = screenshot_config_dict.get(&#39;webhook&#39;, None)
    raise_on_upstream_error = screenshot_config_dict.get(&#39;raise_on_upstream_error&#39;, True)

    return ScreenshotConfig(
        url=url,
        format=format,
        capture=capture,
        resolution=resolution,
        country=country,
        timeout=timeout,
        rendering_wait=rendering_wait,
        wait_for_selector=wait_for_selector,
        options=options,
        auto_scroll=auto_scroll,
        js=js,
        cache=cache,
        cache_ttl=cache_ttl,
        cache_clear=cache_clear,
        webhook=webhook,
        raise_on_upstream_error=raise_on_upstream_error
    )</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="scrapfly.ScreenshotConfig.to_api_params"><code class="name flex">
<span>def <span class="ident">to_api_params</span></span>(<span>self, key: str) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_api_params(self, key:str) -&gt; Dict:
    params = {
        &#39;key&#39;: self.key or key,
        &#39;url&#39;: self.url
    }

    if self.format:
        params[&#39;format&#39;] = Format(self.format).value

    if self.capture:
        params[&#39;capture&#39;] = self.capture

    if self.resolution:
        params[&#39;resolution&#39;] = self.resolution

    if self.country is not None:
        params[&#39;country&#39;] = self.country

    if self.timeout is not None:
        params[&#39;timeout&#39;] = self.timeout

    if self.rendering_wait is not None:
        params[&#39;rendering_wait&#39;] = self.rendering_wait

    if self.wait_for_selector is not None:
        params[&#39;wait_for_selector&#39;] = self.wait_for_selector            

    if self.options is not None:
        params[&#34;options&#34;] = &#34;,&#34;.join(flag.value for flag in self.options)

    if self.auto_scroll is not None:
        params[&#39;auto_scroll&#39;] = self._bool_to_http(self.auto_scroll)

    if self.js:
        params[&#39;js&#39;] = base64.urlsafe_b64encode(self.js.encode(&#39;utf-8&#39;)).decode(&#39;utf-8&#39;)

    if self.cache is not None:
        params[&#39;cache&#39;] = self._bool_to_http(self.cache)
        
        if self.cache_ttl is not None:
            params[&#39;cache_ttl&#39;] = self._bool_to_http(self.cache_ttl)

        if self.cache_clear is not None:
            params[&#39;cache_clear&#39;] = self._bool_to_http(self.cache_clear)

    else:
        if self.cache_ttl is not None:
            logging.warning(&#39;Params &#34;cache_ttl&#34; is ignored. Works only if cache is enabled&#39;)

        if self.cache_clear is not None:
            logging.warning(&#39;Params &#34;cache_clear&#34; is ignored. Works only if cache is enabled&#39;)

    if self.webhook is not None:
        params[&#39;webhook_name&#39;] = self.webhook

    return params</code></pre>
</details>
</dd>
<dt id="scrapfly.ScreenshotConfig.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Export the ScreenshotConfig instance to a plain dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; Dict:
    &#34;&#34;&#34;
    Export the ScreenshotConfig instance to a plain dictionary.
    &#34;&#34;&#34;
    return {
        &#39;url&#39;: self.url,
        &#39;format&#39;: Format(self.format).value if self.format else None,
        &#39;capture&#39;: self.capture,
        &#39;resolution&#39;: self.resolution,
        &#39;country&#39;: self.country,
        &#39;timeout&#39;: self.timeout,
        &#39;rendering_wait&#39;: self.rendering_wait,
        &#39;wait_for_selector&#39;: self.wait_for_selector,
        &#39;options&#39;: [Options(option).value for option in self.options] if self.options else None,
        &#39;auto_scroll&#39;: self.auto_scroll,
        &#39;js&#39;: self.js,
        &#39;cache&#39;: self.cache,
        &#39;cache_ttl&#39;: self.cache_ttl,
        &#39;cache_clear&#39;: self.cache_clear,
        &#39;webhook&#39;: self.webhook,
        &#39;raise_on_upstream_error&#39;: self.raise_on_upstream_error
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scrapfly.UpstreamHttpClientError"><code class="flex name class">
<span>class <span class="ident">UpstreamHttpClientError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UpstreamHttpClientError(UpstreamHttpError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.UpstreamHttpError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.UpstreamHttpServerError" href="errors.html#scrapfly.errors.UpstreamHttpServerError">UpstreamHttpServerError</a></li>
</ul>
</dd>
<dt id="scrapfly.UpstreamHttpError"><code class="flex name class">
<span>class <span class="ident">UpstreamHttpError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UpstreamHttpError(HttpError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.UpstreamHttpClientError" href="errors.html#scrapfly.errors.UpstreamHttpClientError">UpstreamHttpClientError</a></li>
</ul>
</dd>
<dt id="scrapfly.UpstreamHttpServerError"><code class="flex name class">
<span>class <span class="ident">UpstreamHttpServerError</span></span>
<span>(</span><span>request: requests.models.Request, response: Optional[requests.models.Response] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Common base class for all non-exit exceptions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UpstreamHttpServerError(UpstreamHttpClientError):
    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="scrapfly.errors.UpstreamHttpClientError" href="errors.html#scrapfly.errors.UpstreamHttpClientError">UpstreamHttpClientError</a></li>
<li>scrapfly.errors.UpstreamHttpError</li>
<li>scrapfly.errors.HttpError</li>
<li><a title="scrapfly.errors.ScrapflyError" href="errors.html#scrapfly.errors.ScrapflyError">ScrapflyError</a></li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="scrapfly.api_config" href="api_config.html">scrapfly.api_config</a></code></li>
<li><code><a title="scrapfly.api_response" href="api_response.html">scrapfly.api_response</a></code></li>
<li><code><a title="scrapfly.client" href="client.html">scrapfly.client</a></code></li>
<li><code><a title="scrapfly.errors" href="errors.html">scrapfly.errors</a></code></li>
<li><code><a title="scrapfly.extraction_config" href="extraction_config.html">scrapfly.extraction_config</a></code></li>
<li><code><a title="scrapfly.frozen_dict" href="frozen_dict.html">scrapfly.frozen_dict</a></code></li>
<li><code><a title="scrapfly.polyfill" href="polyfill/index.html">scrapfly.polyfill</a></code></li>
<li><code><a title="scrapfly.reporter" href="reporter/index.html">scrapfly.reporter</a></code></li>
<li><code><a title="scrapfly.scrape_config" href="scrape_config.html">scrapfly.scrape_config</a></code></li>
<li><code><a title="scrapfly.scrapy" href="scrapy/index.html">scrapfly.scrapy</a></code></li>
<li><code><a title="scrapfly.screenshot_config" href="screenshot_config.html">scrapfly.screenshot_config</a></code></li>
<li><code><a title="scrapfly.webhook" href="webhook.html">scrapfly.webhook</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scrapfly.ApiHttpClientError" href="#scrapfly.ApiHttpClientError">ApiHttpClientError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ApiHttpServerError" href="#scrapfly.ApiHttpServerError">ApiHttpServerError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.EncoderError" href="#scrapfly.EncoderError">EncoderError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ErrorFactory" href="#scrapfly.ErrorFactory">ErrorFactory</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ErrorFactory.HTTP_STATUS_TO_ERROR" href="#scrapfly.ErrorFactory.HTTP_STATUS_TO_ERROR">HTTP_STATUS_TO_ERROR</a></code></li>
<li><code><a title="scrapfly.ErrorFactory.RESOURCE_TO_ERROR" href="#scrapfly.ErrorFactory.RESOURCE_TO_ERROR">RESOURCE_TO_ERROR</a></code></li>
<li><code><a title="scrapfly.ErrorFactory.create" href="#scrapfly.ErrorFactory.create">create</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ExtractionAPIError" href="#scrapfly.ExtractionAPIError">ExtractionAPIError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ExtractionApiResponse" href="#scrapfly.ExtractionApiResponse">ExtractionApiResponse</a></code></h4>
<ul class="two-column">
<li><code><a title="scrapfly.ExtractionApiResponse.content_type" href="#scrapfly.ExtractionApiResponse.content_type">content_type</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.data" href="#scrapfly.ExtractionApiResponse.data">data</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.error" href="#scrapfly.ExtractionApiResponse.error">error</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.extraction_result" href="#scrapfly.ExtractionApiResponse.extraction_result">extraction_result</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.extraction_success" href="#scrapfly.ExtractionApiResponse.extraction_success">extraction_success</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.handle_api_result" href="#scrapfly.ExtractionApiResponse.handle_api_result">handle_api_result</a></code></li>
<li><code><a title="scrapfly.ExtractionApiResponse.raise_for_result" href="#scrapfly.ExtractionApiResponse.raise_for_result">raise_for_result</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ExtractionConfig" href="#scrapfly.ExtractionConfig">ExtractionConfig</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ExtractionConfig.body" href="#scrapfly.ExtractionConfig.body">body</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.charset" href="#scrapfly.ExtractionConfig.charset">charset</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.content_type" href="#scrapfly.ExtractionConfig.content_type">content_type</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.document_compression_format" href="#scrapfly.ExtractionConfig.document_compression_format">document_compression_format</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.ephemeral_template" href="#scrapfly.ExtractionConfig.ephemeral_template">ephemeral_template</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.extraction_ephemeral_template" href="#scrapfly.ExtractionConfig.extraction_ephemeral_template">extraction_ephemeral_template</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.extraction_model" href="#scrapfly.ExtractionConfig.extraction_model">extraction_model</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.extraction_prompt" href="#scrapfly.ExtractionConfig.extraction_prompt">extraction_prompt</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.extraction_template" href="#scrapfly.ExtractionConfig.extraction_template">extraction_template</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.from_dict" href="#scrapfly.ExtractionConfig.from_dict">from_dict</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.is_document_compressed" href="#scrapfly.ExtractionConfig.is_document_compressed">is_document_compressed</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.raise_on_upstream_error" href="#scrapfly.ExtractionConfig.raise_on_upstream_error">raise_on_upstream_error</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.template" href="#scrapfly.ExtractionConfig.template">template</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.to_api_params" href="#scrapfly.ExtractionConfig.to_api_params">to_api_params</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.to_dict" href="#scrapfly.ExtractionConfig.to_dict">to_dict</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.url" href="#scrapfly.ExtractionConfig.url">url</a></code></li>
<li><code><a title="scrapfly.ExtractionConfig.webhook" href="#scrapfly.ExtractionConfig.webhook">webhook</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.HttpError" href="#scrapfly.HttpError">HttpError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ResponseBodyHandler" href="#scrapfly.ResponseBodyHandler">ResponseBodyHandler</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ResponseBodyHandler.JSONDateTimeDecoder" href="#scrapfly.ResponseBodyHandler.JSONDateTimeDecoder">JSONDateTimeDecoder</a></code></li>
<li><code><a title="scrapfly.ResponseBodyHandler.SUPPORTED_COMPRESSION" href="#scrapfly.ResponseBodyHandler.SUPPORTED_COMPRESSION">SUPPORTED_COMPRESSION</a></code></li>
<li><code><a title="scrapfly.ResponseBodyHandler.SUPPORTED_CONTENT_TYPES" href="#scrapfly.ResponseBodyHandler.SUPPORTED_CONTENT_TYPES">SUPPORTED_CONTENT_TYPES</a></code></li>
<li><code><a title="scrapfly.ResponseBodyHandler.read" href="#scrapfly.ResponseBodyHandler.read">read</a></code></li>
<li><code><a title="scrapfly.ResponseBodyHandler.support" href="#scrapfly.ResponseBodyHandler.support">support</a></code></li>
<li><code><a title="scrapfly.ResponseBodyHandler.verify" href="#scrapfly.ResponseBodyHandler.verify">verify</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScrapeApiResponse" href="#scrapfly.ScrapeApiResponse">ScrapeApiResponse</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScrapeApiResponse.config" href="#scrapfly.ScrapeApiResponse.config">config</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.content" href="#scrapfly.ScrapeApiResponse.content">content</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.context" href="#scrapfly.ScrapeApiResponse.context">context</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.error" href="#scrapfly.ScrapeApiResponse.error">error</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.handle_api_result" href="#scrapfly.ScrapeApiResponse.handle_api_result">handle_api_result</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.large_object_handler" href="#scrapfly.ScrapeApiResponse.large_object_handler">large_object_handler</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.raise_for_result" href="#scrapfly.ScrapeApiResponse.raise_for_result">raise_for_result</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.scrape_config" href="#scrapfly.ScrapeApiResponse.scrape_config">scrape_config</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.scrape_result" href="#scrapfly.ScrapeApiResponse.scrape_result">scrape_result</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.scrape_success" href="#scrapfly.ScrapeApiResponse.scrape_success">scrape_success</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.selector" href="#scrapfly.ScrapeApiResponse.selector">selector</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.sink" href="#scrapfly.ScrapeApiResponse.sink">sink</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.soup" href="#scrapfly.ScrapeApiResponse.soup">soup</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.success" href="#scrapfly.ScrapeApiResponse.success">success</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.upstream_result_into_response" href="#scrapfly.ScrapeApiResponse.upstream_result_into_response">upstream_result_into_response</a></code></li>
<li><code><a title="scrapfly.ScrapeApiResponse.upstream_status_code" href="#scrapfly.ScrapeApiResponse.upstream_status_code">upstream_status_code</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScrapeConfig" href="#scrapfly.ScrapeConfig">ScrapeConfig</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScrapeConfig.PUBLIC_DATACENTER_POOL" href="#scrapfly.ScrapeConfig.PUBLIC_DATACENTER_POOL">PUBLIC_DATACENTER_POOL</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.PUBLIC_RESIDENTIAL_POOL" href="#scrapfly.ScrapeConfig.PUBLIC_RESIDENTIAL_POOL">PUBLIC_RESIDENTIAL_POOL</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.asp" href="#scrapfly.ScrapeConfig.asp">asp</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.auto_scroll" href="#scrapfly.ScrapeConfig.auto_scroll">auto_scroll</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.body" href="#scrapfly.ScrapeConfig.body">body</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.cache" href="#scrapfly.ScrapeConfig.cache">cache</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.cache_clear" href="#scrapfly.ScrapeConfig.cache_clear">cache_clear</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.cache_ttl" href="#scrapfly.ScrapeConfig.cache_ttl">cache_ttl</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.cookies" href="#scrapfly.ScrapeConfig.cookies">cookies</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.correlation_id" href="#scrapfly.ScrapeConfig.correlation_id">correlation_id</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.cost_budget" href="#scrapfly.ScrapeConfig.cost_budget">cost_budget</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.country" href="#scrapfly.ScrapeConfig.country">country</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.data" href="#scrapfly.ScrapeConfig.data">data</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.debug" href="#scrapfly.ScrapeConfig.debug">debug</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.dns" href="#scrapfly.ScrapeConfig.dns">dns</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.extract" href="#scrapfly.ScrapeConfig.extract">extract</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.extraction_ephemeral_template" href="#scrapfly.ScrapeConfig.extraction_ephemeral_template">extraction_ephemeral_template</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.extraction_model" href="#scrapfly.ScrapeConfig.extraction_model">extraction_model</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.extraction_prompt" href="#scrapfly.ScrapeConfig.extraction_prompt">extraction_prompt</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.extraction_template" href="#scrapfly.ScrapeConfig.extraction_template">extraction_template</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.format" href="#scrapfly.ScrapeConfig.format">format</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.format_options" href="#scrapfly.ScrapeConfig.format_options">format_options</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.from_dict" href="#scrapfly.ScrapeConfig.from_dict">from_dict</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.from_exported_config" href="#scrapfly.ScrapeConfig.from_exported_config">from_exported_config</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.headers" href="#scrapfly.ScrapeConfig.headers">headers</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.js" href="#scrapfly.ScrapeConfig.js">js</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.js_scenario" href="#scrapfly.ScrapeConfig.js_scenario">js_scenario</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.lang" href="#scrapfly.ScrapeConfig.lang">lang</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.method" href="#scrapfly.ScrapeConfig.method">method</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.os" href="#scrapfly.ScrapeConfig.os">os</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.proxy_pool" href="#scrapfly.ScrapeConfig.proxy_pool">proxy_pool</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.raise_on_upstream_error" href="#scrapfly.ScrapeConfig.raise_on_upstream_error">raise_on_upstream_error</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.render_js" href="#scrapfly.ScrapeConfig.render_js">render_js</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.rendering_wait" href="#scrapfly.ScrapeConfig.rendering_wait">rendering_wait</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.retry" href="#scrapfly.ScrapeConfig.retry">retry</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.screenshot_flags" href="#scrapfly.ScrapeConfig.screenshot_flags">screenshot_flags</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.screenshots" href="#scrapfly.ScrapeConfig.screenshots">screenshots</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.session" href="#scrapfly.ScrapeConfig.session">session</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.session_sticky_proxy" href="#scrapfly.ScrapeConfig.session_sticky_proxy">session_sticky_proxy</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.ssl" href="#scrapfly.ScrapeConfig.ssl">ssl</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.tags" href="#scrapfly.ScrapeConfig.tags">tags</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.timeout" href="#scrapfly.ScrapeConfig.timeout">timeout</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.to_api_params" href="#scrapfly.ScrapeConfig.to_api_params">to_api_params</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.to_dict" href="#scrapfly.ScrapeConfig.to_dict">to_dict</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.url" href="#scrapfly.ScrapeConfig.url">url</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.wait_for_selector" href="#scrapfly.ScrapeConfig.wait_for_selector">wait_for_selector</a></code></li>
<li><code><a title="scrapfly.ScrapeConfig.webhook" href="#scrapfly.ScrapeConfig.webhook">webhook</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScraperAPI" href="#scrapfly.ScraperAPI">ScraperAPI</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScraperAPI.MONITORING_ACCOUNT_AGGREGATION" href="#scrapfly.ScraperAPI.MONITORING_ACCOUNT_AGGREGATION">MONITORING_ACCOUNT_AGGREGATION</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_PROMETHEUS" href="#scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_PROMETHEUS">MONITORING_DATA_FORMAT_PROMETHEUS</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_STRUCTURED" href="#scrapfly.ScraperAPI.MONITORING_DATA_FORMAT_STRUCTURED">MONITORING_DATA_FORMAT_STRUCTURED</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_1H" href="#scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_1H">MONITORING_PERIOD_LAST_1H</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_24H" href="#scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_24H">MONITORING_PERIOD_LAST_24H</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_5m" href="#scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_5m">MONITORING_PERIOD_LAST_5m</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_7D" href="#scrapfly.ScraperAPI.MONITORING_PERIOD_LAST_7D">MONITORING_PERIOD_LAST_7D</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PERIOD_SUBSCRIPTION" href="#scrapfly.ScraperAPI.MONITORING_PERIOD_SUBSCRIPTION">MONITORING_PERIOD_SUBSCRIPTION</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_PROJECT_AGGREGATION" href="#scrapfly.ScraperAPI.MONITORING_PROJECT_AGGREGATION">MONITORING_PROJECT_AGGREGATION</a></code></li>
<li><code><a title="scrapfly.ScraperAPI.MONITORING_TARGET_AGGREGATION" href="#scrapfly.ScraperAPI.MONITORING_TARGET_AGGREGATION">MONITORING_TARGET_AGGREGATION</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyAspError" href="#scrapfly.ScrapflyAspError">ScrapflyAspError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyClient" href="#scrapfly.ScrapflyClient">ScrapflyClient</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScrapflyClient.CONCURRENCY_AUTO" href="#scrapfly.ScrapflyClient.CONCURRENCY_AUTO">CONCURRENCY_AUTO</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DATETIME_FORMAT" href="#scrapfly.ScrapflyClient.DATETIME_FORMAT">DATETIME_FORMAT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DEFAULT_CONNECT_TIMEOUT" href="#scrapfly.ScrapflyClient.DEFAULT_CONNECT_TIMEOUT">DEFAULT_CONNECT_TIMEOUT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DEFAULT_EXTRACTION_API_READ_TIMEOUT" href="#scrapfly.ScrapflyClient.DEFAULT_EXTRACTION_API_READ_TIMEOUT">DEFAULT_EXTRACTION_API_READ_TIMEOUT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DEFAULT_READ_TIMEOUT" href="#scrapfly.ScrapflyClient.DEFAULT_READ_TIMEOUT">DEFAULT_READ_TIMEOUT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DEFAULT_SCREENSHOT_API_READ_TIMEOUT" href="#scrapfly.ScrapflyClient.DEFAULT_SCREENSHOT_API_READ_TIMEOUT">DEFAULT_SCREENSHOT_API_READ_TIMEOUT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.DEFAULT_WEBSCRAPING_API_READ_TIMEOUT" href="#scrapfly.ScrapflyClient.DEFAULT_WEBSCRAPING_API_READ_TIMEOUT">DEFAULT_WEBSCRAPING_API_READ_TIMEOUT</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.HOST" href="#scrapfly.ScrapflyClient.HOST">HOST</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.account" href="#scrapfly.ScrapflyClient.account">account</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.async_extraction" href="#scrapfly.ScrapflyClient.async_extraction">async_extraction</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.async_scrape" href="#scrapfly.ScrapflyClient.async_scrape">async_scrape</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.async_screenshot" href="#scrapfly.ScrapflyClient.async_screenshot">async_screenshot</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.brotli" href="#scrapfly.ScrapflyClient.brotli">brotli</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.close" href="#scrapfly.ScrapflyClient.close">close</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.concurrent_scrape" href="#scrapfly.ScrapflyClient.concurrent_scrape">concurrent_scrape</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.connect_timeout" href="#scrapfly.ScrapflyClient.connect_timeout">connect_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.debug" href="#scrapfly.ScrapflyClient.debug">debug</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.default_read_timeout" href="#scrapfly.ScrapflyClient.default_read_timeout">default_read_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.distributed_mode" href="#scrapfly.ScrapflyClient.distributed_mode">distributed_mode</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.extract" href="#scrapfly.ScrapflyClient.extract">extract</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.extraction_api_read_timeout" href="#scrapfly.ScrapflyClient.extraction_api_read_timeout">extraction_api_read_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.get_monitoring_metrics" href="#scrapfly.ScrapflyClient.get_monitoring_metrics">get_monitoring_metrics</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.get_monitoring_target_metrics" href="#scrapfly.ScrapflyClient.get_monitoring_target_metrics">get_monitoring_target_metrics</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.host" href="#scrapfly.ScrapflyClient.host">host</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.http" href="#scrapfly.ScrapflyClient.http">http</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.key" href="#scrapfly.ScrapflyClient.key">key</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.max_concurrency" href="#scrapfly.ScrapflyClient.max_concurrency">max_concurrency</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.monitoring_api_read_timeout" href="#scrapfly.ScrapflyClient.monitoring_api_read_timeout">monitoring_api_read_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.open" href="#scrapfly.ScrapflyClient.open">open</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.read_timeout" href="#scrapfly.ScrapflyClient.read_timeout">read_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.reporter" href="#scrapfly.ScrapflyClient.reporter">reporter</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.resilient_scrape" href="#scrapfly.ScrapflyClient.resilient_scrape">resilient_scrape</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.save_scrape_screenshot" href="#scrapfly.ScrapflyClient.save_scrape_screenshot">save_scrape_screenshot</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.save_screenshot" href="#scrapfly.ScrapflyClient.save_screenshot">save_screenshot</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.scrape" href="#scrapfly.ScrapflyClient.scrape">scrape</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.screenshot" href="#scrapfly.ScrapflyClient.screenshot">screenshot</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.screenshot_api_read_timeout" href="#scrapfly.ScrapflyClient.screenshot_api_read_timeout">screenshot_api_read_timeout</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.sink" href="#scrapfly.ScrapflyClient.sink">sink</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.ua" href="#scrapfly.ScrapflyClient.ua">ua</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.verify" href="#scrapfly.ScrapflyClient.verify">verify</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.version" href="#scrapfly.ScrapflyClient.version">version</a></code></li>
<li><code><a title="scrapfly.ScrapflyClient.web_scraping_api_read_timeout" href="#scrapfly.ScrapflyClient.web_scraping_api_read_timeout">web_scraping_api_read_timeout</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyError" href="#scrapfly.ScrapflyError">ScrapflyError</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScrapflyError.KIND_HTTP_BAD_RESPONSE" href="#scrapfly.ScrapflyError.KIND_HTTP_BAD_RESPONSE">KIND_HTTP_BAD_RESPONSE</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.KIND_SCRAPFLY_ERROR" href="#scrapfly.ScrapflyError.KIND_SCRAPFLY_ERROR">KIND_SCRAPFLY_ERROR</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_ASP" href="#scrapfly.ScrapflyError.RESOURCE_ASP">RESOURCE_ASP</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_PROXY" href="#scrapfly.ScrapflyError.RESOURCE_PROXY">RESOURCE_PROXY</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_SCHEDULE" href="#scrapfly.ScrapflyError.RESOURCE_SCHEDULE">RESOURCE_SCHEDULE</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_SCRAPE" href="#scrapfly.ScrapflyError.RESOURCE_SCRAPE">RESOURCE_SCRAPE</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_SESSION" href="#scrapfly.ScrapflyError.RESOURCE_SESSION">RESOURCE_SESSION</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_THROTTLE" href="#scrapfly.ScrapflyError.RESOURCE_THROTTLE">RESOURCE_THROTTLE</a></code></li>
<li><code><a title="scrapfly.ScrapflyError.RESOURCE_WEBHOOK" href="#scrapfly.ScrapflyError.RESOURCE_WEBHOOK">RESOURCE_WEBHOOK</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyProxyError" href="#scrapfly.ScrapflyProxyError">ScrapflyProxyError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyScheduleError" href="#scrapfly.ScrapflyScheduleError">ScrapflyScheduleError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyScrapeError" href="#scrapfly.ScrapflyScrapeError">ScrapflyScrapeError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflySessionError" href="#scrapfly.ScrapflySessionError">ScrapflySessionError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyThrottleError" href="#scrapfly.ScrapflyThrottleError">ScrapflyThrottleError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScrapflyWebhookError" href="#scrapfly.ScrapflyWebhookError">ScrapflyWebhookError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScreenshotAPIError" href="#scrapfly.ScreenshotAPIError">ScreenshotAPIError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.ScreenshotApiResponse" href="#scrapfly.ScreenshotApiResponse">ScreenshotApiResponse</a></code></h4>
<ul class="two-column">
<li><code><a title="scrapfly.ScreenshotApiResponse.error" href="#scrapfly.ScreenshotApiResponse.error">error</a></code></li>
<li><code><a title="scrapfly.ScreenshotApiResponse.handle_api_result" href="#scrapfly.ScreenshotApiResponse.handle_api_result">handle_api_result</a></code></li>
<li><code><a title="scrapfly.ScreenshotApiResponse.image" href="#scrapfly.ScreenshotApiResponse.image">image</a></code></li>
<li><code><a title="scrapfly.ScreenshotApiResponse.metadata" href="#scrapfly.ScreenshotApiResponse.metadata">metadata</a></code></li>
<li><code><a title="scrapfly.ScreenshotApiResponse.raise_for_result" href="#scrapfly.ScreenshotApiResponse.raise_for_result">raise_for_result</a></code></li>
<li><code><a title="scrapfly.ScreenshotApiResponse.screenshot_success" href="#scrapfly.ScreenshotApiResponse.screenshot_success">screenshot_success</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.ScreenshotConfig" href="#scrapfly.ScreenshotConfig">ScreenshotConfig</a></code></h4>
<ul class="">
<li><code><a title="scrapfly.ScreenshotConfig.auto_scroll" href="#scrapfly.ScreenshotConfig.auto_scroll">auto_scroll</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.cache" href="#scrapfly.ScreenshotConfig.cache">cache</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.cache_clear" href="#scrapfly.ScreenshotConfig.cache_clear">cache_clear</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.cache_ttl" href="#scrapfly.ScreenshotConfig.cache_ttl">cache_ttl</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.capture" href="#scrapfly.ScreenshotConfig.capture">capture</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.country" href="#scrapfly.ScreenshotConfig.country">country</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.format" href="#scrapfly.ScreenshotConfig.format">format</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.from_dict" href="#scrapfly.ScreenshotConfig.from_dict">from_dict</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.js" href="#scrapfly.ScreenshotConfig.js">js</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.options" href="#scrapfly.ScreenshotConfig.options">options</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.raise_on_upstream_error" href="#scrapfly.ScreenshotConfig.raise_on_upstream_error">raise_on_upstream_error</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.rendering_wait" href="#scrapfly.ScreenshotConfig.rendering_wait">rendering_wait</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.resolution" href="#scrapfly.ScreenshotConfig.resolution">resolution</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.timeout" href="#scrapfly.ScreenshotConfig.timeout">timeout</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.to_api_params" href="#scrapfly.ScreenshotConfig.to_api_params">to_api_params</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.to_dict" href="#scrapfly.ScreenshotConfig.to_dict">to_dict</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.url" href="#scrapfly.ScreenshotConfig.url">url</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.wait_for_selector" href="#scrapfly.ScreenshotConfig.wait_for_selector">wait_for_selector</a></code></li>
<li><code><a title="scrapfly.ScreenshotConfig.webhook" href="#scrapfly.ScreenshotConfig.webhook">webhook</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scrapfly.UpstreamHttpClientError" href="#scrapfly.UpstreamHttpClientError">UpstreamHttpClientError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.UpstreamHttpError" href="#scrapfly.UpstreamHttpError">UpstreamHttpError</a></code></h4>
</li>
<li>
<h4><code><a title="scrapfly.UpstreamHttpServerError" href="#scrapfly.UpstreamHttpServerError">UpstreamHttpServerError</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>